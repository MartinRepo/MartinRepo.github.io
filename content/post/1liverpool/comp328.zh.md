---
title: "é«˜æ€§èƒ½è®¡ç®—(COMP328)"
date: 2024-02-03T17:39:49Z
draft: false
author: "Martin"
tags: 
- é«˜æ€§èƒ½è®¡ç®—
description: ""
weight: # è¾“å…¥1å¯ä»¥é¡¶ç½®æ–‡ç« ï¼Œç”¨æ¥ç»™æ–‡ç« å±•ç¤ºæ’åºï¼Œä¸å¡«å°±é»˜è®¤æŒ‰æ—¶é—´æ’åº
slug: "comp328"
comments: true
showToc: true # æ˜¾ç¤ºç›®å½•
TocOpen: true # è‡ªåŠ¨å±•å¼€ç›®å½•
hidemeta: false # æ˜¯å¦éšè—æ–‡ç« çš„å…ƒä¿¡æ¯ï¼Œå¦‚å‘å¸ƒæ—¥æœŸã€ä½œè€…ç­‰
disableShare: true # åº•éƒ¨ä¸æ˜¾ç¤ºåˆ†äº«æ 
showbreadcrumbs: true #é¡¶éƒ¨æ˜¾ç¤ºå½“å‰è·¯å¾„
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
mermaid: true
---
# Week1
é«˜æ€§èƒ½è®¡ç®—çš„ç›®æ ‡
- å¯¹äºæœ‰é™çš„æ•°æ®é›†ï¼Œæœ€å°åŒ–è§£å†³æ—¶é—´
- å¯¹äºæ— é™çš„æ•°æ®é›†ï¼Œæœ€å¤§åŒ–ååé‡(throughput)
- æœ‰èƒ½åŠ›è§£å†³ä¸€äº›å¯¹äºå¯ç”¨çš„å†…å­˜æ¥è¯´å¤ªå¤§çš„é—®é¢˜
- æœ€å¤§åŒ–èµ„æºåˆ©ç”¨ - CPU/å†…å­˜/ç½‘ç»œ/åŠ é€Ÿå™¨(GPU)/ç”µåŠ›

## ä¸€äº›å¸¸ç”¨æœ¯è¯­
1. Parallelism vs Concurrency
    - Parallelism: å¤šä¸ªè¿›ç¨‹åŒæ—¶ä¸”ç‹¬ç«‹æ‰§è¡Œ
    - Concurrency: å¤šä¸ªè¿›ç¨‹åŒæ—¶æ‰§è¡Œä¸”å…±äº«è‡³å°‘ä¸€ç§èµ„æº
2. Processor, Die & Socket
    - Processor: æ‰§è¡Œç¨‹åºæŒ‡ä»¤çš„ç”µè·¯ã€‚è®¡ç®—æœºç³»ç»Ÿä¸­å¯èƒ½æœ‰è®¸å¤šå¤„ç†å™¨ï¼Œä¾‹å¦‚å›¾å½¢å¤„ç†å™¨ã€è§†é¢‘å¤„ç†å™¨ã€‚åœ¨æ²¡æœ‰é™å®šçš„æƒ…å†µä¸‹ï¼Œé€šå¸¸æŒ‡ä¸­å¤®å¤„ç†å™¨
    - CPU: è®¡ç®—æœºç³»ç»Ÿä¸­ä¸»è¦çš„é€šç”¨å¤„ç†å™¨ï¼ˆä¹‹ä¸€ï¼‰ï¼Œè€Œéç‰¹å®šç”¨é€”ï¼ˆå¦‚è§†é¢‘è§£å‹ç¼©ï¼‰ã€‚
    - Die: æŒ‡ç¡…æ™¶ç‰‡ï¼ŒåŒ…å«å¤„ç†å™¨ï¼ˆé€šå¸¸æ˜¯ä¸­å¤®å¤„ç†å™¨ï¼‰ä»¥åŠæ¥å£æ‰€éœ€çš„å…¶ä»–ç»„ä»¶ï¼ˆå¦‚å†…å­˜æ§åˆ¶å™¨ï¼‰ã€‚
    - Socket: å¤„ç†å™¨å’Œè®¡ç®—æœºä¸»æ¿ä¹‹é—´çš„ç‰©ç†æ¥å£ï¼Œå®ƒå®šä¹‰äº†å¤„ç†å™¨ä¸ä¸»æ¿è¿æ¥çš„æ–¹å¼ã€‚ä¸åŒçš„å¤„ç†å™¨å’Œä¸»æ¿å¯èƒ½éœ€è¦ä¸åŒç±»å‹çš„Socketã€‚
3. Core & Thread
    - Core: æ ¸å¿ƒæ˜¯CPUå†…éƒ¨çš„ä¸€ä¸ªç‰©ç†å¤„ç†å•å…ƒï¼Œèƒ½å¤Ÿç‹¬ç«‹æ‰§è¡Œè®¡ç®—ä»»åŠ¡ã€‚æ¯ä¸ªæ ¸å¿ƒå¯ä»¥ç‹¬ç«‹å¤„ç†æŒ‡ä»¤å’Œæ‰§è¡Œè®¡ç®—æ“ä½œã€‚
    - Thread: çº¿ç¨‹æ˜¯æ“ä½œç³»ç»Ÿèƒ½å¤Ÿè¿›è¡Œè®¡ç®—è°ƒåº¦çš„æœ€å°å•ä½ã€‚å®ƒæ˜¯ç¨‹åºæ‰§è¡Œæµçš„ä¸€ä¸ªå•ä¸€é¡ºåºï¼Œå¯ä»¥è¢«æ“ä½œç³»ç»Ÿè°ƒåº¦ï¼ˆå¯åŠ¨ã€åœæ­¢ã€æŒ‚èµ·ç­‰ï¼‰ã€‚
    - æ ¸å¿ƒå’Œçº¿ç¨‹å…±åŒå®šä¹‰äº†å¤„ç†å™¨çš„å¤„ç†èƒ½åŠ›
4. Node
    - æŒ‡ä¸€ä¸ªæœåŠ¡å™¨èŠ‚ç‚¹ï¼ˆä¸€å°è®¡ç®—æœºï¼‰
5. Cluster/Supercomputer
    - æˆç™¾ä¸Šåƒä¸ªèŠ‚ç‚¹ç»„æˆé›†ç¾¤
6. Single precision floating-point
    - é€šå¸¸å ç”¨32ä½ï¼ˆ4å­—èŠ‚ï¼‰çš„å­˜å‚¨ç©ºé—´ï¼ŒCè¯­è¨€ä¸­çš„float32ç±»å‹ï¼Œ1ç¬¦å·ä½ï¼Œ8æŒ‡æ•°ä½ï¼Œ23æœ‰æ•ˆæ•°å­—ä½
7. Double precision floating-point
    - é€šå¸¸å ç”¨64ä½ï¼ˆ8å­—èŠ‚ï¼‰çš„å­˜å‚¨ç©ºé—´ï¼ŒCè¯­è¨€ä¸­çš„float64ç±»å‹ï¼Œ1ç¬¦å·ä½ï¼Œ11æŒ‡æ•°ä½ï¼Œ52æœ‰æ•ˆæ•°å­—ä½
8. Flop
    - Floating-point operations per second

## å¦‚ä½•é‡åŒ–/è¯„ä¼°æ€§èƒ½
å…¬å¼å¦‚ä¸‹ğŸ‘‡

$R_{peak} = 2\times w_{vec} \times r_{clock} \times n_{core} \times n_{socket}$

å˜é‡è§£é‡Šï¼š
1. $R_{peak} - $å³°å€¼ç†è®ºæ€§èƒ½
2. $w_{vec} - $å‘é‡å®½åº¦ï¼Œè¡¨ç¤ºæ¯ä¸ªå¤„ç†å™¨æ ¸å¿ƒæ¯ä¸ªæ—¶é’Ÿå‘¨æœŸå†…å¯ä»¥æ‰§è¡Œçš„æµ®ç‚¹è¿ç®—æ•°ã€‚
3. $r_{clock} - $è¡¨ç¤ºå¤„ç†å™¨æ ¸å¿ƒæ¯ç§’é’Ÿå¯ä»¥æ‰§è¡Œçš„æ—¶é’Ÿå‘¨æœŸæ•°ã€‚
4. $n_{core} - $è¡¨ç¤ºå•ä¸ªå¤„ç†å™¨ï¼ˆCPUï¼‰æˆ–è®¡ç®—èŠ‚ç‚¹ä¸­çš„æ ¸å¿ƒæ•°é‡ã€‚
5. $n_{socket} - $è¡¨ç¤ºç³»ç»Ÿä¸­å¤„ç†å™¨ï¼ˆCPUï¼‰çš„æ•°é‡ã€‚

**Linpackæ€§èƒ½æµ‹è¯•**ï¼šLinpackæ€§èƒ½æµ‹è¯•æ˜¯ä¸€ç§è¡¡é‡è®¡ç®—ç³»ç»Ÿè§£å†³é«˜å¯†åº¦çº¿æ€§ä»£æ•°é—®é¢˜èƒ½åŠ›çš„æµ‹è¯•ã€‚è¿™ä¸ªæµ‹è¯•é€šè¿‡æµ‹é‡ç³»ç»Ÿåœ¨æ‰§è¡Œå¤§è§„æ¨¡åŒç²¾åº¦ï¼ˆ64ä½ï¼‰æµ®ç‚¹ç®—æœ¯çŸ©é˜µè¿ç®—æ—¶çš„æ€§èƒ½æ¥è¯„ä¼°è®¡ç®—æœºçš„é€Ÿåº¦å’Œæ•ˆç‡ã€‚Linpackæµ‹è¯•çš„ä¸€ä¸ªå…¸å‹åº”ç”¨æ˜¯è®¡ç®—ç»™å®šå¤§å°çš„çŸ©é˜µ$A$å’Œå‘é‡$b$ï¼Œæ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„$Ax = b$
# Week2
## Plot programs performance
ç”¨"Arithmetic Intensity"ï¼Œè®°ä½œ$I(n)$ï¼Œæ¥è¯„ä¼°æ€§èƒ½ã€‚$I(n) = {W(n)\over Q(n)}$ã€‚

$W(n)$:ç¨‹åºæ‰§è¡Œflopsçš„æ¬¡æ•°ï¼Œ$Q(n)$:ä»å†…å­˜ä¼ è¾“åˆ°ç¼“å­˜çš„å­—èŠ‚æ•°ã€‚

- ä½Arithmetic Intensityçš„ç¨‹åºå«å†…å­˜å—é™ç¨‹åº
- é«˜Arithmetic Intensityçš„ç¨‹åºå«è®¡ç®—å—é™ç¨‹åº
- å¯¹äºå†…å­˜å—é™çš„ç¨‹åºï¼Œå¤„ç†å™¨éœ€è¦èŠ±è´¹æ›´å¤šæ—¶é—´ç­‰å¾…æ“ä½œæ•°ä»å†…å­˜ä¸­ä¼ é€å‡ºæ¥ï¼ˆæ›´å¤šçš„æ—¶é—´èŠ±è´¹åœ¨è®¿é—®å†…å­˜ä¸Šï¼Œè€Œä¸æ˜¯è¿ç®—ä¸Šï¼‰ã€‚

### Roofline Model
Roofline Modelæ˜¯å¸®åŠ©äº†è§£è½¯ä»¶æ€§èƒ½çš„å¯è§†åŒ–å·¥å…·ã€‚

Roofline Modelä¸¤ä¸ªç»„æˆéƒ¨åˆ†
- **å³°å€¼æ€§èƒ½ï¼ˆ$R_{peak} / R_{max}$ï¼‰**ï¼šè¿™æ˜¯è®¡ç®—ç³»ç»Ÿåœ¨ç†æƒ³æƒ…å†µä¸‹å¯ä»¥è¾¾åˆ°çš„æœ€é«˜è®¡ç®—æ€§èƒ½ï¼Œé€šå¸¸ä»¥æ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼ˆFLOPSï¼‰æ¥è¡¡é‡ã€‚å³°å€¼æ€§èƒ½ç”±å¤„ç†å™¨çš„ç¡¬ä»¶ç‰¹æ€§å†³å®šï¼Œæ¯”å¦‚æ ¸å¿ƒæ•°é‡ã€æ—¶é’Ÿé¢‘ç‡å’Œå‘é‡åŒ–èƒ½åŠ›ã€‚å›¾ä¸­æ°´å¹³çº¿å°±æ˜¯å³°å€¼æ€§èƒ½
- **å†…å­˜å¸¦å®½ï¼ˆMemory Bandwidthï¼‰**ï¼šè¿™æ˜¯è®¡ç®—ç³»ç»Ÿåœ¨å•ä½æ—¶é—´å†…èƒ½ä»å†…å­˜ä¸­è¯»å†™æ•°æ®çš„æœ€å¤§é€Ÿç‡ï¼Œé€šå¸¸ä»¥æ¯ç§’ä¼ è¾“çš„å­—èŠ‚æ•°ï¼ˆBytes/sï¼‰æ¥è¡¡é‡ã€‚å†…å­˜å¸¦å®½æ˜¯ç”±ç³»ç»Ÿçš„å†…å­˜æ¶æ„å’Œå†…å­˜ç±»å‹å†³å®šçš„ã€‚å¸¦è§’åº¦çš„æ–œçº¿è¡¨ç¤ºå†…å­˜å¸¦å®½
## å†¯Â·è¯ºä¼Šæ›¼
Von Neumannæ¶æ„å›¾å¦‚ä¸‹æ‰€ç¤ºğŸ‘‡
![The von Neumann Architecture](/img/comp328/vonNeumann.png)
- ç”±æ§åˆ¶å•å…ƒå’Œç®—æœ¯/é€»è¾‘å•å…ƒç»„æˆçš„CPUã€‚
- ç‹¬ç«‹çš„å­˜å‚¨åŒºï¼Œå¯å­˜å‚¨æŒ‡ä»¤å’Œæ•°æ®ã€‚
- æŒ‡ä»¤ç”±CPUæ‰§è¡Œï¼Œå› æ­¤å¿…é¡»å°†æŒ‡ä»¤ä»å­˜å‚¨å™¨å¸¦å…¥CPUã€‚
- æ•°æ®ä¹Ÿå¿…é¡»ä»å­˜å‚¨å™¨è¿›å…¥CPUæ‰èƒ½æ‰§è¡Œã€‚
- CPUåŒ…å«å¯„å­˜å™¨ï¼Œä½œä¸ºä¸´æ—¶å­˜å‚¨çš„åˆ®æ¿ã€‚
- **The von Neumann bottleneck:** æ•°æ®å’ŒæŒ‡ä»¤å…±ç”¨ä¸€æ¡æ€»çº¿ï¼Œå› æ­¤æŒ‡ä»¤è·å–å’Œæ•°æ®æ“ä½œä¸èƒ½åŒæ—¶è¿›è¡Œã€‚

### å†¯Â·è¯ºä¼Šæ›¼ç“¶é¢ˆ
1. ä»å†…å­˜ä¸­æŠ“å–å¯¹åº”ç¨‹åºè®¡æ•°å™¨çš„æŒ‡ä»¤
2. è§£ç æŒ‡ä»¤
3. ä»å†…å­˜ä¸­æŠ“å–æ•°æ®
4. æ‰§è¡ŒæŒ‡ä»¤
5. å†™å›ç»“æœ

### Pipelining
**Pipeliningçš„åŸºæœ¬æ¦‚å¿µ**

æµæ°´çº¿æŠ€æœ¯é€šè¿‡å°†æŒ‡ä»¤çš„æ‰§è¡Œè¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªé˜¶æ®µï¼Œå¹¶è®©ä¸åŒçš„æŒ‡ä»¤åœ¨ä¸åŒçš„æ—¶é—´å¹¶è¡Œå¤„ç†è¿™äº›é˜¶æ®µæ¥æé«˜å¤„ç†é€Ÿåº¦ã€‚è¿™å°±å¥½æ¯”æ˜¯åœ¨ç»„è£…çº¿ä¸Šï¼Œæ¯ä¸ªå·¥äººè´Ÿè´£ç»„è£…çº¿ä¸Šçš„ä¸€ä¸ªç‰¹å®šä»»åŠ¡ï¼Œäº§å“å¯ä»¥æ›´å¿«åœ°å®Œæˆï¼Œå› ä¸ºå¤šä¸ªä»»åŠ¡æ˜¯åœ¨åŒæ—¶è¿›è¡Œï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°å®Œæˆã€‚

**åœ¨von neumann cycleä¸­çš„åº”ç”¨**

åœ¨åº”ç”¨æµæ°´çº¿æŠ€æœ¯åï¼Œå¤„ç†å™¨å¯ä»¥åœ¨å®Œæˆå½“å‰æŒ‡ä»¤çš„æŸä¸ªé˜¶æ®µçš„åŒæ—¶ï¼Œå¼€å§‹æ‰§è¡Œä¸‹ä¸€æ¡æŒ‡ä»¤çš„å‰ä¸€ä¸ªé˜¶æ®µã€‚ä¾‹å¦‚ï¼Œå½“ç¬¬ä¸€æ¡æŒ‡ä»¤åœ¨æ‰§è¡Œé˜¶æ®µæ—¶ï¼Œç¬¬äºŒæ¡æŒ‡ä»¤å¯ä»¥åŒæ—¶è¿›è¡Œè¯‘ç ï¼Œç¬¬ä¸‰æ¡æŒ‡ä»¤å¯ä»¥è¿›è¡Œå–æŒ‡ã€‚è¿™æ ·ï¼Œè™½ç„¶æ¯æ¡æŒ‡ä»¤çš„æ‰§è¡Œä»ç„¶éœ€è¦ä¸²è¡Œç»è¿‡æ‰€æœ‰é˜¶æ®µï¼Œä½†å¤„ç†å™¨å¯ä»¥åœ¨åŒä¸€æ—¶åˆ»å¤„ç†å¤šæ¡æŒ‡ä»¤çš„ä¸åŒé˜¶æ®µï¼Œä»è€Œå¤§å¤§æé«˜äº†æŒ‡ä»¤çš„ååç‡ã€‚

### å¯¹æŠ—å†¯Â·è¯ºä¼Šæ›¼ç“¶é¢ˆçš„æ–¹æ³•
> åœ¨èŠ¯ç‰‡ä¸Šæ·»åŠ é«˜é€Ÿç¼“å­˜ï¼ˆcache)ï¼Œä½†é«˜é€Ÿç¼“å­˜ä¹Ÿå­˜åœ¨é—®é¢˜

ä¾‹å¦‚ï¼Œé«˜é€Ÿç¼“å­˜è¶Šå¤§ï¼Œæ•°æ®è®¿é—®é€Ÿåº¦è¶Šæ…¢ã€‚å¯ä»¥é‡‡ç”¨å¤šçº§ç¼“å­˜æ¶æ„ï¼Œé€šè¿‡åœ¨å¤„ç†å™¨å’Œä¸»å†…å­˜ä¹‹é—´å¼•å…¥å¤šä¸ªå±‚çº§çš„ç¼“å­˜ï¼Œæ—¨åœ¨å¹³è¡¡ç¼“å­˜å¤§å°ã€è®¿é—®é€Ÿåº¦å’Œå‘½ä¸­ç‡ä¹‹é—´çš„å…³ç³»ã€‚

é«˜é€Ÿç¼“å­˜åˆ©ç”¨äº†ç¨‹åºçš„ç©ºé—´å±€éƒ¨æ€§ï¼ˆspatial localityï¼‰å’Œæ—¶é—´å±€éƒ¨æ€§ï¼ˆtemporal localityï¼‰ã€‚

- æ—¶é—´å±€éƒ¨æ€§æŒ‡çš„æ˜¯åœ¨è¾ƒçŸ­çš„æ—¶é—´å†…ï¼Œè¢«è®¿é—®è¿‡ä¸€æ¬¡çš„æ•°æ®é¡¹å¾ˆå¯èƒ½åœ¨ä¸ä¹…çš„å°†æ¥å†æ¬¡è¢«è®¿é—®çš„ç‰¹æ€§ã€‚è¿™ç§è®¿é—®æ¨¡å¼æ„å‘³ç€ä¸€æ—¦æ•°æ®è¢«åŠ è½½åˆ°ç¼“å­˜ä¸­ï¼Œå®ƒå¾ˆå¯èƒ½å¾ˆå¿«å†æ¬¡è¢«éœ€è¦ï¼Œå› æ­¤ä¿ç•™è¿™äº›æ•°æ®é¡¹åœ¨ç¼“å­˜ä¸­å¯ä»¥å‡å°‘å¯¹è¾ƒæ…¢ä¸»å­˜çš„è®¿é—®æ¬¡æ•°ã€‚
- ç©ºé—´å±€éƒ¨æ€§æ˜¯æŒ‡å¦‚æœä¸€ä¸ªæ•°æ®é¡¹è¢«è®¿é—®ï¼Œé‚£ä¹ˆå…¶é™„è¿‘çš„æ•°æ®é¡¹å¾ˆå¿«ä¹Ÿå¯èƒ½è¢«è®¿é—®çš„ç‰¹æ€§ã€‚è¿™ç§æ¨¡å¼åŸºäºæ•°æ®å­˜å‚¨çš„ç‰©ç†ç»“æ„ï¼Œç›¸é‚»çš„æ•°æ®é¡¹é€šå¸¸ä¹Ÿåœ¨å†…å­˜ä¸­ç›¸é‚»å­˜å‚¨ã€‚é«˜é€Ÿç¼“å­˜ç³»ç»Ÿåˆ©ç”¨è¿™ä¸€ç‰¹æ€§é€šè¿‡é¢„å–é™„è¿‘çš„æ•°æ®é¡¹åˆ°ç¼“å­˜ä¸­ï¼Œå³ä½¿è¿™äº›æ•°æ®é¡¹è¿˜æ²¡æœ‰è¢«æ˜¾å¼è¯·æ±‚ã€‚

AMD Bulldozer æœåŠ¡å™¨æ’æ§½çš„å†…å­˜å±‚æ¬¡ç»“æ„
![AMD Bulldozer æœåŠ¡å™¨æ’æ§½](/img/comp328/Hwloc.png)


## How to gain performance form a single core/socket
```java
for (int i = 0; i<1000; i++) {
    b[i] = a[i]*a[i];
}
```
å¯¹äºä¸Šé¢è¿™ä¸ªç¨‹åºï¼Œåº”è¯¥è¿è¡Œ1000ä¸ªclock cyclesã€‚å‡å¦‚ä¸€ä¸ªclock cycleä¸æ˜¯åšä¸€æ¬¡è¿­ä»£ï¼Œè€Œæ˜¯åš4æ¬¡è¿­ä»£ï¼Œé‚£ä¹ˆæ€»å…±éœ€è¦250ä¸ªclock cyclesã€‚è¿™å°±å«vector processingï¼Œä¹Ÿå«Single Instruction, Multiple Dataï¼ˆSIMDï¼‰ã€‚

åœ¨å•æ ¸ä¸Šå‹æ¦¨æ›´å¤šæ€§èƒ½ï¼šSMT(simultaneous multithreading)ï¼Œé€šè¿‡åœ¨å•ä¸ªç‰©ç†CPUæ ¸å¿ƒä¸ŠåŒæ—¶æ‰§è¡Œå¤šä¸ªçº¿ç¨‹æ¥æé«˜å¤„ç†å™¨çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚SMTå…è®¸å•ä¸ªæ ¸å¿ƒåƒæ“ä½œç³»ç»Ÿå’Œåº”ç”¨ç¨‹åºå‘ˆç°å‡ºå¤šä¸ªé€»è¾‘æ ¸å¿ƒæˆ–çº¿ç¨‹ï¼Œä½¿å¾—å¤„ç†å™¨å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å…¶èµ„æºï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸€ä¸ªçº¿ç¨‹ç­‰å¾…æ•°æ®è®¿é—®æˆ–æ‰§è¡Œé•¿æ—¶é—´æ“ä½œæ—¶ï¼Œå¤„ç†å™¨å¯ä»¥è½¬è€Œæ‰§è¡Œå¦ä¸€ä¸ªçº¿ç¨‹çš„ä»»åŠ¡ã€‚

# Week3
## ç¼–è¯‘å™¨ä¼˜åŒ–çš„å¸¸ç”¨æ–¹æ³•
ä¸¤ç§ä¼˜åŒ–æ–¹å¼ï¼š
1. æ—¶é—´ä¼˜åŒ–
2. ç©ºé—´ä¼˜åŒ–

ç¼–è¯‘å™¨ä¼šè‡ªåŠ¨çš„åšä¸€äº›ä¼˜åŒ–ã€‚

von Neumann cycleä¸­çš„æ‰§è¡Œé˜¶æ®µä¹Ÿéœ€è¦å¯¹å†…å­˜è¿›è¡Œè¯»å†™ã€‚æ‰€æœ‰çš„ç®—æœ¯æ“ä½œéƒ½éœ€è¦è¯»å†™äº¤æ›¿è¿›è¡Œã€‚ç¼–è¯‘å™¨çš„å·¥ä½œå°±æ˜¯ç»™ç‰¹å®šç¡¬ä»¶ç¡®å®šåˆç†çš„äº¤é”™é¡ºåºã€‚

è¯»æ•°æ®çš„æ–¹å¼å¯¹æ€§èƒ½è¡¨ç°æ¥è¯´å¾ˆé‡è¦ï¼ˆcacheçš„å­˜åœ¨å°±æ˜¯ä¸ºäº†å‡å°‘å¤„ç†å™¨è®¿é—®ä¸»å­˜çš„æ¬¡æ•°ï¼‰ã€‚

æŠŠå¤šç»´æ•°æ®å­˜å‚¨æˆå•ç»´æ•°æ®çš„ä¸¤ç§æ–¹æ³•
- row majorï¼ŒC/C++/Javaé€šå¸¸ç”¨row major
- column majorï¼ŒFortran/Pascalé€šå¸¸ç”¨column major

å¸¸ç”¨ä¼˜åŒ–æ–¹æ³•
1. Inlining
```c
float add(int a, int b){
float results = a + b;
return result;
}
int main(NULL){
float a = 3.6;
float b = 6.3;
float result = add(a, b);
}
```
ç¼–è¯‘å™¨ä¼šæŠŠæ‰€æœ‰çš„å‡½æ•°ç”¨inline codeä»£æ›¿ï¼Œæ¶ˆé™¤å‡½æ•°è°ƒç”¨çš„å¼€é”€ï¼ŒåŒ…æ‹¬å‹æ ˆã€è·³è½¬å’Œè¿”å›ç­‰æ“ä½œã€‚InliningåğŸ‘‡
```c
int main(NULL){
float a = 3.6;
float b = 6.3;
float result = a+b;
}
```
ç¼ºç‚¹ï¼šå¦‚æœä¸€ä¸ªå‡½æ•°åœ¨å¤šä¸ªåœ°æ–¹è¢«å†…è”ï¼Œé‚£ä¹ˆå¯æ‰§è¡Œæ–‡ä»¶çš„å¤§å°å¯èƒ½ä¼šå¢åŠ ï¼Œè¿™æœ‰æ—¶è¢«ç§°ä¸ºä»£ç è†¨èƒ€ã€‚è€Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¦‚æœå†…è”å¯¼è‡´ç”Ÿæˆçš„ä»£ç è¿‡å¤§ï¼Œå¯èƒ½ä¼šé™ä½æŒ‡ä»¤ç¼“å­˜çš„æ•ˆç‡ï¼Œåè€Œå‡æ…¢ç¨‹åºçš„è¿è¡Œé€Ÿåº¦ã€‚

2. Dead code / Dead store
    - Dead code: ç”±äºä¸€äº›æ¡ä»¶ï¼Œè¿™éƒ¨åˆ†ä»£ç æ°¸è¿œæ— æ³•æ‰§è¡Œ
    - Dead store: è®¡ç®—è¿‡ä½†ä»æœªä½¿ç”¨è¿‡çš„å˜é‡
    ç¼–è¯‘å™¨æ‰¾åˆ°dead codeå’Œdead storeå¹¶å®‰å…¨åœ°å¿½ç•¥ä»–ä»¬ã€‚
3. Code hoisting ï¼ˆä»£ç æå‡ï¼‰
```c
for(i = 0; i < N; i++){
    x[i] = i * 5 * pi;
}
```
æŠŠå¸¸é‡æå‡ºæ¥ï¼Œé˜²æ­¢é‡å¤è®¡ç®—ğŸ‘‡ã€‚!: è¿‡åº¦ä½¿ç”¨å¯èƒ½ä¼šå¯¼è‡´å¯„å­˜å™¨æº¢å‡º
```c
v = 5 * pi;
for(i = 0; i < N; i++){
x[i] = i * v;
}
```

4. Common Sub-expression
```c
y = a * log(x) + pow(log(x), 2);
```
ğŸ‘‡
```c
v = log(x);
y = a * v + pow(v, 2);
```
5. Loop unrolling
```c
for(i = 0; i < N; i++){
x[i] = i * 5 * pi;
}
```
ğŸ‘‡
```c
x[1] = 1 * 5 * pi;
x[2] = 2 * 5 * pi;
x[3] = 3 * 5 * pi;
...
x[N] = N * 5 * pi;
```

ä»¥ä¸Šä¼˜åŒ–æ–¹å¼éƒ½æ˜¯åŸºäºæ—¶ç©ºäº¤æ¢ï¼ˆtime-space trade-offï¼‰ã€‚é€šå¸¸æƒ³èŠ‚çœæ‰§è¡Œæ—¶é—´å°±è¦å¢åŠ ä»£ç ä½“é‡ã€‚é€šå¸¸ä¼˜åŒ–ç¨‹åº¦è¶Šé«˜ï¼Œç¼–è¯‘æ—¶é—´è¶Šé•¿ï¼Œå¯æ‰§è¡Œæ–‡ä»¶è¶Šå¤§ã€‚
## ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
Compilerçš„ä¼˜åŒ–æŒ‡ä»¤
- -O0/-O ç¦ç”¨æ‰€æœ‰ä¼˜åŒ–
- -O1 ä½¿ç”¨æœ€ç®€å•çš„ä¼˜åŒ–æ–¹æ³•
- -O2 æ‰€æœ‰O1çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå†åŠ ä¸€äº›æ›´é«˜çº§çš„ä¼˜åŒ–æ–¹æ³•ï¼Œè¿™é‡Œå¼€å§‹å‡ºç°æ—¶ç©ºäº¤æ¢çš„ä¼˜åŒ–æ–¹æ³•ã€‚Recommended
- -O3 æ¯”O2æ›´å¼ºåŠ²ï¼Œæ¶‰åŠå¤§é‡çš„æ—¶ç©ºäº¤æ¢æ–¹æ³•ï¼Œç¼–è¯‘æ—¶é—´æ˜¾è‘—å¢åŠ ï¼Œå»ºè®®ç”¨äºæœ‰å¯†é›†æµ®ç‚¹è¿ç®—å¾ªç¯çš„ä»£ç 
- -Os é’ˆå¯¹å¯æ‰§è¡Œæ–‡ä»¶çš„å¤§å°è¿›è¡Œä¼˜åŒ–
- -O2-no-vec æ²¡æœ‰vectorisationçš„O2ä¼˜åŒ–

**Intelçš„ç”ŸæˆæŠ¥å‘ŠæŒ‡ä»¤**

-qopt-reportNï¼ŒN=0ï¼Œ1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œ5ã€‚0è¡¨ç¤ºæ²¡æœ‰æŠ¥å‘Šï¼Œ5è¡¨ç¤ºæœ€è¯¦å°½çš„æŠ¥å‘Š
```shell
icc program.c -qopt-report3
```
## åˆ©ç”¨Profiling codeç¡®å®šä¼˜åŒ–ä½ç½®
Profiling: æµ‹é‡ç¨‹åºçš„è¡Œä¸ºå’Œæ€§èƒ½ï¼ŒåŒ…æ‹¬è¿è¡Œæ—¶å’Œèµ„æºåˆ©ç”¨æƒ…å†µã€‚å¯¹ç¨‹åºè¿›è¡Œç»†åˆ†å¹¶æ‰¾åˆ°çƒ­ç‚¹éƒ¨åˆ†ï¼Œå¯¹çƒ­ç‚¹éƒ¨åˆ†è¿›è¡Œä¼˜åŒ–ã€‚

åˆ†æçƒ­ç‚¹éƒ¨åˆ†ï¼š
- å†…å­˜å¸¦å®½?
- å¯„å­˜å™¨çš„æ•°é‡?
- cacheåˆ©ç”¨ç‡?
- ä»£ç å¤ªçƒ‚?

# Week4
## å¦‚ä½•å®ç°parallelising a program
1. è¯†åˆ«parallelismæœºä¼š
2. é€‰æ‹©parallelismç­–ç•¥
3. ä½¿ç”¨å·¥å…·å’Œåº“(OpenMP, CUDA...)
4. å®ç°+è°ƒè¯•
5. æ€§èƒ½æµ‹è¯•+ä¼˜åŒ–

æŠŠé—®é¢˜æ‹†è§£æˆå¹¶è¡Œç»„ä»¶çš„æ™®éæ–¹æ³•
1. Data parallelism
2. Task parallelism
3. Pipelines
4. Mixed Solutions

## ç²’åº¦
1. ç²—ç²’åº¦parallelism
ç²—ç²’åº¦å¹¶è¡Œæ¶‰åŠè¾ƒå¤§çš„ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«ç›¸å¯¹è¾ƒå¤šçš„è®¡ç®—é‡ã€‚è¿™ç§å¹¶è¡Œåº¦è¾ƒä½ï¼Œå› ä¸ºç¨‹åºè¢«åˆ†è§£æˆè¾ƒå°‘çš„ã€ä½†æ¯ä¸ªéƒ½æ¯”è¾ƒå¤§çš„éƒ¨åˆ†ï¼Œåœ¨å¤šä¸ªå¤„ç†å•å…ƒä¸Šæ‰§è¡Œã€‚ç›¸æ¯”äºç»†ç²’åº¦å¹¶è¡Œï¼Œç²—ç²’åº¦å¹¶è¡Œçš„ç®¡ç†å’Œé€šä¿¡å¼€é”€ç›¸å¯¹è¾ƒä½ï¼Œå› ä¸ºä»»åŠ¡ä¹‹é—´çš„äº¤äº’è¾ƒå°‘ã€‚
2. ç»†ç²’åº¦parallelism
ç»†ç²’åº¦å¹¶è¡ŒæŒ‡çš„æ˜¯ç”±å¾ˆå°çš„ä»»åŠ¡ç»„æˆçš„å¹¶è¡Œè®¡ç®—ï¼Œæ¯ä¸ªä»»åŠ¡æ‰§è¡Œçš„è®¡ç®—é‡ç›¸å¯¹è¾ƒå°‘ã€‚å®ƒå…è®¸é«˜åº¦çš„å¹¶è¡Œåº¦ï¼Œå› ä¸ºç¨‹åºè¢«åˆ†è§£æˆè®¸å¤šå°çš„éƒ¨åˆ†å¯ä»¥åœ¨å¤šä¸ªå¤„ç†å•å…ƒä¸Šå¹¶è¡Œæ‰§è¡Œã€‚ç»†ç²’åº¦å¹¶è¡Œçš„æŒ‘æˆ˜åœ¨äºç®¡ç†å’Œåè°ƒå¤§é‡å°ä»»åŠ¡çš„å¼€é”€å¯èƒ½ä¼šå¾ˆå¤§ï¼Œç‰¹åˆ«æ˜¯å½“é€šä¿¡å’ŒåŒæ­¥æˆæœ¬é«˜äºä»»åŠ¡æ‰§è¡Œæˆæœ¬æ—¶ã€‚
## è¡¡é‡å¹¶å‘æ€§èƒ½
å¹¶è¡Œç¼–ç¨‹æ¨¡å‹
- Shared Memory Programming: åœ¨å…±äº«å†…å­˜ç¼–ç¨‹æ¨¡å‹ä¸­ï¼Œæ‰€æœ‰å¤„ç†å™¨éƒ½è®¿é—®åŒä¸€ä¸ªç‰©ç†å†…å­˜ç©ºé—´ã€‚è¿™æ„å‘³ç€æ‰€æœ‰çš„å¹¶è¡Œæ‰§è¡Œçº¿ç¨‹éƒ½å¯ä»¥ç›´æ¥è¯»å†™åŒä¸€å—å†…å­˜åœ°å€ç©ºé—´ä¸­çš„æ•°æ®ã€‚è¿™ç§æ¨¡å‹ç®€åŒ–äº†æ•°æ®çš„å…±äº«ï¼Œå› ä¸ºä¸éœ€è¦æ˜¾å¼åœ°åœ¨å¤„ç†å™¨ä¹‹é—´ä¼ é€’æ¶ˆæ¯æ¥å…±äº«æ•°æ®ã€‚å…±äº«å†…å­˜æ¨¡å‹é€šå¸¸ç”¨äºå¤šæ ¸å¤„ç†å™¨æˆ–å¤šå¤„ç†å™¨è®¡ç®—æœºç³»ç»Ÿï¼Œå…¶ä¸­æ‰€æœ‰æ ¸å¿ƒéƒ½èƒ½å¤Ÿè®¿é—®åŒä¸€ä¸ªå…¨å±€å†…å­˜ã€‚OpenMPå°±æ˜¯å…±äº«å†…å­˜å¹¶è¡Œç¼–ç¨‹çš„API
- Distributed Memory Programming: åœ¨åˆ†å¸ƒå¼å†…å­˜ç¼–ç¨‹æ¨¡å‹ä¸­ï¼Œæ¯ä¸ªå¤„ç†å™¨æˆ–è®¡ç®—èŠ‚ç‚¹æ‹¥æœ‰è‡ªå·±çš„å±€éƒ¨å†…å­˜ï¼Œå¤„ç†å™¨ä¹‹é—´é€šè¿‡ç½‘ç»œæˆ–æ€»çº¿ä¼ é€’æ¶ˆæ¯æ¥äº¤æ¢æ•°æ®ã€‚è¿™ç§æ¨¡å‹è¦æ±‚æ˜¾å¼åœ°åœ¨ä¸åŒçš„å¤„ç†å™¨ä¹‹é—´å‘é€å’Œæ¥æ”¶æ•°æ®ï¼Œé€šå¸¸ä½¿ç”¨æ¶ˆæ¯ä¼ é€’æ¥å£ï¼ˆå¦‚MPIï¼‰æ¥å®ç°ã€‚åˆ†å¸ƒå¼å†…å­˜æ¨¡å‹é€‚ç”¨äºè®¡ç®—é›†ç¾¤ã€å¤šå¤„ç†å™¨ç³»ç»Ÿæˆ–ç½‘ç»œè¿æ¥çš„è®¡ç®—æœºï¼Œæ¯ä¸ªèŠ‚ç‚¹è¿è¡Œå…¶è¿›ç¨‹å¹¶é€šè¿‡æ¶ˆæ¯ä¼ é€’è¿›è¡Œé€šä¿¡å’Œæ•°æ®å…±äº«ã€‚MPI(Message Passing Interface)å°±æ˜¯åˆ†å¸ƒå¼å†…å­˜å¹¶è¡Œç¼–ç¨‹æ ‡å‡†ï¼Œå¯¹å…±äº«å†…å­˜ä¹Ÿé€‚ç”¨ã€‚

**Scalability and Speedup**
- SpeedupæŒ‡ä¸ç”¨å¹¶è¡Œç¼–ç¨‹è¿è¡Œç¨‹åºæ‰€èŠ±çš„æ—¶é—´å’Œä½¿ç”¨å¹¶è¡Œç¼–ç¨‹è¿è¡Œç¨‹åºæ‰€èŠ±çš„æ—¶é—´çš„æ¯”å€¼
- ScalabilityæŒ‡å¤šæ·»åŠ ä¸€ä¸ªæ ¸/å¤„ç†å™¨çš„æƒ…å†µä¸‹ï¼Œspeedupä¼šæœ‰å¤šå°‘æå‡

- $t_1$: ç¨‹åºåœ¨å•æ ¸ï¼ˆæˆ–å•å¤„ç†å™¨ï¼‰ä¸Šè¿è¡Œçš„æ—¶é—´ã€‚
- $t_p$: ç¨‹åºåœ¨pä¸ªæ ¸å¿ƒï¼ˆæˆ–å¤„ç†å™¨ï¼‰ä¸Šè¿è¡Œçš„æ—¶é—´ã€‚
- $S_p = {t_1\over t_p}$: åŠ é€Ÿæ¯”æ˜¯è¡¡é‡å¹¶è¡Œç¨‹åºç›¸å¯¹äºå…¶é¡ºåºç‰ˆæœ¬çš„æ€§èƒ½æå‡çš„æŒ‡æ ‡ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œå½“ä½ ç”¨pä¸ªå¤„ç†å™¨æ¥è¿è¡Œç¨‹åºæ—¶ï¼Œç¨‹åºçš„æ‰§è¡Œæ—¶é—´ä¼šå˜ä¸ºå•å¤„ç†å™¨ä¸Šçš„$1\over p$ï¼Œç„¶è€Œï¼Œç”±äºé€šä¿¡å’ŒåŒæ­¥å¼€é”€ä»¥åŠä»£ç ä¸­ä¸å¯å¹¶è¡ŒåŒ–çš„éƒ¨åˆ†ï¼Œå®é™…åŠ é€Ÿæ¯”å¾€å¾€ä½äºpã€‚
- å¹¶è¡Œæ•ˆç‡å…¬å¼$e_p = {S_p\over p}$: å¹¶è¡Œæ•ˆç‡æ˜¯è¡¡é‡åŠ é€Ÿæ¯”ç›¸å¯¹äºä½¿ç”¨çš„å¤„ç†å™¨æ•°çš„æ•ˆç‡ã€‚å®ƒæ˜¾ç¤ºäº†å¹¶è¡Œèµ„æºçš„åˆ©ç”¨ç¨‹åº¦ï¼Œé€šå¸¸è¡¨ç¤ºä¸ºç™¾åˆ†æ¯”ã€‚

**Multi-node scaling measurements**
- Weak Scaling: åœ¨å¼±æ‰©å±•æ€§æµ‹è¯•ä¸­ï¼Œéšç€èŠ‚ç‚¹æ•°é‡çš„å¢åŠ ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„å·¥ä½œè´Ÿè½½ä¿æŒä¸å˜ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæ€»ä½“å·¥ä½œè´Ÿè½½éšèŠ‚ç‚¹æ•°çº¿æ€§å¢åŠ ï¼Œæ‰§è¡Œæ—¶é—´ä¿æŒæ’å®šã€‚è¿™æ ·å¯ä»¥æµ‹é‡ç³»ç»Ÿå¢åŠ è®¡ç®—èµ„æºæ—¶ç»´æŒç›¸åŒæ€§èƒ½çš„èƒ½åŠ›ã€‚
- Strong Scaling: å¼ºæ‰©å±•æ€§æ˜¯æŒ‡æ€»ä½“å·¥ä½œè´Ÿè½½ä¿æŒä¸å˜ï¼Œè€ŒèŠ‚ç‚¹æ•°å¢åŠ ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæ‰§è¡Œæ—¶é—´éšèŠ‚ç‚¹æ•°çš„å¢åŠ è€Œå‡å°‘ã€‚è¿™ç§æµ‹é‡ä½“ç°äº†ç³»ç»Ÿå¤„ç†å›ºå®šå¤§å°å·¥ä½œè´Ÿè½½çš„æ•ˆç‡ã€‚

**Amdahlâ€™s Law**
ç¨‹åºçš„æœ€å¿«æ‰§è¡Œé€Ÿåº¦å—é™äºé‚£äº›å¿…é¡»ä¸²è¡Œæ‰§è¡Œçš„ä»£ç éƒ¨åˆ†ã€‚è¿™äº›ä¸²è¡Œéƒ¨åˆ†çš„æ€»æ‰§è¡Œæ—¶é—´è®¾ç½®äº†ç¨‹åºåŠ é€Ÿçš„ä¸‹é™ã€‚æ— è®ºå¹¶è¡Œå¤„ç†å¤šä¹ˆé«˜æ•ˆï¼Œæ€»ä½“æ€§èƒ½æå‡æ°¸è¿œä¸èƒ½è¶…è¿‡è¿™ä¸ªä¸‹é™ã€‚
- $\alpha$åŸå§‹é—®é¢˜ä¸­ä¸²è¡Œéƒ¨åˆ†æ‰€å çš„æ¯”ä¾‹ï¼ˆå°±æ—¶é—´è€Œè¨€ï¼‰
- $t_p = {\alpha\times t_1 + {(1-\alpha)\times t_1\over p}}$
- $S_p = {t_1\over t_p} , limit = {1\over\alpha}$
- æœ€å¤§å¯èƒ½çš„speedupæ˜¯$1\over\alpha$

**Gustafsonâ€™s Law**
Gustafson's Lawçš„å‡ºå‘ç‚¹æ˜¯ï¼Œéšç€å¤„ç†å™¨æ•°é‡çš„å¢åŠ ï¼Œäººä»¬å€¾å‘äºè§£å†³æ›´å¤§è§„æ¨¡çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯ç®€å•åœ°åŠ é€Ÿå›ºå®šå¤§å°çš„é—®é¢˜ã€‚å› æ­¤ï¼Œä»–è®¤ä¸ºï¼š
- æ€»å·¥ä½œé‡çš„å¢åŠ ï¼šéšç€å¤„ç†å™¨æ•°é‡çš„å¢åŠ ï¼Œæˆ‘ä»¬ä¸ä»…ä»…æ˜¯å°†ç›¸åŒçš„ä»»åŠ¡åˆ†é…ç»™æ›´å¤šçš„å¤„ç†å™¨ï¼Œè€Œæ˜¯å¢åŠ äº†æ€»ä½“çš„å·¥ä½œé‡ï¼Œä»¥ä¾¿å¡«æ»¡å¹¶åˆ©ç”¨æ‰€æœ‰å¯ç”¨çš„è®¡ç®—èµ„æºã€‚
- ä¸²è¡Œéƒ¨åˆ†çš„å½±å“å‡å°‘ï¼šå½“æ€»å·¥ä½œé‡å¢åŠ æ—¶ï¼Œç¨‹åºä¸­çš„ä¸²è¡Œéƒ¨åˆ†æ‰€å çš„æ¯”ä¾‹å˜å¾—ä¸é‚£ä¹ˆé‡è¦ï¼Œå› ä¸ºç»å¯¹çš„ä¸²è¡Œå¤„ç†æ—¶é—´ç›¸å¯¹äºæ€»å¤„ç†æ—¶é—´çš„å½±å“å˜å°äº†ã€‚
- å¹¶è¡Œéƒ¨åˆ†çš„å¢åŠ ï¼šä¸æ­¤åŒæ—¶ï¼Œå¯å¹¶è¡ŒåŒ–çš„éƒ¨åˆ†åœ¨æ€»å·¥ä½œé‡ä¸­å æ®äº†æ›´å¤§çš„æ¯”ä¾‹ï¼Œå› ä¸ºè¿™äº›éƒ¨åˆ†å¯ä»¥åœ¨æ‰€æœ‰çš„å¤„ç†å™¨ä¸ŠåŒæ—¶è¿›è¡Œã€‚
- Speedup = $\alpha + p(1-\alpha)$

# Week5
## å¹¶è¡Œè§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§
### Round-off error
åœ¨æ•°å€¼è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œç”±äºå¯¹æ•°å€¼è¿›è¡Œå››èˆäº”å…¥æˆ–æˆªæ–­å¯¼è‡´çš„è¯¯å·®ã€‚å½“æˆ‘ä»¬ä½¿ç”¨æœ‰é™ä½æ•°çš„æ•°å­—æ¥è¡¨ç¤ºå’Œå¤„ç†æ— é™å°æ•°æˆ–è€…éå¸¸å¤§æˆ–éå¸¸å°çš„æ•°æ—¶ï¼Œå°±å¿…é¡»å¯¹è¿™äº›æ•°è¿›è¡Œå››èˆäº”å…¥æˆ–æˆªæ–­ï¼Œä»¥é€‚åº”è®¡ç®—æœºæˆ–è®¡ç®—è®¾å¤‡çš„å­˜å‚¨å’Œå¤„ç†èƒ½åŠ›ã€‚è¿™ç§å¤„ç†è¿‡ç¨‹ä¸­ä¸å¯é¿å…åœ°ä¼šå¼•å…¥ä¸€äº›è¯¯å·®ï¼Œè¿™ç§è¯¯å·®å°±æ˜¯round-off errorã€‚

å…‹æœround-off error: 
1. å°½é‡ç”¨åŒç²¾åº¦æµ®ç‚¹æ•°è€Œä¸ç”¨å•ç²¾åº¦æµ®ç‚¹æ•°
2. è®¾ç½®ä¸€äº›å¿è€åº¦ï¼Œå…è®¸è¯¯å·®å­˜åœ¨ï¼Œä¾‹å¦‚``` if(var1 < var2 - tolerance) ```

**DeadLock and LiveLock**
- æ­»é”æ˜¯æŒ‡ä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸Šçš„è¿›ç¨‹åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå› ä¸ºäº‰å¤ºèµ„æºè€Œé€ æˆçš„ä¸€ç§ç›¸äº’ç­‰å¾…çš„ç°è±¡ï¼Œè‹¥æ— å¤–åŠ›å¹²æ¶‰é‚£å®ƒä»¬éƒ½å°†æ— æ³•å‘å‰æ¨è¿›ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ¶‰åŠçš„è¿›ç¨‹éƒ½åœ¨ç­‰å¾…å…¶ä»–è¿›ç¨‹é‡Šæ”¾èµ„æºï¼Œä½†æ²¡æœ‰ä¸€ä¸ªè¿›ç¨‹èƒ½å¤Ÿå‘å‰æ¨è¿›ï¼Œå› ä¸ºå®ƒä»¬éƒ½åœ¨ç­‰å¾…ã€‚
- æ´»é”æ˜¯æŒ‡ä¸¤ä¸ªæˆ–æ›´å¤šçš„è¿›ç¨‹åœ¨å°è¯•è§£å†³æŸä¸ªé—®é¢˜æ—¶ï¼Œç”±äºå„è¿›ç¨‹çš„ååº”å¯¼è‡´å½¼æ­¤ä¹‹é—´ä¸æ–­åœ°æ”¹å˜çŠ¶æ€ï¼Œä½†å®é™…ä¸Šæ²¡æœ‰ä»»ä½•è¿›å±•ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ¶‰åŠçš„è¿›ç¨‹æ²¡æœ‰è¢«é˜»å¡ï¼Œå®ƒä»¬èƒ½å¤Ÿå“åº”å…¶ä»–äº‹ä»¶ï¼Œä½†æ˜¯è¿™äº›è¿›ç¨‹çš„çŠ¶æ€æ›´æ”¹æ˜¯å¾’åŠ³çš„ï¼Œå› ä¸ºè¿™äº›æ›´æ”¹åªä¼šå¯¼è‡´é—®é¢˜å†æ¬¡å‡ºç°ï¼Œä½¿å¾—ç³»ç»Ÿæ— æ³•ç»§ç»­å‘å‰å‘å±•ã€‚

### Race conditions
ç¨‹åºçš„è¡Œä¸ºä¾èµ–äºæŸäº›äº‹ä»¶æˆ–è¿›ç¨‹çš„é¡ºåºæˆ–æ—¶é—´ã€‚å½“å¤šä¸ªè¿›ç¨‹æˆ–çº¿ç¨‹è®¿é—®å’Œæ”¹å˜å…±äº«æ•°æ®ï¼Œå¹¶ä¸”æœ€ç»ˆç»“æœå–å†³äºè¿™äº›è¿›ç¨‹æˆ–çº¿ç¨‹çš„æ‰§è¡Œé¡ºåºæ—¶ï¼Œå°±å¯èƒ½å‘ç”Ÿç«æ€æ¡ä»¶ã€‚ç®€è€Œè¨€ä¹‹ï¼Œç«æ€æ¡ä»¶å‘ç”Ÿåœ¨ä¸¤ä¸ªæˆ–å¤šä¸ªæ“ä½œå¿…é¡»ä»¥æ­£ç¡®çš„é¡ºåºæ‰§è¡Œï¼Œä½†ç¨‹åºçš„è®¾è®¡æ²¡æœ‰æ­£ç¡®åœ°åºåˆ—åŒ–è¿™äº›æ“ä½œï¼Œå¯¼è‡´ç»“æœä¸å¯é¢„æµ‹ã€‚

**Multiple-write race conditions**
å¤šå†™ç«äº‰æ¡ä»¶å‘ç”Ÿåœ¨å¤šä¸ªçº¿ç¨‹è¯•å›¾åŒæ—¶å†™å…¥åŒä¸€ä¸ªå…±äº«å˜é‡æ—¶ã€‚è¿™ä¼šå¯¼è‡´æ•°æ®ä¸ä¸€è‡´æˆ–å†²çªã€‚è§£å†³æ–¹æ¡ˆå¦‚ä¸‹
- çº¿ç¨‹æœ¬åœ°å˜é‡å‰¯æœ¬ï¼šæ¯ä¸ªçº¿ç¨‹ç»´æŠ¤ä¸€ä¸ªå…±äº«å˜é‡çš„æœ¬åœ°å‰¯æœ¬ï¼Œæœ€ç»ˆè¿›è¡Œæ±‡æ€»ã€‚
- è®¿é—®æ§åˆ¶æœºåˆ¶ï¼šä½¿ç”¨é”ç­‰æœºåˆ¶ä¿æŠ¤å˜é‡ï¼Œä½¿æ¯æ¬¡åªæœ‰ä¸€ä¸ªçº¿ç¨‹å¯ä»¥è®¿é—®ã€‚
- ä½¿ç”¨â€œåŸå­â€æ“ä½œæ”¯æŒï¼šç¼–ç¨‹è¯­è¨€è¿è¡Œæ—¶ã€æ“ä½œç³»ç»Ÿæˆ–ç¡¬ä»¶å¯èƒ½æ”¯æŒâ€œåŸå­â€æ“ä½œï¼Œå³åº•å±‚ç³»ç»Ÿå®ç°äº†è®¿é—®æ§åˆ¶ä»¥é¿å…ç«äº‰æ¡ä»¶ã€‚
- é‡æ–°è®¾è®¡ç®—æ³•ï¼šé€šå¸¸æœ€å¥½çš„è§£å†³æ–¹æ¡ˆæ˜¯é‡æ–°æ€è€ƒç®—æ³•ï¼Œç¡®ä¿é¿å…å‡ºç°ç«äº‰æ¡ä»¶çš„æƒ…å½¢ã€‚

### SIMD
ä¸€ç§å¹¶è¡Œè®¡ç®—çš„æ¶æ„ï¼Œç”¨äºåŒæ—¶åœ¨å¤šä¸ªæ•°æ®ç‚¹ä¸Šæ‰§è¡Œç›¸åŒçš„æ“ä½œã€‚åœ¨ç¡¬ä»¶å±‚æä¾›data-parallelism right

Intelçš„SSEï¼ˆStreaming SIMD Extensionsï¼‰å’ŒAVXï¼ˆAdvanced Vector Extensionsï¼‰æ˜¯é’ˆå¯¹x86å¤„ç†å™¨ç³»åˆ—çš„æŒ‡ä»¤é›†æ‰©å±•ï¼Œä¸“é—¨è®¾è®¡æ¥æé«˜ç‰¹å®šç±»å‹è®¡ç®—çš„æ€§èƒ½ï¼Œä¸»è¦æ˜¯é€šè¿‡SIMDï¼ˆSingle Instruction, Multiple Dataï¼‰æŠ€æœ¯å®ç°çš„ã€‚è¿™äº›æ‰©å±•é€šè¿‡å…è®¸å•ä¸ªæŒ‡ä»¤åŒæ—¶æ“ä½œå¤šä¸ªæ•°æ®ç‚¹ã€‚
### MIMD
åœ¨MIMDä½“ç³»ç»“æ„ä¸­ï¼Œæ¯ä¸ªå¤„ç†å™¨æˆ–å¤„ç†å•å…ƒå¯ä»¥ç‹¬ç«‹æ‰§è¡Œå…¶æŒ‡ä»¤æµï¼Œå¹¶ä¸”å¯ä»¥æ“ä½œè‡ªå·±çš„æ•°æ®ã€‚è¿™ç§è®¾è®¡å…è®¸æ›´å¤§çš„çµæ´»æ€§å’Œæ•ˆç‡ï¼Œå› ä¸ºæ¯ä¸ªå¤„ç†å™¨éƒ½å¯ä»¥æ ¹æ®éœ€è¦ç‹¬ç«‹åœ°æ‰§è¡Œä»»åŠ¡ï¼Œè€Œä¸å¿…ç­‰å¾…å…¶ä»–å¤„ç†å™¨çš„åŒæ­¥ã€‚MIMDæ¶æ„å¯ä»¥æ˜¯å…±äº«å†…å­˜æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æ˜¯åˆ†å¸ƒå¼å†…å­˜æ¨¡å‹ï¼š
- å…±äº«å†…å­˜æ¨¡å‹ï¼šåœ¨è¿™ç§æ¨¡å‹ä¸­ï¼Œæ‰€æœ‰å¤„ç†å™¨éƒ½å¯ä»¥è®¿é—®åŒä¸€ä¸ªå…¨å±€å†…å­˜ã€‚è™½ç„¶è¿™ç®€åŒ–äº†æ•°æ®å…±äº«å’Œé€šä¿¡ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´å†…å­˜è®¿é—®å†²çªå’ŒåŒæ­¥é—®é¢˜ã€‚
- åˆ†å¸ƒå¼å†…å­˜æ¨¡å‹ï¼šæ¯ä¸ªå¤„ç†å™¨æœ‰è‡ªå·±çš„æœ¬åœ°å†…å­˜ï¼Œå¤„ç†å™¨é—´å¿…é¡»é€šè¿‡æ¶ˆæ¯ä¼ é€’æ¥äº¤æ¢ä¿¡æ¯ã€‚è¿™ç§æ¨¡å‹é€šè¿‡å‡å°‘å¯¹å…±äº«èµ„æºçš„ç«äº‰æ¥æé«˜å¯æ‰©å±•æ€§ï¼Œä½†å¢åŠ äº†ç¼–ç¨‹çš„å¤æ‚æ€§ã€‚

# OpenMP
## ä»‹ç»
>ä¸€ä¸ªå¹¶è¡Œç¼–ç¨‹æ¨¡å‹
- Thread-based parallelism
- Targeted at shared memory and also accelerations
    - å…±äº«å†…å­˜ï¼šOpenMP è®¾è®¡ä¸»è¦é’ˆå¯¹å…±äº«å†…å­˜æ¶æ„ï¼Œå…¶ä¸­æ‰€æœ‰çº¿ç¨‹å¯ä»¥è®¿é—®åŒä¸€å†…å­˜åœ°å€ç©ºé—´ï¼Œè¿™ç®€åŒ–äº†æ•°æ®çš„å…±äº«å’Œé€šä¿¡ã€‚
    - åŠ é€Ÿå™¨æ”¯æŒï¼šOpenMP ä¹Ÿæ”¯æŒå„ç§åŠ é€Ÿå™¨ï¼Œå¦‚è‹±ç‰¹å°”çš„ Xeon Phi å’Œå„ç±» GPUã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥ä½¿ç”¨ OpenMP å°†è®¡ç®—ä»»åŠ¡åˆ†å‘åˆ°è¿™äº›é«˜æ€§èƒ½è®¾å¤‡ä¸Šï¼Œè¿›ä¸€æ­¥åŠ é€Ÿç¨‹åºçš„æ‰§è¡Œã€‚
- Parallelisation using work-sharing and tasks
    - å·¥ä½œå…±äº«ï¼šOpenMP é€šè¿‡å·¥ä½œå…±äº«æ„é€ è‡ªåŠ¨åˆ†é…å¾ªç¯çš„è¿­ä»£åˆ°å¤šä¸ªçº¿ç¨‹ä¸Šã€‚ç®€åŒ–äº†å¾ªç¯å¹¶è¡ŒåŒ–çš„è¿‡ç¨‹ã€‚
    - ä»»åŠ¡ï¼šOpenMP çš„ä»»åŠ¡æ„é€ å…è®¸ç¨‹åºå®šä¹‰å¯å¹¶è¡Œæ‰§è¡Œçš„ç‹¬ç«‹ä»»åŠ¡å—ï¼Œæä¾›äº†æ›´ç»†ç²’åº¦çš„å¹¶è¡Œæ§åˆ¶ã€‚
- Possible to have a single source code for serial/parallelï¼‰
    - ä¸€ä¸ªé‡è¦çš„ OpenMP ç‰¹æ€§æ˜¯å®ƒå…è®¸åŒä¸€æºä»£ç ç”¨äºä¸²è¡Œå’Œå¹¶è¡Œæ‰§è¡Œã€‚é€šè¿‡ç®€å•åœ°æ·»åŠ æˆ–ç§»é™¤ç‰¹å®šçš„ç¼–è¯‘å™¨æŒ‡ä»¤ï¼Œç¨‹åºå¯ä»¥åœ¨ä¸åŒçš„æ¨¡å¼ä¹‹é—´åˆ‡æ¢ï¼Œè¿™æå¤§åœ°æé«˜äº†ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œå¯æ‰©å±•æ€§ã€‚
- Only needs different compiler arguments to switch
    - æ˜“äºåˆ‡æ¢ï¼šè¦å¯ç”¨ OpenMP å¹¶è¡Œï¼Œé€šå¸¸åªéœ€å‘ç¼–è¯‘å™¨ä¼ é€’ä¸€ä¸ªå‚æ•°ï¼ˆå¦‚ -fopenmpï¼‰ï¼Œè¿™ä¼šæ¿€æ´»ç¼–è¯‘å™¨ä¸­çš„ OpenMP æ”¯æŒå¹¶å¹¶è¡ŒåŒ–é‚£äº›æ ‡è®°äº† OpenMP æŒ‡ä»¤çš„ä»£ç å—ã€‚è¿™ä½¿å¾—ä»ä¸²è¡Œä»£ç åˆ°å¹¶è¡Œä»£ç çš„è½¬å˜éå¸¸ç®€å•ã€‚

ç¼–è¯‘æŒ‡ä»¤
- Intel ```icc â€“qopenmp myFile.c -o output.exe```
- GNU ```gcc â€“fopenmp myFile.c â€“o output.exe```
- è¿è¡ŒæŒ‡ä»¤ ```OMP_NUM_THREADS=4 ./output.exe```

## The fork-join model
>ä¸€ç§å¸¸è§çš„å¹¶è¡Œè®¾è®¡æ¨¡å¼
- Fork
    - åœ¨â€œforkâ€é˜¶æ®µï¼Œmaster threadåˆ›å»ºä¸€ç³»åˆ—æ–°çš„å¹¶è¡Œå­çº¿ç¨‹ã€‚è¿™äº›å­çº¿ç¨‹å¯ä»¥åŒæ—¶æ‰§è¡Œä»»åŠ¡ï¼Œä¾‹å¦‚å¤„ç†æ•°æ®çš„ä¸åŒéƒ¨åˆ†æˆ–æ‰§è¡Œç‹¬ç«‹çš„è®¡ç®—ä»»åŠ¡ã€‚

- Join
    - åœ¨â€œjoinâ€é˜¶æ®µï¼Œè¿™äº›å¹¶è¡Œæ‰§è¡Œçš„å­çº¿ç¨‹å®Œæˆå®ƒä»¬çš„ä»»åŠ¡åï¼Œä¼šè¢«é‡æ–°æ±‡åˆï¼ˆåŒæ­¥ï¼‰ã€‚master threadåœ¨ç»§ç»­æ‰§è¡Œä¹‹å‰ï¼Œä¼šç­‰å¾…æ‰€æœ‰å­çº¿ç¨‹å®Œæˆã€‚è¿™ä¿è¯äº†æ‰€æœ‰å¹¶è¡Œä»»åŠ¡éƒ½å·²å®Œæˆï¼Œä»è€Œå…è®¸ç¨‹åºæ­£ç¡®åœ°ç»§ç»­æ‰§è¡Œåç»­çš„åºåˆ—æ“ä½œã€‚

æ€ä¹ˆå¼€å§‹ä¸€ä¸ªparallel regionï¼Ÿ
```c
//...serial code
#pragma omp parallel
{
    int numThreads = omp_get_num_threads();
    for(int i = 0; i < numThreads; i++){
        printf(â€œThread %d says Helloâ€, i);
    }
}
//...serial code
```
ä¸€äº›å·¥å…·å‡½æ•°
```c
// å½“å‰å¹¶è¡ŒåŒºåŸŸçš„çº¿ç¨‹æ€»æ•°
omp_get_num_threads();
// å½“å‰çº¿ç¨‹çš„ç¼–å·
omp_get_thread_num();
// å¯¹åç»­å°†è¦åˆ›å»ºçš„å¹¶è¡ŒåŒºåŸŸæ‰€å¯èƒ½ä½¿ç”¨çš„æœ€å¤§çº¿ç¨‹æ•°çš„é¢„ä¼°å€¼ã€‚
omp_get_max_threads();
```
relicationçš„å«ä¹‰
```c
#include <stdio.h>
#include <omp.h>

int main() {
    int x = 10; 

    #pragma omp parallel private(x)
    {
        x = omp_get_thread_num(); 
        printf("Thread %d has x = %d\n", omp_get_thread_num(), x);
    }
    // é€€å‡ºå¹¶è¡ŒåŒºåŸŸåï¼ŒåŸå§‹çš„xå€¼ä¸å˜ï¼Œæ¯ä¸ªçº¿ç¨‹çš„ä¿®æ”¹ä»…é™äºå…¶å‰¯æœ¬
    printf("Outside parallel region, x = %d\n", x);
    return 0;
}
```
è¾“å‡ºå¦‚ä¸‹
```shell
$ ./replication
Thread 2 has x = 2
Thread 3 has x = 3
Thread 1 has x = 1
Thread 4 has x = 4
Thread 5 has x = 5
Thread 0 has x = 0
Thread 6 has x = 6
Thread 7 has x = 7
Outside parallel region, x = 10
```
## Clauses
**Data Clauses** (parallel regionä¸­æœ‰ä¸åŒèŒƒå›´çš„å˜é‡)
- shared(x)ï¼šåœ¨parallel regionå¤–å£°æ˜ï¼Œæ‰€æœ‰çº¿ç¨‹éšå¼å…±äº«è¿™ä¸ªå˜é‡
- private(x)ï¼šåœ¨parallel regionå†…å£°æ˜ï¼Œåªæœ‰regionå†…çš„çº¿ç¨‹èƒ½ç”¨è¿™ä¸ªå˜é‡
- firstprivate(x): ç”¨äºç¡®ä¿æ¯ä¸ªçº¿ç¨‹éƒ½æœ‰å…¶è‡ªå·±çš„å˜é‡å‰¯æœ¬ï¼Œå¹¶ä¸”è¿™äº›å‰¯æœ¬åœ¨è¿›å…¥å¹¶è¡ŒåŒºåŸŸæ—¶ä½¿ç”¨å¤–éƒ¨ä½œç”¨åŸŸä¸­ç›¸åº”å˜é‡çš„åˆå§‹å€¼è¿›è¡Œåˆå§‹åŒ–ã€‚è¿™æ„å‘³ç€æ¯ä¸ªçº¿ç¨‹åœ¨å¼€å§‹æ‰§è¡Œå¹¶è¡Œä»£ç å—ä¹‹å‰ï¼Œéƒ½ä¼šä»ä¸»çº¿ç¨‹ï¼ˆæˆ–å¤–éƒ¨ä½œç”¨åŸŸï¼‰é‚£é‡Œç»§æ‰¿å˜é‡çš„å€¼ã€‚
- lastprivate(x): ç”¨äºæŒ‡å®šåœ¨å¹¶è¡ŒåŒºåŸŸç»“æŸåï¼ŒæŸä¸ªå˜é‡çš„å€¼åº”è¯¥æ˜¯ç”±æ‰§è¡Œæœ€åä¸€æ¬¡è¿­ä»£ï¼ˆæˆ–æœ€åä¸€ä¸ªåŒºå—ï¼‰çš„çº¿ç¨‹ä¸­çš„å¯¹åº”å˜é‡å€¼æ¥æ›´æ–°çš„ã€‚è¿™ç¡®ä¿äº†å˜é‡åœ¨å¹¶è¡ŒåŒºåŸŸå®Œæˆåèƒ½å¤Ÿä¿ç•™æœ€åä¸€æ¬¡è¿­ä»£ä¸­çš„å€¼ã€‚

åœ¨ä½¿ç”¨OpenMPè¿›è¡Œå¹¶è¡Œè®¡ç®—æ—¶ï¼Œå·¥ä½œè°ƒåº¦çš„æ–¹å¼å¯¹æ€§èƒ½çš„å½±å“å¾ˆå¤§ã€‚

### Barrier
åœ¨OpenMPä¸­ï¼Œbarrier æ˜¯ä¸€ä¸ªåŒæ­¥åŸè¯­ï¼Œç”¨äºç¡®ä¿åœ¨è¯¥ç‚¹ä¹‹å‰çš„æ‰€æœ‰çº¿ç¨‹éƒ½å®Œæˆå…¶å·¥ä½œæ‰èƒ½ç»§ç»­æ‰§è¡Œåç»­çš„ä»£ç ã€‚ç®€å•æ¥è¯´ï¼Œbarrier åˆ›å»ºäº†ä¸€ä¸ªåŒæ­¥ç‚¹ï¼Œåœ¨è¿™ä¸ªç‚¹ä¸Šï¼Œæ‰€æœ‰çš„çº¿ç¨‹éƒ½å¿…é¡»åˆ°è¾¾ï¼Œç„¶åæ‰èƒ½ä¸€èµ·ç»§ç»­æ‰§è¡Œåé¢çš„æŒ‡ä»¤ã€‚
- è‡ªåŠ¨å±éšœï¼šåœ¨æŸäº›æŒ‡ä»¤ï¼Œå¦‚ ```#pragma omp for``` æˆ– ```#pragma omp sections``` çš„æœ«å°¾ï¼ŒOpenMPä¼šè‡ªåŠ¨æ’å…¥å±éšœã€‚é™¤éä½¿ç”¨ ```nowait``` å­å¥ï¼Œå¦åˆ™æ‰€æœ‰çº¿ç¨‹åœ¨ç»§ç»­ä¹‹å‰éƒ½å¿…é¡»å®Œæˆå®ƒä»¬çš„éƒ¨åˆ†ã€‚ä¾‹å¦‚```#pragma omp for nowait```
- æ˜¾å¼å±éšœï¼šå¯ä»¥é€šè¿‡ ```#pragma omp barrier``` æ˜¾å¼åœ°åœ¨ä»£ç ä¸­æ’å…¥å±éšœã€‚

### NUMA
Non-Uniform Memory Accessï¼ˆNUMAï¼‰æ˜¯ä¸€ç§è®¡ç®—æœºå†…å­˜è®¾è®¡ï¼Œç”¨äºå¤šå¤„ç†å™¨ç³»ç»Ÿï¼Œåœ¨è¯¥è®¾è®¡ä¸­ï¼Œæ¯ä¸ªå¤„ç†å™¨éƒ½æœ‰è‡ªå·±çš„æœ¬åœ°å†…å­˜ã€‚å¤„ç†å™¨å¯ä»¥ç›´æ¥è®¿é—®æœ¬åœ°å†…å­˜ï¼ˆæœ¬åœ°è®¿é—®ï¼‰æ¯”è®¿é—®å¦ä¸€ä¸ªå¤„ç†å™¨çš„å†…å­˜ï¼ˆè¿œç¨‹è®¿é—®ï¼‰æ›´å¿«ï¼Œå› ä¸ºè¿œç¨‹è®¿é—®éœ€è¦é€šè¿‡ä¸€äº›å†…å­˜æ€»çº¿æˆ–äº¤æ¢æœºç­‰æ›´å¤æ‚çš„è¿æ¥ç»“æ„ã€‚å› æ­¤ï¼Œåœ¨NUMAæ¶æ„ä¸­ï¼Œå†…å­˜è®¿é—®æ—¶é—´å–å†³äºå†…å­˜ä½ç½®ç›¸å¯¹äºå¤„ç†å™¨çš„ä½ç½®ã€‚

**first touch in NUMA**
åœ¨NUMAï¼ˆNon-Uniform Memory Accessï¼‰ç³»ç»Ÿä¸­ï¼Œ"first touch" æ˜¯ä¸€ä¸ªé¡µé¢çº§å†…å­˜ç®¡ç†ç­–ç•¥ï¼Œè¿™ä¸€ç­–ç•¥å½±å“å†…å­˜åˆ†é…çš„ä½ç½®ã€‚è¿™ä¸ªåŸåˆ™çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ“ä½œç³»ç»Ÿå°†å†…å­˜é¡µé¢åˆ†é…ç»™è§¦ç¢°å®ƒçš„ç¬¬ä¸€ä¸ªå¤„ç†å™¨çš„æœ¬åœ°å†…å­˜èŠ‚ç‚¹ã€‚"è§¦ç¢°"é€šå¸¸æ˜¯æŒ‡å¯¹å†…å­˜é¡µé¢è¿›è¡Œå†™æ“ä½œã€‚

ä¸¾ä¾‹
```c
#pragma omp parallel for
for (int i = 0; i < large_array_size; i++) {
    large_array[i] = initial_value; // First touch happens here
}

```
æ•°ç»„çš„æ¯ä¸ªéƒ¨åˆ†ç”±é¦–æ¬¡å†™å…¥æ•°æ®çš„çº¿ç¨‹è¿›è¡Œåˆå§‹åŒ–ï¼Œå› æ­¤å†…å­˜é¡µé¢å°†è¢«åˆ†é…åˆ°ä¸è¯¥çº¿ç¨‹ç›¸å¯¹åº”çš„NUMAèŠ‚ç‚¹ä¸Šã€‚è¿™æ ·ï¼Œå½“æ•°ç»„è¢«åç»­çš„è®¡ç®—è®¿é—®æ—¶ï¼Œæ¯ä¸ªçº¿ç¨‹å¤§æ¦‚ç‡è®¿é—®çš„æ˜¯å…¶æœ¬åœ°èŠ‚ç‚¹çš„å†…å­˜é¡µé¢ï¼Œå‡å°‘äº†è¿œç¨‹å†…å­˜è®¿é—®ï¼Œæé«˜äº†æ€§èƒ½ã€‚

# MPI
ä¸€äº›Vocabulary
- Process: "Process" æ˜¯å¹¶è¡Œè®¡ç®—çš„åŸºæœ¬å•ä½ï¼Œå¯ä»¥è¢«ç†è§£ä¸ºæ‰§è¡Œ MPI ç¨‹åºçš„ä¸€ä¸ªç‹¬ç«‹å®ä½“ã€‚æ¯ä¸ª MPI è¿›ç¨‹éƒ½æœ‰è‡ªå·±çš„åœ°å€ç©ºé—´ï¼ˆå³ç‹¬ç«‹çš„å†…å­˜ç©ºé—´ï¼‰ï¼Œè¿™æ„å‘³ç€è¿›ç¨‹é—´ä¸å…±äº«å†…å­˜ã€‚å› æ­¤ï¼ŒMPI è¿›ç¨‹ä¹‹é—´å¿…é¡»é€šè¿‡æ¶ˆæ¯ä¼ é€’æ¥äº¤æ¢æ•°æ®ï¼Œè¿™æ˜¯ MPI å¹¶è¡Œç¼–ç¨‹æ¨¡å‹çš„æ ¸å¿ƒã€‚
- Communicator: "Communicator" ç”¨äºå®šä¹‰ä¸€ç»„å¯ä»¥ç›¸äº’é€šä¿¡çš„è¿›ç¨‹ã€‚Communicator ç¡®å®šäº†å“ªäº›è¿›ç¨‹å‚ä¸åˆ°ç‰¹å®šçš„é€šä¿¡ä¸­ï¼Œå®ƒä¸ºè¿™äº›è¿›ç¨‹æä¾›äº†ä¸€ä¸ªä¸Šä¸‹æ–‡ï¼Œåœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„æ ‡è¯†ç¬¦ï¼ˆrankï¼‰ã€‚è¿™æ ·ï¼ŒCommunicator ä¸ä»…å®šä¹‰äº†é€šä¿¡çš„å‚ä¸è€…ï¼Œè¿˜å®šä¹‰äº†å®ƒä»¬ä¹‹é—´çš„é€šä¿¡æ–¹å¼ã€‚
- Rank: æ¯ä¸ªè¿›ç¨‹çš„rankæ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æ¦‚å¿µï¼Œå®ƒæŒ‡çš„æ˜¯åœ¨æ‰€æœ‰å‚ä¸é€šä¿¡çš„è¿›ç¨‹ä¸­æ¯ä¸ªè¿›ç¨‹çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚Rankæ˜¯ä¸€ä¸ªä»0å¼€å§‹çš„æ•´æ•°ï¼Œç”¨äºåœ¨è¿›ç¨‹ç¾¤ï¼ˆé€šå¸¸æ˜¯åœ¨MPI_COMM_WORLDè¿™ä¸ªå…¨å±€é€šä¿¡å™¨ä¸­ï¼‰å†…å”¯ä¸€æ ‡è¯†æ¯ä¸ªè¿›ç¨‹ã€‚
## How to compile and run MPI code
```c
#include <stdio.h>
#include <mpi.h>
int main(void) {
    int myID;
    // initialize MPI environment
    MPI_Init(NULL, NULL);
    // assign ranks
    MPI_Comm_rank(MPI_COMM_WORLD, &myID);
    printf(â€œHi from process ranked %d\nâ€, myID);
    MPI_Finalize();
}
```
ç¼–è¯‘
```shell
# For Intel
$ mpiicc <filename>.c
# For GNU
$ mpicc <filename>.c
```
è¿è¡Œ
```shell
# Intel and GNU are same
$ mpirun â€“np <num_process> <executable_file>
```
MPIçš„ä¸¤ä¸ªé‡è¦å‡½æ•°```MPI_Send()```å’Œ```MPI_Recv()```
```c
int MPI_Send(
    const void* buf,       // æŒ‡å‘å‘é€æ•°æ®ç¼“å†²åŒºçš„æŒ‡é’ˆ
    int count,             // è¦å‘é€çš„æ•°æ®å…ƒç´ æ•°é‡
    MPI_Datatype datatype, // æ•°æ®å…ƒç´ çš„ç±»å‹
    int dest,              // ç›®æ ‡è¿›ç¨‹çš„rank
    int tag,               // æ¶ˆæ¯æ ‡ç­¾ï¼Œç”¨äºåŒºåˆ†ä¸åŒç±»å‹çš„æ¶ˆæ¯
    MPI_Comm comm          // ä½¿ç”¨çš„é€šä¿¡å™¨
);

int MPI_Recv(
    void* buf,             // æŒ‡å‘æ¥æ”¶æ•°æ®ç¼“å†²åŒºçš„æŒ‡é’ˆ
    int count,             // èƒ½å¤Ÿæ¥æ”¶çš„æœ€å¤§æ•°æ®å…ƒç´ æ•°é‡
    MPI_Datatype datatype, // æ•°æ®å…ƒç´ çš„ç±»å‹
    int source,            // æºè¿›ç¨‹çš„rank
    int tag,               // æ¶ˆæ¯æ ‡ç­¾
    MPI_Comm comm,         // ä½¿ç”¨çš„é€šä¿¡å™¨
    MPI_Status* status     // ç”¨äºè¿”å›å…³äºæ¥æ”¶çš„æ¶ˆæ¯çš„çŠ¶æ€ä¿¡æ¯
);
```
## Blocking Communications
é˜»å¡é€šä¿¡æŒ‡çš„æ˜¯åœ¨é€šä¿¡æ“ä½œï¼ˆå¦‚å‘é€æˆ–æ¥æ”¶æ¶ˆæ¯ï¼‰å®Œæˆä¹‹å‰ï¼Œå‘èµ·é€šä¿¡çš„è¿›ç¨‹ä¼šè¢«æš‚åœï¼ˆé˜»å¡ï¼‰ã€‚
- **MPI_Send**ï¼šåœ¨æ¶ˆæ¯å®Œå…¨å‘é€åˆ°æ¥æ”¶ç¼“å†²åŒºä¹‹å‰ï¼Œå‘é€æ“ä½œä¸ä¼šå®Œæˆï¼Œå‘é€æ–¹è¿›ç¨‹ä¼šè¢«é˜»å¡ã€‚
- **MPI_Recv**ï¼šå¦‚æœæ²¡æœ‰æ•°æ®åˆ°è¾¾ï¼Œæ¥æ”¶æ–¹è¿›ç¨‹å°†ç­‰å¾…ï¼Œç›´åˆ°æ¶ˆæ¯åˆ°è¾¾å¹¶è¢«å®Œå…¨æ¥æ”¶åæ‰ç»§ç»­æ‰§è¡Œã€‚
é˜»å¡é€šä¿¡ç®€åŒ–äº†ç¨‹åºçš„ç¼–å†™ï¼Œå› ä¸ºä½ å¯ä»¥ç¡®ä¿¡åœ¨è°ƒç”¨è¿”å›åé€šä¿¡å·²ç»å®Œæˆã€‚ä½†æ˜¯ï¼Œè¿™ç§æ–¹å¼å¯èƒ½ä¼šå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—ä»»åŠ¡ä¸­ï¼Œå› ä¸ºå®ƒå¯èƒ½å¯¼è‡´èµ„æºçš„é—²ç½®ã€‚
## Non-blocking Communications
éé˜»å¡é€šä¿¡å…è®¸ä¸€ä¸ªè¿›ç¨‹å‘èµ·ä¸€ä¸ªé€šä¿¡æ“ä½œåç«‹å³ç»§ç»­æ‰§è¡Œå…¶ä»–å·¥ä½œï¼Œè€Œä¸å¿…ç­‰å¾…é€šä¿¡æ“ä½œå®Œæˆã€‚è¿™ç§æ–¹å¼å¯ä»¥æé«˜åº”ç”¨ç¨‹åºçš„å¹¶è¡Œæ€§ã€‚
- **MPI_Isend**ï¼šå¯åŠ¨ä¸€ä¸ªå‘é€æ“ä½œï¼Œä½†ç«‹å³è¿”å›ï¼Œè®©å‘é€è¿›ç¨‹å¯ä»¥ç»§ç»­æ‰§è¡Œå…¶ä»–è®¡ç®—æˆ–å‘èµ·æ›´å¤šçš„éé˜»å¡é€šä¿¡ã€‚
- **MPI_Irecv**ï¼šå¼€å§‹æ¥æ”¶æ“ä½œï¼ŒåŒæ—¶å…è®¸è¿›ç¨‹æ‰§è¡Œå…¶ä»–ä»»åŠ¡ï¼Œç›´åˆ°æ•°æ®åˆ°è¾¾ã€‚
# Lab Tips
> è‹±æ–‡è®°å½•äº†ï¼Œæ–¹ä¾¿ç­”é¢˜ã€‚
## Lab02
1. What is the difference between **usigned int** and **int**
    - **Unsigned int** only allows natural numbers, which means it can store larger positive integers.
2. If you were to parallelise this program, and divide the problem into different iterations, what may cause an issue in parallel performance?
    - Load imbalance would affect this program. Some iterations take longer which means some threads will complete earlier, and wait (do nothing).

## Lab03
1. What optimisation levels do you think contributed most to the decrease in execution time?
    - O2 seemed to produce the biggest decrease in execution time. Even though O1 decreased it the most from the previous (roughly 6000ms to 1400ms) O2 achieved roughly 200ms. This would be because of vectorisation which O2 performs, but not O1. O3 did not reduce the time much after this.
2. Why, as the number of cores increased, do you think the scalability began to decrease?
    - Multiple reasons, one could be resource contention as more threads begin to access memory locations used by other threads. But considering that mkl is meant to be heavily optimised reducing this, the more likely reason would be Amdahl's law, as the number of threads increase, the serial portion of the program begins to dominate compared to the parallel time.
## Lab04
1. Avoiding excessive synchronization in programming is important for several reasons, primarily related to performance and complexity:
    - Performance Degradation: Synchronization often means that some operations must wait for others to complete, leading to thread blocking. This waiting reduces the efficiency of program execution. In high-performance computing or applications where response time is critical, such delays are unacceptable.
    - Risk of Deadlocks: Improper synchronization can lead to deadlocks, where multiple processes or threads wait indefinitely for each other to release resources, thus halting their execution. Deadlocks are not only difficult to debug but can also cause the program to stop responding entirely.
    - Increased Programming Complexity: Implementing and maintaining synchronization mechanisms requires careful design to ensure all threads or processes acquire and release resources at the correct times. This not only complicates the code but also makes debugging and testing more challenging.
    - Scalability Issues: Over-reliance on synchronization can limit a programâ€™s scalability on multicore or multiprocessor systems. Synchronization restricts the possibility of parallel execution, potentially underutilizing multiple cores or processors.
2. Use mutex to remove race condition
    - ``` c
        void* mutex_testing(void* param){
            int i;
            for (i = 0; i < 5; i++) {
                pthread_mutex_lock(&myMutex);/*lock..*/
                counter++;
                printf("thread %d counter = %d\n", (int)param, counter);
                pthread_mutex_unlock(&myMutex); /*unlock..*/
            }
        }

        int main() {
            int one = 1, two = 2, three = 3;
            pthread_t thread1, thread2, thread3;
            pthread_mutex_init(&myMutex, 0);

            pthread_create(&thread1, 0, mutex_testing, (void*)one);
            pthread_create(&thread2, 0, mutex_testing, (void*)two);
            pthread_create(&thread3, 0, mutex_testing, (void*)three);
            pthread_join(thread1, 0);
            pthread_join(thread2, 0);
            pthread_join(thread3, 0);

            pthread_mutex_destroy(&myMutex);
            return 0;
        }
        ```
    - Mechanism for Removing Race Conditions
        - Locking (pthread_mutex_lock): Before each modification of counter, the thread attempts to acquire the mutex. If the mutex is already held by another thread, the current thread will block until the mutex is released.
        - Unlocking (pthread_mutex_unlock): After modifying counter, the thread releases the mutex, allowing other waiting threads to acquire the lock and proceed with their execution.
    - Trade-offs and Consequences
        - Performance Impact: The use of mutexes introduces synchronization overhead. Acquiring and releasing locks takes time, especially in high-contention environments, where frequent locking operations can lead to significant performance bottlenecks.
        - Reduced Scalability: As the number of threads increases, contention for locks can become more frequent, further impacting the program's scalability. Mutexes can become a bottleneck in scenarios where many threads try to access the same resource, limiting the programâ€™s ability to scale on multiprocessor systems.
        - Risk of Deadlocks: Although unlikely in this simple example, improper use of locks in more complex applications (such as acquiring multiple locks in an incorrect order) can lead to deadlocks, causing the program to hang or crash.
## Lab05
1. Difference between replication and workshare
    - **Replication** refers to duplicating the same computational tasks across multiple processors or computing nodes. In this strategy, every node or processor performs exactly the same code and operations, typically used to enhance fault tolerance and reliability. For example, in a high-availability system, multiple servers might replicate the same request processing tasks to ensure that if one server fails, others can seamlessly take over, thereby improving the overall availability of the system.
    - **Workshare** involves dividing a larger task into smaller chunks, which are then distributed among multiple processors or nodes for parallel execution. Each processor is responsible for a portion of the overall task. This method effectively utilizes the parallel processing capabilities of multicore or multi-node systems to enhance computational efficiency. For example, in parallel loops, iterations of the loop can be distributed among several processors, with each processor handling a segment of the iterations.
## Lab06
1. What is race condition?
    - A condition where the program's behaviour changes depending on the sequence or timing of events outside the control of the program, non-deterministic results.
2. What is false sharing?
    - False sharing occurs when 2 threads are accessing the same cache line. When one thread modifies a variable in the cache line, the cache line is considered stale due to how cache lines are designed for serial code. Because of this, data can be stored in.

[{"content":"Week1 高性能计算的目标\n对于有限的数据集，最小化解决时间 对于无限的数据集，最大化吞吐量(throughput) 有能力解决一些对于可用的内存来说太大的问题 最大化资源利用 - CPU/内存/网络/加速器(GPU)/电力 一些常用术语 Parallelism vs Concurrency Parallelism: 多个进程同时且独立执行 Concurrency: 多个进程同时执行且共享至少一种资源 Processor, Die \u0026amp; Socket Processor: 执行程序指令的电路。计算机系统中可能有许多处理器，例如图形处理器、视频处理器。在没有限定的情况下，通常指中央处理器 CPU: 计算机系统中主要的通用处理器（之一），而非特定用途（如视频解压缩）。 Die: 指硅晶片，包含处理器（通常是中央处理器）以及接口所需的其他组件（如内存控制器）。 Socket: 处理器和计算机主板之间的物理接口，它定义了处理器与主板连接的方式。不同的处理器和主板可能需要不同类型的Socket。 Core \u0026amp; Thread Core: 核心是CPU内部的一个物理处理单元，能够独立执行计算任务。每个核心可以独立处理指令和执行计算操作。 Thread: 线程是操作系统能够进行计算调度的最小单位。它是程序执行流的一个单一顺序，可以被操作系统调度（启动、停止、挂起等）。 核心和线程共同定义了处理器的处理能力 Node 指一个服务器节点（一台计算机） Cluster/Supercomputer 成百上千个节点组成集群 Single precision floating-point 通常占用32位（4字节）的存储空间，C语言中的float32类型，1符号位，8指数位，23有效数字位 Double precision floating-point 通常占用64位（8字节）的存储空间，C语言中的float64类型，1符号位，11指数位，52有效数字位 Flop Floating-point operations per second 如何量化/评估性能 公式如下👇\n$R_{peak} = 2\\times w_{vec} \\times r_{clock} \\times n_{core} \\times n_{socket}$\n变量解释：\n$R_{peak} - $峰值理论性能 $w_{vec} - $向量宽度，表示每个处理器核心每个时钟周期内可以执行的浮点运算数。 $r_{clock} - $表示处理器核心每秒钟可以执行的时钟周期数。 $n_{core} - $表示单个处理器（CPU）或计算节点中的核心数量。 $n_{socket} - $表示系统中处理器（CPU）的数量。 Linpack性能测试：Linpack性能测试是一种衡量计算系统解决高密度线性代数问题能力的测试。这个测试通过测量系统在执行大规模双精度（64位）浮点算术矩阵运算时的性能来评估计算机的速度和效率。Linpack测试的一个典型应用是计算给定大小的矩阵$A$和向量$b$，求解线性方程组$Ax = b$\nWeek2 Plot programs performance 用\u0026quot;Arithmetic Intensity\u0026quot;，记作$I(n)$，来评估性能。$I(n) = {W(n)\\over Q(n)}$。\n$W(n)$:程序执行flops的次数，$Q(n)$:从内存传输到缓存的字节数。\n低Arithmetic Intensity的程序叫内存受限程序 高Arithmetic Intensity的程序叫计算受限程序 对于内存受限的程序，处理器需要花费更多时间等待操作数从内存中传送出来（更多的时间花费在访问内存上，而不是运算上）。 Roofline Model Roofline Model是帮助了解软件性能的可视化工具。\nRoofline Model两个组成部分\n峰值性能（$R_{peak} / R_{max}$）：这是计算系统在理想情况下可以达到的最高计算性能，通常以每秒浮点运算次数（FLOPS）来衡量。峰值性能由处理器的硬件特性决定，比如核心数量、时钟频率和向量化能力。图中水平线就是峰值性能 内存带宽（Memory Bandwidth）：这是计算系统在单位时间内能从内存中读写数据的最大速率，通常以每秒传输的字节数（Bytes/s）来衡量。内存带宽是由系统的内存架构和内存类型决定的。带角度的斜线表示内存带宽 冯·诺伊曼 Von Neumann架构图如下所示👇 由控制单元和算术/逻辑单元组成的CPU。 独立的存储区，可存储指令和数据。 指令由CPU执行，因此必须将指令从存储器带入CPU。 数据也必须从存储器进入CPU才能执行。 CPU包含寄存器，作为临时存储的刮板。 The von Neumann bottleneck: 数据和指令共用一条总线，因此指令获取和数据操作不能同时进行。 冯·诺伊曼瓶颈 从内存中抓取对应程序计数器的指令 解码指令 从内存中抓取数据 执行指令 写回结果 Pipelining Pipelining的基本概念\n流水线技术通过将指令的执行过程分解为多个阶段，并让不同的指令在不同的时间并行处理这些阶段来提高处理速度。这就好比是在组装线上，每个工人负责组装线上的一个特定任务，产品可以更快地完成，因为多个任务是在同时进行，而不是一个接一个地完成。\n在von neumann cycle中的应用\n在应用流水线技术后，处理器可以在完成当前指令的某个阶段的同时，开始执行下一条指令的前一个阶段。例如，当第一条指令在执行阶段时，第二条指令可以同时进行译码，第三条指令可以进行取指。这样，虽然每条指令的执行仍然需要串行经过所有阶段，但处理器可以在同一时刻处理多条指令的不同阶段，从而大大提高了指令的吞吐率。\n对抗冯·诺伊曼瓶颈的方法 在芯片上添加高速缓存（cache)，但高速缓存也存在问题\n例如，高速缓存越大，数据访问速度越慢。可以采用多级缓存架构，通过在处理器和主内存之间引入多个层级的缓存，旨在平衡缓存大小、访问速度和命中率之间的关系。\n高速缓存利用了程序的空间局部性（spatial locality）和时间局部性（temporal locality）。\n时间局部性指的是在较短的时间内，被访问过一次的数据项很可能在不久的将来再次被访问的特性。这种访问模式意味着一旦数据被加载到缓存中，它很可能很快再次被需要，因此保留这些数据项在缓存中可以减少对较慢主存的访问次数。 空间局部性是指如果一个数据项被访问，那么其附近的数据项很快也可能被访问的特性。这种模式基于数据存储的物理结构，相邻的数据项通常也在内存中相邻存储。高速缓存系统利用这一特性通过预取附近的数据项到缓存中，即使这些数据项还没有被显式请求。 AMD Bulldozer 服务器插槽的内存层次结构 How to gain performance form a single core/socket for (int i = 0; i\u0026lt;1000; i++) { b[i] = a[i]*a[i]; } 对于上面这个程序，应该运行1000个clock cycles。假如一个clock cycle不是做一次迭代，而是做4次迭代，那么总共需要250个clock cycles。这就叫vector processing，也叫Single Instruction, Multiple Data（SIMD）。\n在单核上压榨更多性能：SMT(simultaneous multithreading)，通过在单个物理CPU核心上同时执行多个线程来提高处理器的效率和性能。SMT允许单个核心像操作系统和应用程序呈现出多个逻辑核心或线程，使得处理器可以更有效地利用其资源，特别是在一个线程等待数据访问或执行长时间操作时，处理器可以转而执行另一个线程的任务。\nWeek3 编译器优化的常用方法 两种优化方式：\n时间优化 空间优化 编译器会自动的做一些优化。\nvon Neumann cycle中的执行阶段也需要对内存进行读写。所有的算术操作都需要读写交替进行。编译器的工作就是给特定硬件确定合理的交错顺序。\n读数据的方式对性能表现来说很重要（cache的存在就是为了减少处理器访问主存的次数）。\n把多维数据存储成单维数据的两种方法\nrow major，C/C++/Java通常用row major column major，Fortran/Pascal通常用column major 常用优化方法\nInlining float add(int a, int b){ float results = a + b; return result; } int main(NULL){ float a = 3.6; float b = 6.3; float result = add(a, b); } 编译器会把所有的函数用inline code代替，消除函数调用的开销，包括压栈、跳转和返回等操作。Inlining后👇\nint main(NULL){ float a = 3.6; float b = 6.3; float result = a+b; } 缺点：如果一个函数在多个地方被内联，那么可执行文件的大小可能会增加，这有时被称为代码膨胀。而且在某些情况下，如果内联导致生成的代码过大，可能会降低指令缓存的效率，反而减慢程序的运行速度。\nDead code / Dead store Dead code: 由于一些条件，这部分代码永远无法执行 Dead store: 计算过但从未使用过的变量 编译器找到dead code和dead store并安全地忽略他们。 Code hoisting （代码提升） for(i = 0; i \u0026lt; N; i++){ x[i] = i * 5 * pi; } 把常量提出来，防止重复计算👇。!: 过度使用可能会导致寄存器溢出\nv = 5 * pi; for(i = 0; i \u0026lt; N; i++){ x[i] = i * v; } Common Sub-expression y = a * log(x) + pow(log(x), 2); 👇\nv = log(x); y = a * v + pow(v, 2); Loop unrolling for(i = 0; i \u0026lt; N; i++){ x[i] = i * 5 * pi; } 👇\nx[1] = 1 * 5 * pi; x[2] = 2 * 5 * pi; x[3] = 3 * 5 * pi; ... x[N] = N * 5 * pi; 以上优化方式都是基于时空交换（time-space trade-off）。通常想节省执行时间就要增加代码体量。通常优化程度越高，编译时间越长，可执行文件越大。\n生成优化报告 Compiler的优化指令\n-O0/-O 禁用所有优化 -O1 使用最简单的优化方法 -O2 所有O1的优化方法，再加一些更高级的优化方法，这里开始出现时空交换的优化方法。Recommended -O3 比O2更强劲，涉及大量的时空交换方法，编译时间显著增加，建议用于有密集浮点运算循环的代码 -Os 针对可执行文件的大小进行优化 -O2-no-vec 没有vectorisation的O2优化 Intel的生成报告指令\n-qopt-reportN，N=0，1，2，3，4，5。0表示没有报告，5表示最详尽的报告\nicc program.c -qopt-report3 利用Profiling code确定优化位置 Profiling: 测量程序的行为和性能，包括运行时和资源利用情况。对程序进行细分并找到热点部分，对热点部分进行优化。\n分析热点部分：\n内存带宽? 寄存器的数量? cache利用率? 代码太烂? Week4 如何实现parallelising a program 识别parallelism机会 选择parallelism策略 使用工具和库(OpenMP, CUDA\u0026hellip;) 实现+调试 性能测试+优化 把问题拆解成并行组件的普遍方法\nData parallelism Task parallelism Pipelines Mixed Solutions 粒度 粗粒度parallelism 粗粒度并行涉及较大的任务，每个任务包含相对较多的计算量。这种并行度较低，因为程序被分解成较少的、但每个都比较大的部分，在多个处理单元上执行。相比于细粒度并行，粗粒度并行的管理和通信开销相对较低，因为任务之间的交互较少。 细粒度parallelism 细粒度并行指的是由很小的任务组成的并行计算，每个任务执行的计算量相对较少。它允许高度的并行度，因为程序被分解成许多小的部分可以在多个处理单元上并行执行。细粒度并行的挑战在于管理和协调大量小任务的开销可能会很大，特别是当通信和同步成本高于任务执行成本时。 衡量并发性能 并行编程模型\nShared Memory Programming: 在共享内存编程模型中，所有处理器都访问同一个物理内存空间。这意味着所有的并行执行线程都可以直接读写同一块内存地址空间中的数据。这种模型简化了数据的共享，因为不需要显式地在处理器之间传递消息来共享数据。共享内存模型通常用于多核处理器或多处理器计算机系统，其中所有核心都能够访问同一个全局内存。OpenMP就是共享内存并行编程的API Distributed Memory Programming: 在分布式内存编程模型中，每个处理器或计算节点拥有自己的局部内存，处理器之间通过网络或总线传递消息来交换数据。这种模型要求显式地在不同的处理器之间发送和接收数据，通常使用消息传递接口（如MPI）来实现。分布式内存模型适用于计算集群、多处理器系统或网络连接的计算机，每个节点运行其进程并通过消息传递进行通信和数据共享。MPI(Message Passing Interface)就是分布式内存并行编程标准，对共享内存也适用。 Scalability and Speedup\nSpeedup指不用并行编程运行程序所花的时间和使用并行编程运行程序所花的时间的比值\nScalability指多添加一个核/处理器的情况下，speedup会有多少提升\n$t_1$: 程序在单核（或单处理器）上运行的时间。\n$t_p$: 程序在p个核心（或处理器）上运行的时间。\n$S_p = {t_1\\over t_p}$: 加速比是衡量并行程序相对于其顺序版本的性能提升的指标。理想情况下，当你用p个处理器来运行程序时，程序的执行时间会变为单处理器上的$1\\over p$，然而，由于通信和同步开销以及代码中不可并行化的部分，实际加速比往往低于p。\n并行效率公式$e_p = {S_p\\over p}$: 并行效率是衡量加速比相对于使用的处理器数的效率。它显示了并行资源的利用程度，通常表示为百分比。\nMulti-node scaling measurements\nWeak Scaling: 在弱扩展性测试中，随着节点数量的增加，每个节点上的工作负载保持不变。理想情况下，总体工作负载随节点数线性增加，执行时间保持恒定。这样可以测量系统增加计算资源时维持相同性能的能力。 Strong Scaling: 强扩展性是指总体工作负载保持不变，而节点数增加。理想情况下，执行时间随节点数的增加而减少。这种测量体现了系统处理固定大小工作负载的效率。 Amdahl’s Law 程序的最快执行速度受限于那些必须串行执行的代码部分。这些串行部分的总执行时间设置了程序加速的下限。无论并行处理多么高效，总体性能提升永远不能超过这个下限。\n$\\alpha$原始问题中串行部分所占的比例（就时间而言） $t_p = {\\alpha\\times t_1 + {(1-\\alpha)\\times t_1\\over p}}$ $S_p = {t_1\\over t_p} , limit = {1\\over\\alpha}$ 最大可能的speedup是$1\\over\\alpha$ Gustafson’s Law Gustafson\u0026rsquo;s Law的出发点是，随着处理器数量的增加，人们倾向于解决更大规模的问题，而不是简单地加速固定大小的问题。因此，他认为：\n总工作量的增加：随着处理器数量的增加，我们不仅仅是将相同的任务分配给更多的处理器，而是增加了总体的工作量，以便填满并利用所有可用的计算资源。 串行部分的影响减少：当总工作量增加时，程序中的串行部分所占的比例变得不那么重要，因为绝对的串行处理时间相对于总处理时间的影响变小了。 并行部分的增加：与此同时，可并行化的部分在总工作量中占据了更大的比例，因为这些部分可以在所有的处理器上同时进行。 Speedup = $\\alpha + p(1-\\alpha)$ 几个并发问题的例子 Week5 并行解决方案的正确性 Round errors Race conditions 克服race conditions 计算模型 ","permalink":"https://martinspace.top/zh/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97comp328/","summary":"Week1 高性能计算的目标 对于有限的数据集，最小化解决时间 对于无限的数据集，最大化吞吐量(throughput) 有能力解决一些对于可用的内存来说太大","title":"高性能计算(COMP328)"},{"content":"2023年5月7日 为同一个文件添加多种文件格式 \u0026lt;video controls\u0026gt; \u0026lt;source src=\u0026#34;html_5.mp4\u0026#34; type=\u0026#34;video/mp4\u0026#34;\u0026gt; \u0026lt;source src=\u0026#34;html_5.ogv\u0026#34; type=\u0026#34;video/ogg\u0026#34;\u0026gt; Your browser does not support the video tag. \u0026lt;/video\u0026gt; 为什么要添加多种格式？\n解决备份支持和媒体支持 什么是备份支持和媒体支持？\n备份支持：提供替代方案，一般是相同的浏览器，版本更老的一代需要这个替代方案。 媒体支持：需要提供多种格式的支持来适应不同的浏览器。 标签 \u0026lt;audio\u0026gt; 与 \u0026lt;/audio\u0026gt; 之间插入的内容不是用来解释控件的，而是在浏览器不支持audio标签时显示的文字。 当 \u0026lt;video\u0026gt; 标签中包含 controls 属性时，浏览器将自动为视频播放器提供一组默认的控制选项。用户可以根据自己的需求控制视频的播放。 \u0026lt;datalist\u0026gt; 标签与 \u0026lt;input\u0026gt; 标签结合使用，可以为用户提供一个预定义的选项列表。 \u0026lt;input list=\u0026#34;fruits\u0026#34; id=\u0026#34;fruit\u0026#34; name=\u0026#34;fruit\u0026#34;\u0026gt; \u0026lt;datalist id=\u0026#34;fruits\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;Apple\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;Banana\u0026#34;\u0026gt; \u0026lt;/datalist\u0026gt; 当 \u0026lt;progress\u0026gt; 标签没有设置 max 和 value 属性时，它会显示一个不确定进度的滚动条。在这种情况下，进度条的动画将自动滑动，但无法显示具体的进度值。 \u0026lt;progress value=\u0026#34;50\u0026#34; max=\u0026#34;100\u0026#34;\u0026gt;\u0026lt;/progress\u0026gt; DHTML DHTML（Dynamic HTML）是一种将 HTML、CSS、JavaScript 等技术结合起来，以创建动态、交互式网页的技术组合。 DHTML实现了网页从Web服务器下载后无需再经过服务的处理，而在浏览器中直接动态地更新网页的内容、排版样式和动画的功能 DOM元素带有ID属性 唯一性：ID 必须在整个 HTML 文档中是唯一的。如果有多个元素具有相同的 ID，将导致 JavaScript 在访问元素时出现问题，可能导致意外行为。因此，在为元素分配 ID 时，确保不会重复。 样式污染：如果您的 CSS 使用 ID 选择器为元素应用样式，这可能导致样式污染问题。ID 选择器具有较高的优先级，这可能会导致其他 CSS 规则无法覆盖 ID 选择器的样式。要解决这个问题，您可以考虑使用 CSS 类选择器来应用样式，以便更容易地控制和覆盖样式。 JavaScript 性能：当使用 JavaScript 查询具有特定 ID 的元素时，最好使用 document.getElementById() 方法，因为这是最快的查询方式。使用其他查询方法，如 document.querySelector() 或 document.querySelectorAll()，可能会导致较慢的查询性能，尤其是在大型的 DOM 结构中。 与现有代码冲突：在开发大型应用程序或与其他人协作时，如果不注意命名约定和唯一性，给元素分配 ID 可能会导致与现有代码冲突。在这种情况下，可以采用一致的命名规则和确保 ID 唯一性的方法来避免潜在冲突。 2023年5月8日 今天刷了几道react的题，记录一下\nstate和props state和props都可以控制组件的渲染输出 props和state都是普通的JavaScript对象 Error boundaries 错误边界 Error boundaries是一种react组件，这个组件可以捕获并处理子组件树发生的JS错误，当发生错误时，错误边界组件可以捕获错误，展示一个备用 UI，从而避免整个应用崩溃。 错误边界仅适用于捕获组件渲染阶段、生命周期方法和构造函数中的错误。它无法捕获事件处理程序或异步代码中的错误。 要将一个组件变成错误边界，需要在该组件内定义 componentDidCatch(error, info) 或 static getDerivedStateFromError(error) 生命周期方法之一。 componentDidCatch(error, info) 打印错误信息，如日志记录或向服务器报告错误。这个方法不会改变组件状态。 static getDerivedStateFromError(error) 方法用于在发生错误时更新组件状态，以便根据错误信息渲染备用UI。 React剪贴板事件 onCopy，onCut，onPaste Portal的冒泡逻辑 Portal 是一种特殊的技术，它允许将子组件渲染到父组件以外的 DOM 节点 当使用 ReactDOM.createPortal(child, container) 渲染子节点时，尽管子节点被渲染到父组件以外的 DOM 节点上，但它在 React 组件树中仍然被认为是父组件的子节点。 当通过 Portal 进行事件冒泡时，事件将沿着 React 组件树向上冒泡，而不是 DOM 树。 在事件冒泡过程中，事件将继续向父组件和其他祖先组件传播，就像子节点直接挂载在父组件内部一样。这是 React 事件系统的一个特性，它保证了组件树中的事件行为的一致性，而不受 DOM 结构的影响。 useEffect useEffect()接收两个参数，一个是回调函数，一个是依赖项数组。只有当依赖项发生变化时，回调函数才执行 useEffect 可以模拟不同的生命周期方法(componentDidMount，componentDidUpdate, componentWillUnmount) 2023年5月13日 前几天在赶due，没时间刷题，终于空下来，又刷了几道题\nJSX 使用JSX写的代码最终会转换成使用React.createElement()的形式，React.createElement：创建并返回指定类型的新React元素。 React支持的触摸事件 React支持的触摸事件有onTouchCancel，onTouchEnd，onTouchMove，onTouchStart Redux中的store 不存在store.createStore方法，store可以使用Redux提供的createStore生成 store.getState() // 获取状态 store.dispatch({ type: xxx, data: xxx }) // 分发动作对象 store.subscribe(() =\u0026gt; console.log(\u0026#39;@\u0026#39;)) // 设置监听函数，一旦 State 发生变化，就自动执行这个函数 ","permalink":"https://martinspace.top/zh/%E5%89%8D%E7%AB%AF%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/","summary":"2023年5月7日 为同一个文件添加多种文件格式 \u0026lt;video controls\u0026gt; \u0026lt;source src=\u0026#34;html_5.mp4\u0026#34; type=\u0026#34;video/mp4\u0026#34;\u0026gt; \u0026lt;source src=\u0026#34;html_5.ogv\u0026#34; type=\u0026#34;video/ogg\u0026#34;\u0026gt; Your browser does not support the video tag. \u0026lt;/video\u0026gt; 为什么要添加多种格式？ 解决备份支持和媒体支持 什么是备份支持和","title":"前端刷题笔记"},{"content":"上星期被算法课考试狠狠拷打，痛思悔过一小时。发现自己学习算法缺乏实际操作，只知理论，操作甚少，导致对理论的理解也十分浅显。\n所以从基础数据结构开始，重新系统学习算法，在这里记录学习的过程。\nStacks and Queue 非常简单的两个概念。栈就是后进先出，队列是先进先出。栈和队列都是依靠数组实现，只不过是用上了指针，使其效果看起来像是栈或队列。 比如弹出操作。数组A[1,2,3,4,5,6]，弹出位置6处的元素，并不是位置6真的在这个结构中消失了，而是定义了一个指针，当pop(A)的时候，将指针向前挪一位到5。\n栈中主要有三个方法：push（推入栈），pop（弹出栈），还有判断栈是否为空，因为栈为空了就不能进行pop操作了。\n伪代码也很好理解👇\nSTACK-EMPTY(S) // S.top表示栈顶元素，在数组的末尾。 if S.top==0 return true else return false PUSH(S, x) S.top=S.top+1 S[S.top]=x POP(S) if STACK-EMPTY(S) error \u0026#34;underflow\u0026#34; else S.top=S.top-1 return S[S.top+1] 队列\n队列和栈非常相似，也是一个数组。和栈不同的是，队列有两个指针，一个指向队列头部一个指向队列尾部（头部指向第一个元素，尾部指向的是最后一个元素的后面一个位置）。当插入一个元素时，队列的尾部+1，当删除一个元素时，队列的头部+1。 当队列的头和尾两个指针指向的索引位置相同时，队列为空，这是再想拿出来元素，就会underflow。当头部指针=尾部指针+1 或 头部=1，尾部=数组长度，那么队列就是满载的。这是想再插入数据，就会overflow。\n//这里暂时省略判断underflow和overflow的逻辑 // ENQUEUE 向队列中插入数据 ENQUEUE(Q, x) Q[Q.tail] = x if Q.tail == Q.length // 判断是否满载，满载把尾部索引设置回到头部，提示满载 Q.tail = 1 else Q.tail = Q.tail+1 //DEQUEUE 从队列中删除数据 DEQUEUE(Q) x = Q[Q.head] if Q.head==Q.length Q.head = 1 else Q.head = Q.head+1 return x 栈和队列的所有操作都是在O(1)时间内\nLinked lists 链表是一种将对象以线性顺序排列的数据结构。和数组不同，数组是以索引来定义线性顺序。而链表中的顺序是有每个对象上的指针决定的。现在使用的都是双向链表，包含前后(prev, next)两个指针，也就是说，要使列表保持完整，插入的时候prev和next要分别赋值一次和被赋值一次。当x.next为空时，说明x是链表的尾部元素，当x.prev为空时，说明x是链表的头部元素。\n链表还有单向链接的，这还分为有序和无序。还有循环链接的，就是头部元素的prev指针不指向空，而是指向尾部元素。\n//k表示key，就是节点的值，x表示整个节点 LIST-SEARCH(L, k) x = L.head while x is not NULL and x.key is not k x = x.next return x search的复杂度是Θ(n),因为最坏情况可能要遍历整个链表元素\n//插入操作是将x插在链表的头部，而不是尾部。 LIST-INSERT(L, x) x.next = L.head //原来链表的头部赋值到x.next if L.head is not NIL L.head.prev = x //把x赋值到链表头部.prev上，这样才完整 L.head = x //将头部设置成x x.prev = NIL //将x.prev设置为空 insert的复杂度为O(1)。不用过多解释了\n删除操作之前，应该调用search找到想要删除的key的位置。所以时间复杂度是Θ(n)，因为删除之前通常要search\nLIST-DELETE if x.prev is not NIL x.prev.next = x.next else L.head = x.next if x.next is not NIL x.next.prev = x.prev 对于删除操作，要多说一下关于x.prev.next = x.next。为什么不直接使用x=x.next?\n现在已知一个列表1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5，如果删除操作中代码是x=x.next，那么结果会变成1-\u0026gt;2-\u0026gt;4-\u0026gt;4-\u0026gt;5，我们想要的结果是1-\u0026gt;2-\u0026gt;4-\u0026gt;5。这就说明3处的节点依然存在，并没有被跳过。而使用x.prev.next = x.next的意思是，将x.prev指向的节点的next位置，赋值给x.next节点。这样是将2和4两个节点跳过3连接起来了。\n","permalink":"https://martinspace.top/zh/%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","summary":"上星期被算法课考试狠狠拷打，痛思悔过一小时。发现自己学习算法缺乏实际操作，只知理论，操作甚少，导致对理论的理解也十分浅显。 所以从基础数据结构","title":"基础数据结构"},{"content":"今天上算法课的tutorial，主要是讲贪婪算法。老师在课上提到了canonical coins system，但并没有深入讲解。回来看了一篇论文，整个问题很有意思，写下来记录一下。\n引入 以中国为例，我们的货币系统中，有1角，5角，1元，5元，10元，20元，50元，100元这几种面值。我们的目的是，当给定一个需要找零的值，我们要用最少的货币数量来组合成这个值。用什么策略能达到这个目的？贪婪算法是否对每一种货币系统都有最优解？这是要讨论的问题。\n这是一个NP-hard问题，但是在大多数国家所使用的货币系统中，贪婪算法是会起作用的。即先拿最大的面值去匹配，然后拿第二大的\u0026hellip;\u0026hellip;但是得到的基本上都不是最优解。举个例子，一个货币系统中的基础面值有{1, 3, 4}，如果想要拿出6块。使用贪婪算法会拿出一张4和两张1，这是三张钞票，但实际上最优解是两张3。\n什么是Canonical coins system 贪心和动态规划都可以解决这个问题，只不过在时间复杂度和所得结果上需要权衡\n贪心：贪心算法的基本思想是优先选择面值最大的硬币，直到组合成所需金额。贪心表示的优点是算法简单，计算速度快，但不一定能够得到最少的硬币数。 动态规划：优点是准确性高，但算法复杂度较高。因为它要求对所有可能的状态进行计算和记录，需要消耗大量的计算和空间资源。 详细说一下\n给定一个货币系统的面值集合，是一个向量C=($c_1, c_2, c_3, \u0026hellip;, c_n$) 给定一个系数集合，也是一个向量V=($v_1, v_2, v_3, \u0026hellip;, v_n$) $C\\cdot V = x$, 即$x=\\sum_{i=1}^n v_ic_i$ 给定$C=(4,3,1), V=(v_1, v_2, v_3), x=24$, 找出对应的i值\n贪心，将每个v都取尽量大，那么$24=6\\times 4 + 0\\times 3 + 0\\times 1$ 动态规划，计算出每一种情况，然后比较，看那种情况需要的货币数量最少。 因此，令G(x)是用贪心计算出的结果，令M(x)表示动态规划计算出的结果（就是最优的结果）。如果G(x)=M(x)，那么我们称这套货币系统为Canonical coins system（经典货币系统）。\n时间复杂度分析 对于M(w)，定义i是第1个非零的系数，j是最后一个非零的系数。系数的排列是这种类型{$0, 0, m_i, m_{i+1}, \u0026hellip;, m_j, 0,0$}。\n那么在此引入一个定理，在系数的第1到第j-1项，M(w)=G($c_{i-1}-1$)。具体的证明过程可以到Reference指向的文章中看，这里就不写了。\n那么在一个固定的货币系统中我们可以知道i和j的值。那么就会有$O(n^2)$的时间去分别测试i和j的值。但是定理是需要验证的，w必须是最小的值使得$M(w)\u0026lt;G(w)$，这需要O(n)的时间。因此，总共需要$O(n^3)$。\nReference👇\nA Polynomial-time Algorithm for the Change-Making Problem\n","permalink":"https://martinspace.top/zh/canonical-coins-system/","summary":"今天上算法课的tutorial，主要是讲贪婪算法。老师在课上提到了canonical coins system，但并没有深入讲解。回来看了一篇论文，整个","title":"Canonical coins system"},{"content":"R语言基础（W1） 基本数据类型 character(string) logical(boolean) numeric(number) factors(categories) factors表示分类，将分类数据存储为整数向量。下面用代码解释\n# 创建一个性别的factor \u0026gt; gender \u0026lt;- factor(c(\u0026#34;男\u0026#34;, \u0026#34;女\u0026#34;, \u0026#34;女\u0026#34;, \u0026#34;男\u0026#34;, \u0026#34;男\u0026#34;)) # 显示gender对象的水平(分类) \u0026gt; levels(gender) [1] \u0026#34;女\u0026#34; \u0026#34;男\u0026#34; # 显示gender对象的摘要信息 \u0026gt; summary(gender) 女 男 2 3 # 显示gender对象中每个水平的计数 \u0026gt; table(gender) gender 女 男 2 3 is和as的区别 is.***()函数用于判断一个对象是否属于某个特定的类别，返回一个逻辑值。 as.***()函数则用于将一个对象转换为指定的类别，返回一个转换后的对象。 vector和list的区别 类型： vector是一种简单的数据结构，其中所有元素必须是相同的类型。 list是一种复杂的数据结构，其中每个元素可以是任意类型的对象。 长度： vector必须是一个固定长度，一旦创建就不能更改。 list的长度是可变的，可以随时添加或删除元素。 索引： vector中的元素可以使用整数索引来访问，例如x[1]表示访问x中的第一个元素。 list中的元素可以使用名称或位置索引来访问，例如x[[1]]表示访问x中的第一个元素，x[[\u0026quot;name\u0026quot;]]表示访问x中名称为\u0026quot;name\u0026quot;的元素。 函数 给一个函数，写法如下。显然，该函数是计算输入的平均值。\nmyMean \u0026lt;- function(input=1:5) { sum \u0026lt;- 0 for (x in input) sum \u0026lt;- sum + x return(sum/length(input)) } 关于参数 以上面代码为例。我们可以置顶参数，例如这里的input=1:5。当然也可以不指定参数，不指定参数时（例如只输入input），这时会对不兼容的参数类型报错，例如vector。\n如果指定了参数，那么在传入参数时，可以使用默认也可以重写（如下面代码所示）。\n\u0026gt; print(myMean()) # 传入默认的参数 [1] 3 \u0026gt; print(myMean(1:10)) # 重写默认的参数 [1] 5.5 在函数中没有return语句？ 直接返回最后一个变量的赋值\n\u0026gt; test1 \u0026lt;- function(a=1) return(a) \u0026gt; test3 \u0026lt;- function(a=1) b \u0026lt;- a \u0026gt; test2 \u0026lt;- function(a=1) a \u0026gt; print(test1()); print(test2()); print(test3()) [1] 1 [1] 1 [1] 1 Matrices和Data.frames的区别 和vector还有list很相似\n类型 Matrix中所有元素的类型必须都相同 Data.frame中元素的类型可以不同。 索引 Matrix中的元素可以使用行列索引来访问，例如m[1,2]表示访问m中第1行第2列的元素。 Data.frame中的元素可以使用行列索引和列名称来访问，例如df[1,2]表示访问df中第1行第2列的元素，df$colname表示访问df中名为\u0026quot;colname\u0026quot;的列。 维度 Matrix是一个具有固定行列数的二维数组 data.frame可以拥有任意行数和列数。 Recycing R中挺有意思的性质\nRecycling 是指在进行二元运算时，如果其中一个运算对象长度不足，R 会自动将其进行“重复”直到和另一个对象长度相等。这样可以避免出现因为维度不匹配而导致的错误。\n举例说明 使用recycling创建一个vector 1 4 3 8 5 12 7 16 9 20\na\u0026lt;-c(1,2)*1:11 下面表格说明一下👇\n1 2 3 4 5 6 7 8 9 10 11 (1,2) (1,2) (1,2) (1,2) (1,2) (1) 最终结果：1*1, 2*2, 3*1, 4*2 \u0026hellip;\u0026hellip;\nAPPLY family apply family可以说是循环的平替。在apply家族中，每一种函数都对一种数据结构进行操作\nlapply (lists/vectors) sapply (lapply的简化版本) apply (matrices/data.frames) mapply (multiple lists/vectors/matrices) \u0026gt; lst \u0026lt;- list(c(1,2,3), c(2,3,4), c(0,10)); \u0026gt; lapply(lst, mean) [[1]] [1] 2 [[2]] [1] 3 [[3]] [1] 5 \u0026gt; sapply(lst, mean) [1] 2 3 5 \u0026gt; df Stock1 Stock2 Stock3 Day1 17.34 1.32 612 Day2 19.43 1.31 580 Day3 15.64 1.22 695 Day4 15.66 NA 690 # MARGIN=2 对列计算 \u0026gt; apply(df, mean, MARGIN=2) Stock1 Stock2 Stock3 17.0175 NA 644.2500 # MARGIN=1 对行计算 \u0026gt; apply(df, mean, MARGIN=1) Day1 Day2 Day3 Day4 210.2200 200.2467 237.2867 NA # MARGIN=c(1,2) 对行和列都计算 # mapply用法 mapply(FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE) mapply(rep, 1:3, 3:1) # equivilent to list(rep(1:3), rep(2:2), rep(3:1)) FUN 是要应用于向量的函数。 ... 是多个要操作的向量，它们可以是列表、矩阵或数据框，他们之间用逗号隔开。 MoreArgs 是一个可选的参数，表示传递给 FUN 的额外参数。 SIMPLIFY 表示是否要尝试将结果简化为向量或矩阵，默认为 TRUE。 USE.NAMES 表示是否使用结果的名称，默认为 TRUE。 mapply() 函数的作用是将函数 FUN 应用于多个向量的对应元素，然后返回一个向量或矩阵。它也会自动使用 recycling 特性，即如果两个或更多的向量长度不一致，则会将较短的向量复制多次以使它们的长度相等。\nFinanacial time series and quantmod 金融时间序列（Finanacial time series）是以时间为索引的测量序列。\n每隔一定时间 间隔不均匀时间 通俗来讲，Finanacial time series指金融市场中一系列基于时间序列的经济和金融变量的数据。这些变量通常包括股票价格、利率、货币汇率、商品价格、经济指标等等。\nquantmod包\n可以拿到每天的金融数据 绘制financial time series图表 # quantmod使用方法 library(quantmod) getSymbols(\u0026#34;AAPL\u0026#34;) print(head(AAPL)) # first few rows of the data print(first(AAPL)) # first row print(last(AAPL)) # last row 这里拿出来的数据是xts（eXtensible Time Series）类型的，后面会提到。\nxts包\n在R中有很多时间的类型，POSIXct, POSIXlt, Date, chron, timeData, yearmon, yearqtr。这里主要使用Date(日期）和POSIXct（日期和时间）。\nR语言中有两种时间序列的包\nzoo（“Z’s ordered observations”）提供了一种用于处理缺失数据和不规则时间序列的框架。处理的数据必须类型统一 xts（“extensible time series”）是基于zoo的扩展包，提供了更多的功能和更高的性能，适用于高频时间序列数据。这意味着所有zoo函数都能作用于xts对象。 市场微观结构（W2-W3） 金融市场和自动交易 主要是一些名词需要熟悉一下 金融市场大致分为以下四种\nCapital markets (equities and fixed income) 资本市场 Foreign exchange markets 外汇市场 Money markets 货币市场 Derivative markets (e.g. futures and options) 衍生品市场 主动投资vs被动投资\n被动投资指跟踪市场（例如一个指数基金跟踪S\u0026amp;P500（标普500）） 主动投资制作交易策略来打破被动投资的基准 股票市场 股份允许公司通过公共所有制为自己融资 股票代表公司固有价值或权益的一部分 一定时间内的股份数量的已知的 公司可以发行和回购股票或 定期向股东支付股息 在评估企业估值和交易策略时，企业的这些行为很重要 用adjusted prices来表示股票 在资本市场中，adjustment是指针对金融工具（如股票、债券或衍生品）的条款作出的更改，以响应市场情况或事件。这种变化可能影响该工具的价值、风险或到期时间。\n调整方式（Adjustment） Cash dividends，现金股利，从收盘价中扣除股息。 Stock dividends / stock splits，股票分红 / 股票分割 股票分红\n分红是公司利润的一部分，按照股东持有的股份比例，以现金或股票的形式向股东分配的。一般情况下，公司会定期向股东支付股息，以鼓励投资者持有股票。股票分红的大小和频率取决于公司的盈利情况、财务状况、管理层决策等因素。\n举例：某公司规定每持有一股在分红时会获得两股分红股份，因此原股票价格要除以3。例如分红前股价为25元，分红后每股为8.33元，这意味着股票持有者将获得2股分红股份/股。\n股票分割\n分割是指将一股股票分成若干股，比如将一股股票分割为两股、三股等等。分割之后，每一股的价格会降低，但是股东所持有的股票总数和股票市值不会改变。股票分割通常是为了让公司的股票更容易流通，以及吸引更多的投资者。股票分割的比例一般由公司决定，通常会在股价上涨的情况下进行。\n举例：某公司股票价格为25元，分割成两份后，每股变成12.5元。即股票持有者并没有获得额外收益。\n期货（Futures） 远期合约（forward）是一种协议，指在未来某一特定日期以某一价格买入或卖出某一固定数量的特定资产。\n期货合约（Futures contract）是一种具有标准化条款的远期合约。\nS\u0026amp;P 500 E-mini Futures 标普500E-mini期货 FTSE 100 Futures 富时100期货 Crude Oil Futures 原油期货 Eurodollar futures 欧洲美元期货(利率期货) CME Euro FX Futures 芝加哥商品交易所欧元外汇期货 期货适用于广泛的基础资产（wide array of underlying assets）。\n期货合约\n期货合约的交易一般在期货交易所进行。在交易所上，买家和卖家通过经纪人或交易所直接交易合约。在合约到期日，双方需要履行合约义务，即按合约规定的价格买入或卖出商品。如果合约到期时仍未平仓，合约将会被强制执行。这给价差交易提供了交易机会。\nETF（Exchange-traded funds） ETF，中文翻译交易所的交易基金。也叫一篮子资产\n包括指数（index），商品（commodity），债券（bond），货币（concurrency）etf。\n交易角色\nDealers 用自己的账户买卖证券 Brokers 经纪人，为客户买卖证券 Broker-Dealers 经纪商，同时扮演以上两个角色（通常是投资银行） Limit Order Books 通常由公共交易所运营 Dark Pools 由经纪商或公共交易所运营 Limit order book market 这是指一种金融市场的交易方式，这个簿维护了市场中所有买家和卖家的订单，并且会在买卖双方的限价单价格相等时匹配这些订单。在这种市场中，交易员可以提交各种不同类型的订单，包括限价单、市价单等等。\n未匹配的限价订单存储在订单簿中（order book） 买盘存储在买盘簿中（bid book） 卖盘或询价/报价存储在询价簿中（ask book) Tick size和Bid spread\nTick size指的是股票价格在交易所报价时的最小变动单位。例如，如果一个股票的Tick size为0.01美元，那么这个股票的报价必须以0.01美元为单位递增或递减。较小的Tick size通常会提高股票市场的流动性，因为更多的价格水平可以被交易，从而提高市场的活跃度。\nBid spread是指买方的最高出价和卖方的最低要价之间的差距。较小的Bid spread通常是股票市场流动性较高的标志，因为交易者可以更容易地买入或卖出股票，而且交易成本更低。一个大的价差表明实际价格的不确定性。\n对于频繁交易的品种，bid spread通常等于tick size。\n做空（short selling or shorting）\n举个例子，张三现在想要做空某粮液的股票。他先向券商借了两万股，找准时机，在市场抛售这两万股股票。此举引起大量恐慌盘的抛售，股票价格应声下跌。之后张三找准时机，再偷偷买回之前抛售的2万股股票，并将其还给券商。他挣到的就是下跌过程中的价差。\n这里引入了多空的概念。多头就是看好股票的后市价值，因此引发的动作是买入。空头就是不看好这支股票的后市价值，选择卖掉股票，就叫卖空。\n限价单和市价单（Limit orders \u0026amp; market orders）\n限价单将买方或卖方的最大或最小价格限制在一个特定的范围内。如果市场价格达到或超过指定的价格水平，交易就会发生，否则订单将被保留在交易所的订单簿中，等待市场价格到达交易者指定的价格水平。 市价单则是以市场当前的价格来执行的订单类型。交易者不指定一个特定的价格，而是要求交易所以市场当前的最优价格来执行订单。市价单在交易所中立即执行，不管证券当前的价格是多少。 市价单中的slippage指订单在执行过程中出现的价格差异，即交易者预期得到的价格和实际成交价格之间的差异。 市价单的特点是以市场当前的最优价格来执行，但是市场价格变化较快，因此在订单提交和执行之间可能会出现价格波动，导致交易者的实际成交价格与预期的价格不同。这种价格差异就被称为slippage。 市场数据粒度\nk线图 tick数据（tick表示由于交易而导致的价格变化） Market Makers 在金融市场中，市场做市商（Market Maker）是一种为证券和衍生品提供流动性的交易者。他们通常是由投资银行、证券公司或专门的做市商公司提供服务的交易者。\n所以他们经常进行的是高频交易，反复买卖抓住价差。他们的存在会缩小价差，但是也有证据表明，在市场压力大的时候，高频交易者往往会放弃市场。算是有争议的一个角色了。\ntoxic order flow（有毒订单流）\nmarket maker面临的最重要的一个问题就是如何避免有毒订单流。\n有毒订单流指的是出于某些目的导致市场价格异常波动或产生重大冲击的交易订单流。这些订单流可能会给市场造成不稳定性和影响市场的公平性。\n市场做市商是为了提高市场的流动性和减少市场波动而存在的，他们通过在买卖价格之间提供报价来赚取利润，同时承担了买卖价差的风险。当有毒订单流出现时，市场的流动性和价格稳定性会受到影响，市场做市商需要承担更大的风险，因为有毒订单流可能导致市场价格剧烈波动，对市场做市商的盈利和风险管理造成不利影响。\nFront running \u0026amp; Dark pool Front running指交易员提前知道有大订单要来，于是价格提前被抬高，因为大家都往能挣钱的地方涌。\nLimit order就是front running\n如果交易员通过跳价（penny jump）来“front running”大额限价订单，会对市场做市商产生负面影响。如果市场做市商看到交易员跳价并且认为这可能是大额限价订单正在到来的信号，他们可能会相应地调整自己的报价，以适应订单执行后市场价格的变化。这样一来，市场做市商的报价将失去准确性，可能导致市场波动增大，从而影响交易者的投资决策。\n跳价（penny jump）：当买方想要购买大量的股票时，他们可能会放置一个大额限价单，这可能会影响到市场价格。因此，为了避免被其他交易者抢先执行，他们可能会将订单价格设定在略高于当前市场价格的水平，以确保他们的订单在市场价格上涨前先被执行。\n由于限价单人人都能看到，所以叫做light pool，与之对应的是dark pool。\n暗池（Dark Pool）指的是不公开交易数据的交易平台，这些平台允许投资者以匿名方式进行交易，同时也不公开交易价格和数量。这些平台通常由银行、投资机构和证券公司等机构创建，旨在为大型交易提供一种低风险、低成本的交易方式。\n与公开市场不同，暗池不会公开所有的订单信息，也不会立即公开交易结果，而是将这些信息保密并推迟披露。这使得交易者可以在不暴露他们的交易策略的情况下进行大额交易，而且交易也不会对市场价格产生影响。可以理解为暗池中的玩家有自己的规则，他们会以市场当前的价格作为参考，买卖双方不想让市场知道有大单子来或者有大单子走，就在暗池中以交易双方谈好的价格和数量进行交易。这也可能导致有些交易者无法了解整个市场的真实情况，进而影响他们的交易决策（通常指散户🤣）。\n算法交易者会大量利用暗池。\n执行算法 养老基金或其他大公司通常会进行大体量的交易，他们将这些交易任务交给代理。由于体量非常庞大，一下子全部进入市场会引起很大的价格波动。市价单会产生slippage，限价单会无法执行。因此，代理们会使用执行算法来分割大体量资金并增量执行，目的就是隐藏大订单的存在和最小化价格波动。\n执行算法的三种benchmarks\nVWAP - Volume-Weighted Average Price VWAP是指在一段时间内（通常为一天），基于成交量的加权平均价格，用于衡量一个证券的交易价格表现。 比如在某一天，一只股票上午9:00-11:30交易量是40，下午1:00-3:00交易量是10。那么假如我的执行算法要买入100股，就在第二天上午时间段买入100*(40/(40+10))，下午的时间段买入100*(10/(40+10)) 用订单的VWAP和市场的VWAP进行比较，至少达到市场的VWAP水准。 TWAP - Time-Weighted Average Price TWAP算法通过将指定的时间段分成若干个固定时间间隔，然后将交易平均分配在这些时间间隔中，以获得期间的平均价格。 计算方法：（两次交易之间的时间差*每次交易的价格）相加后/总的时间差 用订单的VWAP和市场的TWAP进行比较，至少达到市场的TWAP水准。 Implementation Shortfall 一种衡量交易执行质量的方法，用于评估交易策略的有效性。 比较交易成本和买入/卖出股票成本之间的差异 交易成本包括滑点成本、交易费用和市场影响成本等。 买入/卖出股票的成本可以通过VWAP或者TWAP等指标来计算。 IS的值表示交易策略的效果，如果IS值为正，表示交易策略产生了超额收益；如果IS值为负，则表示交易策略产生了亏损。 衡量交易策略的表现（W4-W5） 收益（Returns） 收益是判断profit-seeking交易策略的效果的基本要素。\n和执行算法（以最低成本执行预先定好的大订单）不同，profit-seeking交易策略目的是产生正的收益。\nsimple returns \u0026amp; logarithmic returns 假设P(t)和P(t-1)是连续观测的价格时间序列。\nsimple returns: R(t) = (P(t)-P(t-1))/P(t-1) = P(t)/P(t-1) - 1 log returns: R(t) = log(P(t)/P(t-1)) = logP(t) - logP(t-1) PS: 这里的log以e为底，是自然对数。为什么要有log收益？\n乘法变加法：对数收益将复利收益的乘法运算转换为加法运算，使计算变得更加简单。例如，如果我们有连续的对数收益率 r1、r2 和 r3，那么总收益率就是 r1 + r2 + r3，而不是复利收益的乘法运算。 对称性：对数收益率具有对称性，即正收益和负收益之间的比例关系保持不变。这使得对数收益更易于分析和比较，尤其是在计算投资组合的风险和收益时。 连续时间：在连续时间模型中，使用对数收益率更为方便。这是因为在连续时间中，收益率可以被看作是随时间连续变化的，而对数收益率的加法特性使得在这种情况下的计算更加简单。 数值稳定性：对数收益率提供了更好的数值稳定性。当收益率很小或者数据量很大时，使用对数收益率可以减少浮点数计算中的误差。 正态分布：在许多金融模型中，收益率被假设为正态分布。对数收益率的分布更接近于正态分布，因此在这些模型中更适用。 对于一个资产，我们可能会采取以下三种立场\nlong，做多（买入），因为我们期望价格走高。 flat，不买不卖，不确定。 short，做空（卖出），我们预测价格会走低。 当我们选择做空，买入时间在卖出时间之后。做空后两种收益比较\nlog return: 做空的log return = - 做多的log return simple return: 相对于多头的return，空头的return稍微复杂一些：$-R_t/(R_t+1)$ ROC函数 用TTR包中的ROC函数计算return\nROC函数的参数类型可以有两种\n对于简单收益，就是离散的（discerete） 对于log return，就是连续的（continuous） simple_ret \u0026lt;- ROC(prices,type=\u0026#39;discrete\u0026#39;) log_ret \u0026lt;- ROC(prices,type=\u0026#39;continuous\u0026#39;) Equity Curves 交易策略盈利能力的时间序列图。它代表了在特定时间内，投资组合价值的变化。equity curves是由投资组合的每个周期结束时的净值（即资产总值减去负债总值）计算得出的。\nPerfect Returns 这是通过perfect position计算出来的，当市场上升时做多，下降时做空\nCopycat strategy 我们几乎不可能拿到perfect return，没人会准确预测市场。Copycat strategy就是很粗暴的策略。如果当天收盘价\u0026gt;当天开盘价，那么第二天就买入，否则第二天就卖出。\n这个策略很蠢，但什么时候会有用？\n价格连续很多天都朝一个方向移动。 当价格移动方向和前一天不同时，我们不会有太多损失。 测试这个策略\n使用utilities.R中的函数 getLogReturns(prices)从调整后的价格计算log return getEquityLog(log_ret,pos)从log return和位置向量上计算一个simple returns equity curve Risk reward measures 盈利能力的衡量标准与风险的衡量标准相结合的绩效衡量标准。\nSharpe Ratio Information Ratio Sortino Ratio Maximum drawdown Calmer Ratio 这些比率都在PerformanceAnalytics包中实现。\nMaximum drawdown最大回撤 描述的是投资组合在选定的时间段内可能遭受的最大损失，即从某个时间点的投资组合价值高点到随后的低点之间的资产价值的最大下降幅度。\n举个例子，如果你投资的股票在一段时间内从100元上升到200元，然后又降到50元，那么你的最大回撤就是（200-50）/200 = 75%。这说明在最差的情况下，你可能会损失掉75%的投资价值。\n最大回撤是一个非常有用的风险度量指标，因为它能够帮助投资者理解其投资组合在未来可能面临的最坏情况。然而，也要注意的是，最大回撤仅仅是过去的历史表现，并不能预测未来。另外，最大回撤也不应该单独使用，应该与其他风险和收益度量一起使用，以全面评估投资组合的性能。\nWealth Index财富指数 交易策略 (W6-W7) Moving averages 前面提到的copycat策略只是根据前一天的数据来决定第二天的买卖操作。而moving averages策略纳入了更多的历史数据来进行决策。\n时间序列过滤器\nlow-pass filter: 消除短期波动，留下趋势 high-pass filter: 消除趋势，留下短期波动 其中low和high代表的是频率\nCausal filters: 目的是在不引入未来信息（未来偏见）的情况下，从历史数据中提取有用的模式或信号。\n如果其output仅依赖于现在和过去的input，那么就是casual 如果其output依赖了未来的input，那么就是non-casual 在策略开发过程中，使用未来信息（在实际交易中是不可获取的）对策略产生不切实际的、过于乐观的效果。通过 Causal Filter，可以确保交易策略的有效性和实际可行性。\nSimple \u0026amp; Exponential moving average\nSimple: 它计算一段时间内（如5天、20天、50天等）数据的算术平均值。简单移动平均线可能受到异常值的影响，因此在处理数据时可能不够灵活。 Exponential: 这是一种加权移动平均线方法，它赋予最近的数据点更高的权重，但权重是按指数衰减的方式分配的。这意味着较早的数据点权重逐渐减小，但永远不会完全消失。指数移动平均线对于跟踪价格趋势非常有用，因为它能平滑短期波动，同时对长期趋势反应灵敏。 Bollinger Bands超买超卖策略 Bollinger Bands 是20世纪80年代开发的一种技术分析指标，用于研究金融市场的价格波动。布林带包含三条线，即中间线、上限线和下限线，它们可以帮助交易者识别市场的过度买入或过度卖出现象，从而指导交易决策。\n均值回归交易策略就使用了布林带\n当它们暗示市场超卖(oversold)时买入 当它们暗示市场超买(overbought)时卖出 布林带的三条线\n中间线（Middle Band）：通常为价格的简单移动平均（SMA），常见的周期设置为20天。 上限线（Upper Band）：上限线是中间线加上一个标准差乘以系数（通常为2）。标准差是一个统计概念，用于衡量数据的波动程度。这里的系数是用于调整带宽的一个参数，通常设为2。 下限线（Lower Band）：下限线是中间线减去一个标准差乘以系数（通常为2）。 布林带的宽度会随着价格波动的加大而加大，反之则缩小。布林带的宽度的变化可以用来衡量市场的波动性。当布林带变窄时，表示市场波动较小，可能预示着即将出现的大幅波动（突破）。当布林带变宽时，表示市场波动加大，可能暗示着趋势的持续。\n使用AAPL股票数据，应用布林带技术指标，并选择了2008年。图表的类型被设置为线形图（type=\u0026lsquo;l\u0026rsquo;）。\n\u0026gt; library(quantmod) \u0026gt; getSymbols(\u0026#39;AAPL\u0026#39;) \u0026gt; taString \u0026lt;- \u0026#39;addBBands();addBBands(draw=\u0026#34;p\u0026#34;)\u0026#39; \u0026gt; chartSeries(AAPL,TA=taString,subset=\u0026#39;2008\u0026#39;,type=\u0026#39;l\u0026#39;) 布林带的参数 bbands\u0026lt;-BBands(prices, n=n, sd=sd)，其中n表示计算中轨线的周期为n天，sd表示上下轨道线相对于中轨线的标准差倍数为sd（不是标准差，是标准差的倍数！）。\nPath independence \u0026amp; dependence 如果一个交易策略不依赖过去的决策，那么这种决策被称为路径无关，path independence.\n布林带策略就是典型的路径无关交易策略\n路径无关实际上是一种很严格的限制，现在大部分的交易策略都是路径相关的。\n许多策略的构建都需要路径依赖，例如一些特殊的卖空条件：\nHolding period (持有时间) profit target (盈利目标) stop loss (止损点) Slippage 之前提到过，就是量很大没有单子能一下接的住，那么卖出的价格就会偏低（买入的价格会偏高）。这种情况通常在市场波动较大或流动性较低的情况下发生。在策略回测中，为了保守起见，我们通常假设滑点对交易者是不利的。\n在策略回测时采用这种假设滑点为负的标准方法是在策略回测中常用的一种方法。这样可以使回测结果更加保守，从而避免在实际交易中因对策略过于乐观而导致的损失。\n所以在构建交易模型时要估计实际成交价格与预期成交价格之间的差异，这叫做模拟滑点。\n交易策略的特点：了解策略在哪些市场环境中进行交易。如果其他市场参与者采取类似策略，滑点可能会变得更糟糕。例如： 如果在涨势中买入，可能会出现负滑点，因为许多交易者可能在涨势中买入，从而推高价格。 如果在抛售过程中买入，可能会出现正滑点，因为在供应过剩的情况下，你可能可以以较低的价格买入。 特定市场的特点：考虑你所交易的特定市场，因为不同市场的流动性和波动性可能会影响滑点。要考虑的一些因素包括： 被交易的资产：某些资产（如流动性较低的股票或加密货币）可能比流动性较强的资产（如主要货币对或大盘股）具有更高的滑点。 市场交易时间：在流动性较低的时段（例如，非正常交易时间）进行交易可能导致较高的滑点。 新闻与事件：重大新闻事件或经济数据发布可能导致波动性突然增加，从而导致滑点增加。 数据可用性：为了准确地模拟滑点，理想情况下需要访问高质量数据，包括： 订单簿数据：此类数据提供了不同价格和数量的信息，有助于更准确地估计滑点。 成交量数据：可以帮助了解市场的流动性状况，从而更好地预测滑点。 均值回归策略 2类策略\nmean reversion/contrarian/overbought-oversold 均值回归/反向/超买超卖 trend following / momentum 第一类策略是基于市场可能出现错误定价的观点。这些策略在实施时可能会有所不同。\n均值回归策略认为价格会在一段时间内回归到其长期平均值。 Contrarian策略是投资者相信市场可能对某些资产的价格作出错误判断，因此会采取与大多数人相反的交易行为。他们试图利用市场的过度反应来获得利润。 超买超卖策略则关注市场中的短期波动，试图捕捉价格在达到极端水平后可能出现的反转。 统计套利（Statistical Arbitrage） 这是一种复杂的量化投资策略，利用统计方法预测和利用金融市场的价格偏差。 基本思路是建立一个模型来识别价格偏离其预期值的情况，然后利用这种偏离进行交易，以期待价格回归预期值。 配对交易（Pairs Trading） 配对交易是统计套利的一种特殊形式。它基于两只或更多相关资产（通常是股票）之间的历史价格关系。 当这些资产的价格关系偏离历史正常水平时，交易员会买入价格偏低的资产并卖出价格偏高的资产，等待价格回归正常关系时再平仓，从而获得利润。 配对交易的难度在于找到正确的配对资产，需要对行业和公司有深入的理解，并且需要足够的数据来支持策略。 PS: 统计套利和配对交易都属于均值回归策略的一种，这些策略基于一个假设：市场价格或价格差（在配对交易中）会回归到它们的历史平均值。\n协整理论（Cointegration）\n用于描述两个或更多个时间序列之间的长期均衡关系。 在金融中，协整理论的应用主要体现在配对交易或其他统计套利策略中。如果两只股票的价格是协整的，那么他们的价格将保持一定的长期关系。如果这种关系暂时偏离了历史正常水平，那么交易员可以预期它会在未来回归到这种关系，并据此进行交易。 例如，如果股票A和股票B是协整的，并且A相对于B暂时过高，那么交易员可能会卖出A并买入B，等待价格回归到他们的长期关系。 隐式仓位比率（Implied Position Ratio）/对冲比率（Hedge Ratio）\n这个比率描述了需要用一个资产去对冲另一个资产风险的数量。 这种策略的关键在于，两个资产的价格变化是相关的。两个资产的价格变化也会在很短的时间内变得不再相关，所以要密切关注。 对冲比率通常通过回归分析来确定。 假设有两只股票，苹果（AAPL）和微软（MSFT），通过回归分析，我们发现每当苹果股价变动1美元，微软股价通常变动0.5美元，那么对冲比率就是0.5。这意味着，如果我们持有1美元的苹果股票，可以通过卖出0.5美元的微软股票来对冲我们的苹果股票风险。 在配对交易或统计套利策略中，投资者并不是因为预测到某一支股票会亏损才选择持有它，而是因为他们预测到两支股票之间的价格差距会收敛。 假设苹果（AAPL）和微软（MSFT）的股票价格有一定的相关性，而现在这两者的价格差距大于历史平均水平。也就是说，相对于微软，苹果的股票被低估了。在这种情况下，投资者可能会购买苹果股票，并同时卖空微软股票，希望当两者的价格差距回归到历史正常水平时获利。 在这个过程中，即使苹果的股票价格下跌，如果微软的股票价格下跌得更多（这是有可能的，因为两者的股票价格有相关性），那么投资者仍然有可能获利，因为他们是从两者的价格差距收敛中获利，而不是从单一股票的涨跌中获利。 上面的话使我对于对冲的理解更加深刻👆 spread通常指的是两个或更多相关资产之间的价格差异。根据资产的种类和交易的时间，spread可以在很多不同的情况下建立。以下是一些常见的例子：\n在同一个商品的不同月份之间。例如可以在3月份的原油期货和5月份的原油期货之间建立一个spread。这是一种常见的期货交易策略，投资者预期同一商品在不同交割月份的价格会有一定的变化。(called an interdelivery spread/ calendar spread) 在同一个或相关商品之间建立，通常是在同一个月份。例如，在黄金和白银之间建立一个spread。这种策略通常用于套利交易，投资者预期两种相关商品的价格差将回归到其历史平均水平。(intercommodity spread) 在在两个不同的交易所上交易的同一种或相关商品之间建立。例如，可以在纽约商品交易所交易的黄金期货和伦敦金属交易所交易的黄金期货之间建立一个spread。这种策略通常用于利用不同市场之间的价格差异进行套利。(Intermarket Spread) 在两只股票之间建立的，这两只股票通常是在同一行业或市场中，有相似的商业模式或经济影响因素。例如，可以在苹果（AAPL）和微软（MSFT）之间建立一个spread。这种策略通常用于利用两只股票价格的相对波动进行套利。(Pairs Trading) 第二类策略是基于资产价格行为的一种方式。\n趋势追踪策略是一种基于历史价格趋势预测未来价格趋势的策略。趋势追踪者认为市场价格展现出一定的动量或趋势，并会尝试利用这些趋势进行投资。当资产价格显现出上涨趋势时，趋势追踪者将买入该资产；当资产价格显现出下跌趋势时，趋势追踪者将卖出或做空该资产。 动量投资策略是一种基于资产近期价格表现预测未来价格表现的策略。动量投资者认为过去一段时间内表现较好的资产将在未来继续表现优秀，而表现较差的资产将在未来继续表现疲弱。因此，他们会购买近期表现优秀的资产，并卖出或做空近期表现疲弱的资产。 二者区别：这两种策略都是试图从市场趋势和动量中获利，但趋势追踪更注重长期趋势，而动量投资更注重短期价格表现。\n跟踪止损和盈利目标\n动量策略通常使用止损（Stop Loss）作为退出策略，而均值回归策略通常使用利润目标（Profit Target）作为退出策略。 Stop Loss：这是一种预先设定的订单，旨在限制投资者可能遭受的损失。这通常在价格向对投资者不利的方向移动时触发。在动量策略中，止损订单可以帮助投资者在市场反转时保护其资本。 Profit Target：这是预先设定的价格级别，一旦达到该级别，投资者将关闭位置以获取利润。在均值回归策略中，利润目标可以帮助投资者在价格回归到其均值时锁定利润。 Trailing Stop Loss：这是一种动态的止损策略。当交易价格向有利的方向移动时，跟踪止损将跟随市场价格移动，从而允许投资者在保护其资本的同时，捕获更多的利润。跟踪止损通常相对于交易的最佳价格而不是入场价格进行设置。 回溯测试，优化以及交叉验证(W7-W9) 交叉验证 想知道模型在未来交易中表现怎么样，要用到交叉验证。统计学中交叉验证的概念：把训练的模型一般化到不可见的数据集中表现怎么样。交叉验证降低了data-snooping bias（数据窥探风险）\nIn-sample和out-sample test in-sample就是训练集，out-sample就是测试集\n作用\n防止过拟合 评判在为未来表现的重要指标 注意：out-sample中性能下降是很正常的现象\nRSI strategy RSI(Relative strength Index)相对强度指数。这是TTR包中的一个标准的指标。这个指标返回一个0-100之间的值，值越高表明市场最近正在上涨趋势，反之亦然。\nBias Look-ahead Bias: 这种偏误发生在你的模型或策略使用了在实际情况下不可能在当前时点获取的信息。例如，假设你在模型中使用了明天的股价作为今天投资决策的依据，这就产生了向前偏误。 Survivorship Bias: 生存偏误发生在，当你的样本只包含“生存者”（即，成功或者持续存在的例子），而忽视了“非生存者”（即，失败或者已经不存在的例子）。例如，在股票市场研究中，如果你只研究当前仍在交易的公司，而忽视了已经破产或被收购的公司，那么你的研究就可能存在生存偏误。 Data-snooping Bias: 这种偏误出现在过度拟合过去的数据，试图找出符合这些数据的模式或策略，但这些模式或策略在未来可能并不适用。这种过度拟合可能导致过于乐观的策略性能预期。 Time period Bias: 这种偏误出现在，当策略的性能依赖于选择的特定时间段时。例如，如果你的策略在牛市中表现出色，但在熊市中表现不佳，那么你就可能过于依赖特定的时间段（例如，仅在牛市中）进行策略测试。 Confirmation Bias: 确认偏误是一种心理学现象，当人们倾向于寻找和关注那些符合自己预设观点的信息，而忽视或者低估那些与自己观点相悖的信息。在交易策略的开发中，如果你只关注那些支持你的策略的测试结果，而忽视了那些反对你的策略的结果，那么你可能就会陷入确认偏误。 交叉验证可以避免Data-snooping Bias。\n","permalink":"https://martinspace.top/zh/%E9%87%91%E8%9E%8D%E5%B8%82%E5%9C%BA%E4%BA%A4%E6%98%93comp226/","summary":"R语言基础（W1） 基本数据类型 character(string) logical(boolean) numeric(number) factors(categories) factors表示分类，将分类数据存储为整数向量。下面用代码解释 # 创建一个性别的factor \u0026gt; gender \u0026lt;- fa","title":"金融市场交易(COMP226)"},{"content":"时间复杂度和online algorithm (W1) 算法分析中我们常用到三个符号：O，Ω(Omega) 和 Θ(Theta)\nO表示的是渐进上界，它描述了算法在最坏情况下的时间复杂度，表示算法在最坏情况下所需的时间不会超过O的函数。 Ω表示渐进下界，它描述了算法在最好情况下的时间复杂度，表示算法在最好情况下所需的时间不会低于Ω的函数。 Θ，描述了算法的时间复杂度的渐进上界和下界，表示算法的时间复杂度在最好和最坏情况下的增长速度是相同的。 在线算法指的是，算法必须在输入数据不是完全可知的情况下，完成相应的计算并输出计算结果。\n与之对应的是离线算法，它充分利用了输入数据之间完整的关联特性，即算法运行前所有输入数据均是已知的。\n竞争分析针对的是在线算法，已知一个在线算法A，S是问题的一个变量输入序列，用cost(A, S)记录算法A应对S所产生的成本，用cost(OPT, S)记录最优离线算法应对S所产生的费用。若对于任意S都有cost(A, S)\u0026lt;a*cost(OPT, S)，则A是一个a-竞争算法。\nBST和其他搜索树 (W2) 链表 双向链表，如下图所示，每个节点都有一个前继(prev)和一个后驱(next)。 双向链表的几个常用方法\nreplaceElement(p,e) // p - position, e - element. swapElements(p,q) // p,q - positions. insertFirst(e) // e - element. insertLast(e) // e - element. insertBefore(p,e) // p - position, e - element. insertAfter(p,e) // p - position, e - element. remove(p) // p - position. 介绍其中一个方法：insertBefore()\n技巧：要确保每个节点的前后两个指针都有连接，如上图所示。\ns.prev = p; /* 把 p 赋值给 s 的前驱 */ s.next = p.next; /* 把 p-\u0026gt;next 赋值给 s 的后继 */ (p.next).prev = s; /* 把 s 赋值给 p-\u0026gt;next 的前驱 */ p.next = s; /* 把 s 赋值给 p 的后继 */ List更新的复杂度分析（remove和insert）\n如果元素p的位置已知，则复杂度为O(1) 如果只知道list的头，则复杂度为O(p)，因为需要遍历list来找到p 二分查找（BST) 一个有序数组通常称为lookup table。\n输入一个有序数组 输出要查找的值及其对应的key BINARYSEARCH(S,k ,low ,high) if low\u0026gt;high return no_such_key else mid = Math.floor((low+high)/2) if k = key(mid) return elem(mid) else if k\u0026lt;key(mid) return BINARYSEARCH(S, k, low, mid-1) else return BINARYSEARCH(S, k, mid+1, high) key (i): the key at index i elem(i): the element at index i lookup table 和 linked list比较\nMethod LinkedList lookup table 查找元素 O(n) O(logn) 插入元素(已知位置) O(1) O(n) 移除元素 O(n) O(n) ClosestKeyBefore O(n) O(logn) 有没有一种数据结构可以既保证查找的效率又保证插入的效率？（综合lookup和linkedlist的优点） \u0026mdash; BST maybe possible.\n二叉树的节点深度：节点的祖先的数量（排除节点自己） 二叉树的高度=该树最大的节点深度 二叉树的遍历方法\n前序遍历(pre-order) 中序遍历(in-order) 后序遍历(post-order) 所有的BST的操作的时间复杂度都是O(h)，h表示树的高度。如果情况糟糕的话，h很有可能等于n（对于一个degenerate tree来说）。 如果BST不平衡的话，它最差的搜索时间可能是O(n)，这就失去了它的优点。\n因此我们衍生出一种具有高度平衡的性质的树——AVL树\nAVL树 AVL树的性质：对于树T中的每个内部节点v，v的子节点之间的高度差最多是1。\n存储n个节点的AVL树的高度是O(logn)\n证明：一个高度为h的AVL树，令这棵树的内部节点最少有$n_h$个。最坏情况下，左树的高度为h-1，右树高度为h-2，且两颗子树都是AVL树。\n所以$n_h = n_{h-1} + n_{h-2} + 1$，意思是整个树的节点是由左右两颗子树的节点加上根节点，因为$n_{h-1} \u0026gt; n_{h-2}$，所以$n_h\u0026gt;2\\times n_{h-1}$ -\u0026gt; $n_h\u0026gt;2^{{h\\over 2}-1}$\nAVL树中的搜索操作，时间复杂度为O(logn) AVL中的插入和移除操作要小心（要使用rotation来保证height balance） 旋转这个操作太麻烦，在b站找到一个宝藏嚣张教程，记录一下。\n宗旨：自下而上地找最小的一颗不平衡子树，如果向上找的节点太多，就选择离根节点最近的三个节点（包括根节点），将三个节点拿出来树状排序，插入原来的位置。剩下的元素根据二叉树的规则重新插入。\n在下面这个AVL树中插入90 flowchart TD id1(50)--\u003eid2(26)--\u003eid3(21) id2--\u003eid4(30) id1--\u003eid5(66)--\u003eid6(60) id5--\u003eid7(68)--\u003eid8(67) id7--\u003eid9(70) 插入后效果如下 flowchart TD id1(50)--\u003eid2(26)--\u003eid3(21) id2--\u003eid4(30) id1--\u003eid5(66)--\u003eid6(60) id5--\u003eid7(68)--\u003eid8(67) id7--\u003eid9(70)--\u003eid10(90) 很明显，这不是一颗平衡的AVL树。因此，从底部开始向上寻找最小的不平衡子树的根节点，发现是66。路径是90\u0026ndash;\u0026gt;66，找到这条路径上距离66最近的三个节点（包括66本身），找到了66，68，70三个数。 对这三个数进行重新排列，如下图所示 flowchart TD id1(68)--\u003eid2(66) id1--\u003eid3(70) 排列后插入原来的位置，剩下的元素根据二叉树的规则重新插入，结果如下 flowchart TD id1(50)--\u003eid2(26)--\u003eid3(21) id2--\u003eid4(30) id1--\u003eid5(68)--\u003eid6(66) id5--\u003eid7(70)--\u003eid8(90) id6--\u003eid9(60) id6--\u003eid10(67) 整套操作行云流水，十分清爽，爱了。删除也是同样的操作，删除后不平衡，找到最小不平衡树，然后重新排列。在AVL树中，所有的操作都可以在O(logn)内解决\n(2,4)树 (2,4)树的定义是这棵树的子节点，最少有两个，最多有四个。每个内部节点都包含1-3个键，这些键定义了存储在子树中的键的范围。这样的键要保证，比它最左边的子树中最大的键大，比最右边的子树中最小的键小。\n(2,4)树的所有叶子节点都具备相同的深度。\nn个元素存储在(2,4)树中，其高度为O(logn) 拆分，转移和融合操作花费O(1)的时间 搜索，插入和删除元素需要访问树中O(logn)个节点。 排序-Sorting（W3） 排序问题，给一个集合C，包含n个元素，将C中的元素以升序方式重新排列。\n排序通常是解决问题过程中的一个子例程。也就是说，想要实现算法的良好性能，高效的排序是很有必要的。\nPriority Queues 优先队列是一个存储元素的容器，每个元素都有对应的key。\n优先队列的基本方法：\ninsertItem(k, e) removeMin() minElement() minKey() 使用优先队列对一个集合C进行排序分为以下两个步骤\n首先使用insertItem(k, e)，将C中的元素都放在优先队列P中。 使用removeMin()方法以升序方式从P中提取元素。 堆 堆是一种完全二叉树，即除了最后一层外，其它层都是满的，最后一层从左到右填满。n个元素的高度的是O(logn)\n堆是有效的优先队列的实现，在插入元素和删除元素方面，堆允许在log时间内插入和删除元素。\n在堆中，元素和键都被存在二叉树中。子节点中元素的键要大于等于其父节点中的键（堆序性-最小堆）\n堆插入 堆中的插入操作，需要维护堆序性\n将要插入的元素放在堆的底部（最后一个叶子节点的右侧） 比较该元素与其父节点的值，如果该元素的值小于（最小堆）其父节点的值，则交换这两个节点，然后继续向上比较（这个过程和bubble类似），直到满足堆序性为止。 插入操作完成后，堆中会多出一个元素，需要将堆的大小加 1。（堆的大小=堆中元素数量） 堆删除 堆中的删除操作，同样需要维护堆序性\n取出堆顶元素 将最后一个叶子节点的值赋给堆顶元素。将最后一个叶子节点的值放到堆顶，相当于将堆顶元素删除，同时保证了堆仍然是一棵完全二叉树。 对新的堆顶元素进行下滤操作。下滤的过程类似于插入操作，将新的堆顶元素与它的子节点比较，如果子节点中有比它更小的，则交换位置，直到找不到比它小的为止。 Operation time size,isEmpty O(1) minElement,minKey O(1) insertItem O(logn) removeMin O(logn) 堆排序 从下到上构造一个有n个元素的堆，时间复杂度为O(nlogn) 升序提取n个元素的时间复杂度为O(nlogn) 分治（Divide and Conquer） 顾名思义了，把一个大问题拆分，解决子问题后，再进行合并。（Divide-Recur-Conquer）\n归并排序（MergeSort） 分治的经典例子，mergesort\n对mergesort的divide部分，很好理解，但是到了merge部分就有些懵懂，因为有些课程省略了步骤，现在记录一下详细步骤。\n假设到了合并这一步，现在有两个数组arr1和arr2\narr1 = [2, 5, 7, 8] arr2 = [1, 3, 6, 9] 我们要将它们合并成一个有序数组。首先，我们需要开辟一个长度为len(arr1) + len(arr2)的临时数组temp\n接下来，需要设置两个指针i和j，分别指向两个数组的第一个元素，用于比较两个数组中的元素。还需要一个指针k，指向临时数组的第一个位置，用于存储排序后的元素。\n时间复杂度分析\nn个元素进行mergesort的时间复杂度为O(nlogn)\n将mergesort的过程理解为二叉树，那么这颗二叉树的高度为O(logn)。这颗二叉树中每个子节点的复杂度为O(n)，因为要进行排序，最坏情况下，每个元素都会被访问一次。因此时间复杂度为O(nlogn)。空间复杂度为O(n)，因为需要另开辟一个长度为n的数组，存储排序好的数据。\nCounting inversions 长话短说，一组序列，有一对数i和j，都有i\u0026lt;j, 而且a[i]\u0026gt;a[j]，则称(i, j)是一个逆序对，Counting inversions就是一种计算逆序对数量的算法。\n最笨的解决办法：找出所有数对并一个一个查看。这样做的时间复杂度是O(n^2)，太贵，做不起。\n使用分治，可以把时间复杂度压缩到O(nlogn)\n具体做法\n将一个序列按照中间位置分为左右两个子序列，递归地对左右两个子序列进行排序，并计算它们内部的逆序对数。 将左右两个子序列合并为一个有序的序列，同时计算左右两个子序列之间的逆序对数。 返回左右两个子序列内部的逆序对数、左右两个子序列之间的逆序对数以及合并后的有序序列。 逆序对数=两个子序列内部的逆序对数 + 左右两个子序列之间的逆序对数 时间复杂度分析\n因为需要遍历两个子序列中的所有元素，因此复杂度是O(n)，n是序列中元素的数量。 由于每次递归的时候，问题规模减半，因此归并排序需要递归 logn 层 每层的复杂度是O(n)，因此归并排序时间复杂度是O(nlogn) 快排（QuickSort） 快排也是分治算法的一种，但是这种排序方法不同于归并排序。\n基本思想：随机选择一个基准元素(pivot element)，然后将数组分为两个子序列，小于基准元素的放在左边，大于基准元素的放在右边，然后递归地对两部分排序，知道序列变得有序。\n具体过程\n选择一个基准元素（pivot），可以选择第一个元素、最后一个元素、中间的元素或者随机选择。 从序列的两端开始向中间扫描，左端找到一个比基准元素大的数，右端找到一个比基准元素小的数，交换这两个数的位置。 继续扫描，直到左右两个端点相遇。此时，所有比基准元素小的数都在基准元素的左边，比基准元素大的数都在基准元素的右边。 递归地对左右两个子序列进行快速排序。 当序列中只剩下一个元素时，排序结束。 时间复杂度分析 最坏情况下，每次划分都只能将序列分成一个元素和 n - 1 个元素两部分，这种情况下递归树的深度为 n，每层的时间复杂度是 O(n)（因为需要遍历所有 n 个元素进行比较）。因此，最坏情况下的时间复杂度为 O($n^2$)。\n在最好情况下，每次划分都将序列分成长度相等的两部分，这种情况下递归树的深度为 log(n)，每层的时间复杂度是 O(n)。因此，最好情况下的时间复杂度为 O(n log n)。\n如何避免worst case? 随机选择pivot。随即快排的期望运行时间是O(nlogn)\n如果分割的两部分的长度都不大于序列长度的3/4，则这样的pivot就是好的随机pivot。所以成功选择一个好的随机pivot的概率是1/2\n线性时间排序（Bucket Sorting) 桶排序就是将数据分到不同的桶里，然后对每个桶中的数据进行排序，最后把所有桶中的数据合并起来得到有序序列\n具体过程\n将待排序的数据分配到不同的桶中。 对每个桶中的数据进行排序，可以选择任意一种排序算法，比如插入排序、归并排序等。 将各个桶中的数据按照顺序合并起来，得到有序序列。 桶排序适用于数据分布比较均匀的场景，如果数据分布不均匀，有些桶的数据量会比较大，就会影响桶排序的效率。比如对成绩进行排序，可以将分数分为 0-59、60-69、70-79、80-89、90-100 等五个区间，分别放到对应的桶中进行排序。\n基数排序（Radix Sorting） 这是一种非比较排序算法，它是根据每个位的值来进行排序的。 在每一列上进行桶排序，然后逐列进行排序。可以将待排序的数据按照位数的不同，将其分配到不同的桶中进行排序，最后将所有桶中的数据按顺序合并起来。\n时间复杂度分析 基数排序的时间复杂度取决于每次排序的子排序算法\n如果使用插入排序作为子排序算法，基数排序的时间复杂度为 O($d\\cdot(n+k)$)，其中 d 表示待排序数据的位数，n 表示待排序数据的个数，k 表示桶的个数。 如果使用quicksort或mergesort作为子排序算法，基数排序的时间复杂度可以降低到 O($d\\cdot n$)，但是空间复杂度会略有增加。 细说分治（W4） 我们通常使用递归来分析分治算法的运行时间。\n当输入数据的大小为n时，使用T(n)来表示运行时间。\n以Mergesort为例\nT(n) = c if n$\u0026lt;$2 T(n) = 2$\\times$T(n/2) + cn if n$\\ge$2 c是一个常数，表示合并当前层的两个子列表时需要做多少次比较或移动操作，因为具体计算c的大小与具体的算法有关，所以只用一个常数来表示，cn的时间复杂度就是O(n)。2$\\times$T(n/2) 表示递归排序2个子序列的时间复杂度。\n分治的递归表达式 T(n) = c if n$\\le$d T(n) = aT(n/b) + f(n) if n$\u0026gt;$d 表达式中，a表示每次递归需要解决的子问题的数量，b表示子问题的规模，f(n)表示非递归部分的时间复杂度。\n举个例子$T(n) = 3T(n/2) + n^2$ 。这个递归式的意思是将一个规模为 n 的问题分成 3 个规模为 n/2 的子问题来解决，然后再用 O($n^2$) 的时间将这些子问题的解合并起来得到原问题的解。\n主定理（Master Method） 第一种情况，表示子问题的时间复杂度是主要因素，即时间复杂度主要是子问题说了算。 第二种情况，表示子问题规模与原问题规模相等，如果我们可以将原问题分成几个大小相等的子问题，并且解决每个子问题的复杂度相同。例如mergesort。 第三种情况，表示子问题规模对主问题的时间复杂度影响很小，主要由f(n)决定。 整数快速乘法（Fast multiplication of integers） 当有两个非常大的整数相乘（位数很多），其时间复杂度是O(n^2)，因为数字中的每一位都要和另一个数的所有位进行乘法操作。快速整数乘法就是找出算法来缩小时间复杂度。\nKaratsuba Algorithm 利用分治的思想，假设我们要计算两个n位整数x和y的积，x和y可以分别表示为\n$x = a * 10^{n/2} + b$ $y = c * 10^{n/2} + d$ 其中，这里以十进制为例，如果是二进制，把10替换成2就可以。a和c是x和y的高位部分，b和d是它们的低位部分\n因此，$xy=ac \\cdot 10^n + (ad + bc) \\cdot 10^{n/2} + bd$。我们可以通过递归计算这些值，并组合起来得到最终结果。由于ad+bc=(a+b)(c+d)-ac-bd，ac和bd用到的是前后已经求出来的值，因此每次只需要被分解成3个子问题即可，即\n计算ac的值 计算bd的值 计算(a+b)(c+d)的值 由此，我们可以得出递归表达式为T(n) = 3T(n/2)+cn，即进行了3次乘法计算和6次加减法计算。根据主定理，$cn\u0026lt;log3$，因此这种算法的时间复杂度为$n^{log3}\u0026lt;n^2$，此处log以2为底。\n矩阵乘法（Matrix Multiplication） 根据图中的递归表达式，我们可以得出时间复杂度为O($n^3$)，很费钱。\nStrassen’s Algorithm 理解了上面图中的东西，这个算法就很好理解了。这个人找到了一种方法，用更少的步骤就可以解决矩阵乘法。 首先找到7个矩阵乘积\n$S_1$ $S_2$ $S_3$ $S_4$ $S_5$ $S_6$ $S_7$ a(f-h) (a+b)h (c+d)e d(g-e) (a+d)(e+h) (b-d)(g+h) (c-a)(e+f) 之后，结果可以用如下式子表示\n$I = S_4 + S_5 + S_6 − S_2$ $J = S_1 + S_2$ $K = S_3 + S_4$ $L = S_1 + S_5 + S_7 − S_3$ 因此，递归表达式可以写成\n$T (n) = 7T(n/2) + bn^2$\n在根据master method可以得出，时间复杂度有所缩小，变成了O($n^{\\log_2 7}$)\n浅说贪心（W5） 一些广泛应用在不同问题上的算法工具\n贪心 分治 动态规划 第一个想到的就应该是贪心。贪心算法能求出解的问题具备greedy-choice的属性。\nGreedy-choice property\nGreedy-choice property是指每一步所做的贪心选择都能导致局部最优解（即当前情况下最优的解），从而最终得到全局最优解。\n可以被贪心算法解决的问题\nFractional Knapsack Problem Interval Scheduling Task Scheduling Minimum spanning trees Shortest Paths Change Making Maximum Spacing k-clustering FKP FKP问题指的是：一个含有n个元素的集合S，S中的每个元素i都有一个正的收益和正的权重。目的是在不超过总权重W的情况下，找到一个具有最大收益的子集。\n需要说明的是，这个问题和0/1背包问题不同（每个物品只能选择放或者不放）。FKP可以将物品分成一部分放入背包，而不必完全舍弃。对0/1背包问题，greedy通常不会发现最优解。（动态规划可以\n对于分割成的部分，我们将其份数定义为$x_i$，对于所有的元素i，$x_i\u0026lt;w_i$。因此固定总权重下总收益的公式为 $$\\sum_{i\\in S} b_i(x_i/w_i)$$ 因此我们可以计算每个元素i的价值指数。$v_i = b_i/w_i$。这可以计算出每单位重量的价值，价值指数越高的物品越容易被优先选择。\n说到优先选择就会想起优先队列，想到优先队列就会想起堆。把最大的价值指数插在堆的根节点。建造一个堆需要的时间复杂度为O($n\\log n$)。每次贪心选择（移走堆中剩下的最大的价值指数）需要O($\\log n$)。\n因此，给出一个FKP问题，我们都可以构造出一个最大收益的子集在O($n\\log n$)时间内。\nInterval Scheduling Interval Scheduling指的问题是，我们有一系列任务在一个集合中。现在只有一个机器去处理这些任务。我们的目的是选择一个调度顺序使得任务的完成数量最大化。\n怎么办？找最先开始的任务？那万一这个任务和大部分其他任务都冲突怎么办？那找具有最小时间间隔的两个任务？那和其他任务时间间隔很长怎么办？或者找一个和大部分任务都不冲突的任务？同样无法保证上述问题被解决（没法保证中间有很多不该浪费的时间被浪费）\n综合上面的想法，我们发现问题并不是出现在谁先开始，而是如何保证开始后不浪费不该浪费的时间。因此可以想到\n把任务按照结束时间进行排序，选择第一个结束的任务作为第一个，然后移除所有和这个任务冲突的任务。然后一直重复这样做，直到集合中为空。这样就可以保证不浪费过多的时间。这个算法的时间复杂度为O($n\\log n$)。主要是排序的时间。\nTask Scheduling Interval Scheduling是只用一个机器调用任务，Task Scheduling是可以用几台机器调用任务，目的是求出想处理所有问题，机器的最小使用数量。贪心仍然可以解决这个问题，只不过我们可以考虑用初始时间来排序了。\n对于每一个任务i，如果这台机器空闲可以处理，那么就把这个任务分配给这台机器。否则，我们分配一个新机器去处理它。重复这个过程直到所有任务都被处理。\n处理n个任务，用最少的机器数量，产生这个调度的时间复杂度为O($n\\log n$)\n为什么贪心有效？（证明）\n假设现在的算法可以找到k个机器来解决问题，突然冒出来个说法说用k-1个机器就可以解决。\n假设k是现在的算法分配的最后一个机器。任务i是这台机器上被分配的第一个任务。根据我们的算法来讲，任务i之所以被分配到第k个机器上，是因为剩下k-1台机器上的任务都与任务i冲突。也就是说这些任务都是开始时间在i之前，但是在i之后结束。如果这些任务都是这样，那也说明了他们之间也是两两冲突的。也就是说，集合中有k个任务是两两冲突的，那就不可能用k-1台机器来解决这个问题。因此，贪心给了一个全局最优解。\nClustering 这个问题说白了就是距离问题，有三点规则\nd($p_i, p_i$) = 0, 对于所有的i d($p_i, p_j$) = 0, $i\\neq j$ d($p_i, p_j$) = d($p_j, p_i$) Maximum Spacing Clustering 目的：给n个对象和一个k值，找到具有最大簇间距的k聚类。\n已知N个点，给出一对点之间距离的定义（比如欧几里德距离），spacing定义为任意两个属于不同类的点之间距离的最小值，要求聚成k个类，使得spacing最大。类似kruskal算法，将所有边从小到大排序，开始每个点属于1个cluster，然后将距离最小的两个点合并，继续下去，直到只剩下k个cluster。\n通过将点分组到簇中，使得每个簇内部的点之间距离的最小值尽可能大，进而使得不同簇之间的最小距离尽可能大。最大化两个cluster之间最近的两个点，才能达到良好的聚类效果（类似放缩思想）。\n时间复杂度分析\n假设有n个点，已知对n个元素进行排序的时间复杂度为O($n\\log n$)，但是我们要计算两两点之间的距离并进行排序，因此$n\\log n$中的n=$n^2$，所以是$n^2\\log n^2 = 2n^2 \\log n$。因此时间复杂度为O($n^2\\log n$)\n聊聊动态规划 （W6） 动态规划的思想和分治比较像，都是将一个大问题拆分成很多子问题来解决。区别是，\n分治将问题拆开后，求得子问题的解，在解决问题是要合并子问题（重复递归调用）得到最终解。 动态规划是将问题拆成若干子问题，求得子问题的解并存储起来，以便下一次需要求解相同的子问题时可以直接使用已经求解过的结果。 当暴力解法不可行的时候，动态规划通常会派上用场。\n使用动态规划的常用手段就是先找到问题的递归解，然后存储子问题的解自底向上迭代计算。什么是递归解？斐波那契数列中f(n) = f(n-1) + f(n-2)就是一个递归解。\nmemoization 正常情况下，动态规划是需要指数级的时间复杂度，因为它需要计算所有情况然后取最优解。但是使用备忘录，将计算过的子问题的解存入数据结构中，不管是数组还是HashMap，这可以避免重复计算相同的子问题，将指数级的时间复杂度降到多项式级别。\n0-1背包问题 困扰了我一晚上的问题，终于搞懂了。我主要的误区在于用贪心的思想来看这道动态规划的题目。动态规划就是找出所有可能的情况，然后在这些可能的情况中找出最优解。\n首先定义状态dp[i][j]，意思是前i个物品，在背包剩余重量为j时，可以获得的最大价值。 因此状态转移方程可以有如下定义 如果要装的第i个物品的重量大于背包的剩余重量j，那么不装这个物品，则dp[i][j] = dp[i-1][j]。 如果要装的第i个物品的重量小于背包的剩余重量j，那么可以选择装或不装，两种情况中，价值更大的一种，即$dp[i][j] = max(dp[i-1][j], dp[i-1][j-w_i] + v_i)$。 到这里我就有疑问了，既然把东西装进去了，怎么可能比不装这个东西的价值要小？这个max的存在根本就没意义？ 这是因为，在某些情况下，选择当前物品可能会导致在之后的决策中错过更高价值的物品。 问题又来了，那我怎么能在装这个物品的时候知道之后的事情？ 我的这个思路就是贪心的思想，总是考虑当前步骤，但实际的动态规划的步骤是这样的\n🌰 举个例子，有三个物品123，W=5, w={1,4,3}, v={2,4,7}\n当我们考虑物品 2 时，假设我们选择了它。这将导致背包剩余承重为 5-4=1，我们无法再选择物品 3，因为它的重量超过了剩余承重。这种情况下，我们应该选择1，现在背包中物品的总价值为 v1 + v2 = 6。 但是，如果我们不选择物品 2，这将保留足够的空间来容纳物品 3。在这种情况下，背包中物品的总价值为 v1 + v3 = 9，这比选择物品 2 的情况要高。\n// i=1 (物品1) dp[1][0] = max(dp[0][0], dp[0][-1]) = 0 dp[1][1] = max(dp[0][1], dp[0][0] + 2) = 2 dp[1][2] = max(dp[0][2], dp[0][1] + 2) = 2 dp[1][3] = max(dp[0][3], dp[0][2] + 2) = 2 dp[1][4] = max(dp[0][4], dp[0][3] + 2) = 2 dp[1][5] = max(dp[0][5], dp[0][4] + 2) = 2 // i=2 (物品2) dp[2][0] = max(dp[1][0], dp[1][-4]) = 0 dp[2][1] = max(dp[1][1], dp[1][-3]) = 2 dp[2][2] = max(dp[1][2], dp[1][-2]) = 2 dp[2][3] = max(dp[1][3], dp[1][-1]) = 2 dp[2][4] = max(dp[1][4], dp[1][0] + 4) = 4 dp[2][5] = max(dp[1][5], dp[1][1] + 4) = 6 // i=3 (物品3) dp[3][0] = max(dp[2][0], dp[2][-3]) = 0 dp[3][1] = max(dp[2][1], dp[2][-2]) = 2 dp[3][2] = max(dp[2][2], dp[2][-1]) = 2 dp[3][3] = max(dp[2][3], dp[2][0] + 7) = 7 dp[3][4] = max(dp[2][4], dp[2][1] + 7) = 9 dp[3][5] = max(dp[2][5], dp[2][2] + 7) = 9 因此，逻辑应该是两个循环，外面的循环是物品的个数，内部的循环是背包总容量，对于每个物品，都要计算出其在不同剩余容量下所对应的价值。最后才能通过max取到最大值。\nWeighted Interval Scheduling 在贪心算法中，有提到过Interval Scheduling，意思是有多个任务，在一台机器上调度，问最多可以调度多少个任务。而Weighted Interval Scheduling，意思是每个任务都带有价值，问如何调度才能使价值最大化。\n这个问题用贪心给出的方案很可能不是最优解，需要使用动态规划。\n首先我们定义p(j)，p(j)表示和任务j不相交，但是最接近的任务i，i\u0026lt;j。\n接下来定义状态，对于一个任务n，机器只有两个选择，调度或者不调度。\n如果调度序列包含n，那么这个序列一定是{1, 2, \u0026hellip;, p(n)} 如果调度序列不包含n，那么这个序列一定是{1, 2, \u0026hellip;, n-1} 然后定义序列$S_j$是对于序列{1, 2, \u0026hellip;, j}的最优解，Opt(j)代表$S_j$的值。\nIf $j\\in S_j$ $Opt(j) = v_j + Opt(p(j))$ If $j\\notin S_j$ $Opt(j) = Opt(j-1)$ 由此可得，$ Opt(j) = max ({ v_j + Opt(p(j)), Opt(j-1) }) $\n为了避免指数级的时间复杂度，先计算出Opt(1)到Opt(n)的值，之后再找Opt(n)的值需要O(nlogn)的时间复杂度\n探秘图-最短路径（W7） Connectivity information 通常是描述一个网络中节点之间连接方式的信息。\nGraph 图是一个对象的集合，包括顶点和边。\nSome Terminologies\n一条edge的两个顶点时adjacent的 一条edge incident在这个顶点上，如果这个顶点是edge的一个顶点 从一个顶点出发的有向边称为outgoing edge，-\u0026gt; 一个顶点接受的有向边称为incoming edge，\u0026lt;- 一个顶点v的degree表示incident在当前顶点上边的数量，记为deg(v) 对于有向图，incoming和outgoing的edge，记为indeg(v)和outdeg(v) Theorem\n对于有m条边的无向图，$\\sum_{v \\in V} deg(v) = 2m$ 对于有m条边的有向图，$\\sum_{v \\in V} indeg(v) = \\sum_{v \\in V} outdeg(v) = m$ 如果无向图中每一对顶点之间都有边连接，则这个图是简单图，simple graph\n如果有向图中每一对顶点之间最多有一条边连接，则这个图是简单图。\nspanning subgraph指的是包含图G中所有顶点的子图\nspanning tree生成树是一个图的spanning graph，同时还是一颗free tree\n如果这个图不是connected，那么图G的最大连接子图叫做图G的connected component。\nConnected, tree和forest的关系\n全连接图是最密集的形式，任意两点都直接相连； 树是无环且连通的，边数恰好比节点数少一； 森林则是多个树的集合，可能是不连通的。 G是一个有n节点和m条边的无向图\n如果G是一颗树，m = n-1。 数学归纳法证明 Base case考虑只有一个节点的图，此时它没有边，满足节点数（1）等于边数（0）加一的条件。只有两个节点的树，也满足条件。 Hypothesis 假设所有具有k个节点的树都满足“节点数等于边数+1”的规则。 Induction step 现在有一个k+1个节点的树，选择一个叶子节点并移除，剩下的图仍然是一颗树，这棵树满足我们的假设“有k个节点和k-1条边”。然后，将移除的叶子节点和边加回到图中。这样，我们就得到了原始的树，它有k+1个节点和k条边。因此这棵树也满足“m=n+1”的规则 如果G的边的数量大于节点的数量，即$m\\geq n$，那么G中存在cycle。 数学归纳法证明 Base case 考虑节点数为1，2，3。如果想要有环，边的数量一定要大于等于节点数。 Hypothesis 节点小于k的图中，边的数量大于等于节点数，那么这个图中一定存在环。 Induction step 现在一个图中有k+1个节点，想要证明当边的数量\u0026gt;=节点数时，这个图中存在环。现在移除图中的一个节点及其所连接的所有边。因为不知道移除的节点附带几条边，所以分成两种情况讨论 移除的点的deg=1，移除了一个节点和一条边，那么现在边的数量依然大于等于节点的数量，符合我们的假设。因此k+1个节点时图中必定有环。 移除的点的deg\u0026gt;1，移除的这个节点由于连接了多于一条的边，所以在移除它之前，图中必然已经存在环。因此，对于节点数为k+1并且边数大于等于k+1的图，仍然可以确定图中一定存在环。 如果G中没有cycle，那么$m\\leq n-1$ 上面的反证，没啥好说的了。 如果G是connected(连通，注意和全连接的区别)，那么$m\\geq n-1$ 数学归纳法证明 Base case 考虑节点数1，2。都是连通的。 Hypoithesis 假设节点数量小于或等于k的图，如果边的数量\u0026gt;=节点数量减1，那么图一定是连通的。 Indection step 现在一个图中有k+1个节点，想要证明当边的数量\u0026gt;=节点数-1时图是连通的（即边的数量为k）。假设当前的图是不连通的，即有至少两个子图。那么当前边的数量一定是小于节点数量-2。这与当前的假设矛盾，因此这个图是连通的。 Adjacency list and matrix 领接列表和邻接矩阵是表示图的两种主要的数据结构\n邻接矩阵是一个二维数组，其中的行和列都代表图中的节点。如果节点i和节点j之间存在一条边，那么矩阵的第i行第j列（和第j行第i列，如果是无向图）的元素就是1（或者是边的权重，如果是加权图），否则就是0。 邻接列表：邻接列表是一个一维数组，数组的每个元素都是一个链表，代表一个节点的所有邻居。如果节点i和节点j之间存在一条边，那么节点j就会出现在节点i的链表中（节点i也会出现在节点j的链表中，如果是无向图）。 Diagraph 有向图 如果一个有向图上任意两个点u, v都满足u reaches v，v reaches u，那么这个图是强连接\n有向图的cycle示例 flowchart LR id1(A)--\u003eid2(B)--\u003eid3(C) id3--\u003eid4(D) id4--\u003eid6(E)--\u003eid7(F) id7--\u003eid1 如果一个有向图没有cycle，这个有向图是acyclic。\nDFS \u0026amp; BFS 深度优先搜索和广度优先搜索的时间复杂度都是O(n+m），n表示图中节点的数量，m表示图中边的数量。因为在搜索过程中，每个节点都需要被访问，每条边也会被检查，因为需要通过边来找到节点的邻居。\nDFS使用栈实现，BFS使用队列实现。\nDFS实现\nvoid DFS(int vertex, boolean visited[]) { visited[vertex] = true; System.out.print(vertex + \u0026#34; \u0026#34;); Iterator\u0026lt;Integer\u0026gt; it = adjLists[vertex].listIterator(); while (it.hasNext()) { int adj = it.next(); if (!visited[adj]) DFS(adj, visited); } } void DFS(int vertex) { boolean visited[] = new boolean[numVertices]; DFS(vertex, visited); } BFS实现\nvoid BFS(int vertex) { boolean visited[] = new boolean[numVertices]; LinkedList\u0026lt;Integer\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); visited[vertex] = true; queue.add(vertex); while (!queue.isEmpty()) { int s = queue.poll(); System.out.print(s + \u0026#34; \u0026#34;); Iterator\u0026lt;Integer\u0026gt; it = adjLists[s].listIterator(); while (it.hasNext()) { int n = it.next(); if (!visited[n]) { visited[n] = true; queue.add(n); } } } } 两种算法都使用了一个 boolean 数组来追踪哪些顶点已经被访问过。然后，DFS 使用了递归来访问顶点的每个邻居，BFS 使用了一个队列来按照距离顶点的顺序访问邻居。\n单源最短路径 \u0026amp; Dijkstra’s algorithm sssp(single-source shortest path)问题是基于带权重的图。找到从一个顶点到另一个顶点的最短路径，每条边都是带有权重的。\nDijkstra的算法基于广度优先搜索，它考虑了边的权重。\n在开始时，将源节点的距离设为0，将所有其他节点的距离设为无穷大 将所有节点放入一个优先队列（或者最小堆），其中优先级由节点到源节点的距离决定。 从优先队列中取出距离最小的节点，对其所有未访问过的邻居进行松弛操作（如果通过当前节点到达邻居的距离小于邻居当前的距离，则更新邻居的距离。） 将当前节点标记为已访问，并将其从优先队列中移除。 重复上两个步骤，直到优先队列为空，即所有可达的节点都已访问。 细致说下松弛操作：我们检查通过u到v的路径是否比当前从源节点到v的路径更短。也就是说，我们比较d[v]和d[u] + w(u, v)。如果d[u] + w(u, v)更小，那么我们就更新d[v]的值为d[u] + w(u, v)。有点动态规划的思想。\n最大流问题和二分图匹配 Ford-Fulkerson algorithm 这是解决最大流问题的算法\n最大流问题：有一个有向图，其中每条边都有一定的容量。还有两个特殊的节点：源节点（source）和汇点（sink）。目标是找到一种方式，使得从源节点到汇点的流量最大，同时遵守每条边的容量限制。下面是最大流问题的图。 flowchart LR id1(s)--3/7--\u003eid2(v1)--2/2--\u003eid3(v4)--6/6--\u003eid4(t) id2--1/1--\u003eid5--4/5--\u003eid3 id1--5/5--\u003eid5(v3)--2/3--\u003eid4 id1--6/6--\u003eid6(v2)--6/9--\u003eid7(v5)--6/8--\u003eid4 id6--0/3--\u003eid5 数字的含义：3/7，该条边承担的最大流量为7，现在通过已经这条边的流量为3。\nFord-Fulkerson algorithm的重要思想\naugmenting paths 增广路径：是一条从源点到终点的路径，路径上的所有边的剩余容量都大于零。 residual networks 残叉网络 Forward edges 原始网络中存在的边，它们在残差网络中的容量等于原始容量减去当前流量。表示可以在这条边上增加多少流量。 Backward edges 原始网络中不存在的边，它们在残差网络中表示可以减少多少流量。向后边是原始边的反向边，容量等于原始边的当前流量 cuts 削减 没有增广路径存在时，流量是最大的。 一个cut(S, T) 是对图 G = (V, E) 的顶点集 V 的一种划分，其中 V 是顶点集，E 是边集。在这个划分中，S 和 T = V - S 是两个不相交的子集，它们的并集是 V，即每个顶点要么在 S 中，要么在 T 中，且源点 s 属于 S，汇点 t 属于 T。一个cut的容量是所有从 S 到 T 的边的容量之和 Max-Flow/Min-Cut Theorem: Max-Flow的值等于Min-cut的容量\nBipartite graphs \u0026amp; Max matching Bipartite graphs\n在一个二分图中，所有的顶点可以被分为两个不交的集合（我们通常称之为部分或者分区），并且图中的每一条边都连接了一个集合中的顶点和另一个集合中的顶点。也就是说，不存在一条边连接了同一个集合中的两个顶点。\nMax matching\n一个匹配是图中一组边的集合，满足这组边中的任意两条边都不共享顶点。最大匹配则是包含边数最多的匹配。 常见的经典问题：假设有一组工人和一组任务，每个工人可以完成特定的任务，但每个工人一次只能完成一个任务，每个任务也只能由一个工人完成。这样的问题可以通过寻找二分图的最大匹配来解决。\n加密与数论 数论 可整除性 a|b表示a能整除b，b是a的倍数，b=ak。 如果 a|b 并且 b|c, 那么 a|c. (transitivity) 如果 a|b 并且 a|c, 那么 a|(i · b + j · c). i和j是任意整数 如果 a|b 并且 b|a，那么a和b互为相反数。 算术基本理论 一个质数集合$p$ = {$p_1, p_2, \u0026hellip;, p_k$}，一个整数集合$e$ = {$e_1, e_2, \u0026hellip;, e_k$} 任何一个整数m都可以表示成$m$ = $p_1^{e_1}p_2^{e_2}\u0026hellip;p_k^{e_k}$ 最大公约数(GCD) gcd(a,b)=1，a和b是relatively prime a\u0026gt;0, gcd(a,0)=a or gcd(0,a)=a gcd(a,b)=gcd(|a|,|b|)，无论ab是正是负。 gcd(0,0) is undefined. The modulo operator and congruences(模和同余) 如果 a mod n = b mod n. In this case, a is congruent to b modulo n（a和b对n同余）, note a $\\equiv$ b (mod n) 如果a $\\equiv$ b (mod n)，意味着a-b=kn −10 mod 3 = 2, -10/3 = -4, so -10 - (-4)*3 = 2 Euclid’s algorithm 一种找最大公约数的算法 如果$a\\ge b \u0026gt; 0$，gcd(a, b) = gcd(b, a mod b) while b!=0 do (a,b)\u0026lt;-(b,a mod b) return a 加密通信 对称加密 传统加密机制中，Alice和Bob共享一个公钥k。这个公钥是用来加密和解密的，之所以叫对称加密，是因为发出者和接收者使用的是同一个密钥来加密/解密信息。\n对称密码中最简单的一种形式是substitution cipher(替换密码)。加密过程：\n加密：明文M中的每个字符x都通过密钥π映射到对应的字符y。比如，如果π定义为\u0026quot;A变为D，B变为H\u0026quot;，那么明文\u0026quot;AB\u0026quot;就会被加密为\u0026quot;DH\u0026quot;。 传输：加密后的密文C被发送给接收者。 接收者在知道密钥π的情况下可以轻松地解密密文： 解密：密文C中的每个字符y都通过π的逆排列$x=π^{−1}(y)$映射回原始的字符x。继续上面的例子，如果接收者知道\u0026quot;A变为D，B变为H\u0026quot;，那么他就可以将密文\u0026quot;DH\u0026quot;解密回原始的明文\u0026quot;AB\u0026quot;。 缺点：但这很容易被破解。。。 另一种是凯撒密码。明文中的每个字符x被替换成新的字符y，其计算公式为 y = (x + k) mod n。其中：\nn是字母表的大小。例如，如果我们使用英语字母表，那么n就是26，因为英语字母表包含26个字母。 k是秘密密钥，是一个正整数，并且满足1 ≤ k \u0026lt; n。这个密钥决定了字符替换的位移。例如，如果k = 3，那么字母\u0026quot;A\u0026quot;就会被替换成字母\u0026quot;D\u0026quot;，字母\u0026quot;B\u0026quot;替换成\u0026quot;E\u0026quot;，依此类推。当到达字母表的末尾时，会从字母表的开头继续替换。 缺点：k值有限，非常容易被暴力破解，今天也没人用。。。 一种安全的对称加密(OTP)一次性密码本方案。其工作原理如下：\n生成密钥：Alice和Bob共享一个随机比特串K，长度至少与他们要发送的消息M一样长。比特串K用于加密和解密的对称密钥。 加密：Alice将明文消息M与密钥K进行异或操作($0\\oplus0=1\\oplus1=0, 0\\oplus1=1$)，生成密文C。因为K是随机的，所以C也是随机的，没有任何可以用来猜测M或K的模式。 传输：密文C被发送给Bob。 解密：Bob收到密文C后，他也将C与同样的密钥K进行异或操作，恢复出明文消息M。这是因为任何数值与自身异或两次都会返回原始值。$C\\oplus K = (M\\oplus K)\\oplus K=M\\oplus(K\\oplus K)=M\\oplus0=M$。 所以，理论上讲只要密钥K不被拦截窃取，且每个密钥只使用一次，那么密文就无法被破解。 缺点：K会非常大，并且只能用一次，很多应用场景被限制住了。我们倾向于可以重复使用并且简短的密钥 非对称加密 基于OTP的需求痛点，公钥加密出现了\n公钥加密 假设加密函数是E，解密函数是D，消息是M，公钥加密有一下几个性质\nD(E(M)) = M，经加密函数处理后再解密，得到加密前的消息M D和E函数都很易于计算。花费太长时间会使很多应用场景失去意义 由D推导出E被认为是不可能的，不然每个人都很容易被攻击 E(D(M)) = M，在一些公钥系统中，这是成立的，但不是所有的公钥系统都满足这个属性。它表明解密后的消息可以再次被加密回到原始消息。这个性质在某些情况下是有用的，比如数字签名，但并不是所有的公钥密码体系都需要（或满足）这个性质。 加密函数E也被称为单向函数，即无法通过它推导出D。这也是为什么这种加密方式叫做公钥加密。加密函数是公有的，解密函数E是私有的。这就不需要担心消息发送过程中的安全，因为只要解密函数不被偷走，即使消失被半路拦截也无法被解密。 RSA加密模式 RSA是最早提出的一种公钥加密算法之一。\n主要思想： p和q是两个不同的大质数。令n=pq, $\\phi(n)=(p-1)(q-1)$，选择两个数字e和d满足以下条件\n$1\u0026lt;e\u0026lt;\\phi(n)$ $e, \\phi(n)$是relatively prime，即$gcd(e, \\phi(n))=1$ $ed\\equiv 1 ($ mod $ \\phi(n))$ (e, n)这个值对就是公钥，d就是私钥。\nRSA加密\nM是一个整数，0\u0026lt;M\u0026lt;n，M是每个字符的ASCII值的拼接，如果大于等于n，就把M拆成几块 用公钥e和n来加密M，$C = M^e$ mod $n$ RSA解密\n$M = C^d$ mod $n$ 证明过程在这里 NP Complete Hard computational problems 有很多问题难以找到一个有效的算法来解决。但是我们也无法证明出这些问题就是难以解决的，我们只是无法确定是否有算法存在来解决这种问题还是说这种问题就是无法解决的。所谓有效的算法指的是可以在多项式时间内解决所有输入的函数。\n0-1背包问题就是一个很典型的NP完全问题，因为其时间复杂度是 O(nW)，（n 是物品的数量，W 是背包的容量）。动态规划解法的时间复杂度 O(nW) 被认为是非多项式的，因为 W 是背包容量的具体数值，而不是它的位数（所以O(nW)实际是O(nlogW)，整数W用logW位的二进制数字表示。相当于执行了$2^W$次操作，这是一个指数时间的算法）。所以，如果 W 的值增长非常大，那么算法的运行时间会急剧增长。\n但是可以在多项式时间内验证一个给定的解是否正确。 这是通过检查给定解的总价值和总重量来完成的。 假设我们有一个可能的解，即一个标识每个物品是否被选择放入背包的向量。我们可以通过以下步骤在多项式时间内验证这个解是否有效：\n初始化总重量和总价值为0。 遍历物品列表，对于每个被选择的物品，将其重量加到总重量，并将其价值加到总价值。 最后，我们检查总重量是否超过背包的容量。如果超过，那么这个解是无效的。如果没有超过，那么这个解是有效的，总价值是我们得到的解的价值。 这个过程的时间复杂度是 O(n)，其中 n 是物品的数量，因此可以在多项式时间内完成。 Hamiltonian Graph 如果一个图是哈密顿图，那么在这个图中就可以找到一条路径，该路径恰好访问图中的每个顶点一次，并且如果这个路径是闭环（即起始和终止于同一顶点）。需要注意的是，确定一个给定的图是否是哈密顿图是一个著名的NP完全问题，意味着没有已知的多项式时间算法可以解决这个问题。\n决策/优化问题 决策问题(decision problem)的输出要么是yes要么是no。 优化问题(optimisation problem)中我们尝试最大或最小化某些函数。 一个优化问题通过添加一个参数k就可以转换成决策问题。这个函数的值是否最大（或最小）为k。 如果解决一个决策问题很困难，那么与其相关的优化问题也会变的很困难。 一个🌰，还挺助于理解的。\n优化问题：G是一个连通图，每条边的权重都是整数。求G的最小生成树的权重值 决策问题：G是一个连通图，每条边的权重都是整数，现在有一个整数k，是否有这样一个生成树权重值不超过k？ 用二分查找的思想找到优化解\n假设B是0-1背包问题的最大可能收益 我们首先问，是否有一个子集能使收益达到至少B/2？ 如果存在这样的子集，我们继续问，是否有一个子集能使收益达到至少3B/4？ 如果不存在这样的子集，我们继续问，是否有一个子集能使收益达到至少B/4？ 这个过程就是不断地在可能的最大收益范围内进行二分查找，逐步缩小搜索范围，直到找到最大可能收益。这个策略的效率在于它利用了二分查找的思想，每次都将搜索范围减半，从而减少了需要检查的可能性。 Complexity Class P 这是一个决策问题的集合，这些决策问题的最坏的情况都可以在多项式时间内被解决。\n首先，决策问题是那些输出结果为\u0026quot;是\u0026quot;或\u0026quot;否\u0026quot;的问题，例如“这个图有没有哈密尔顿路径？”或者“这个数是不是素数？”\n这段话中的算法A是一个假设存在的算法，它可以在多项式时间内解决一个决策问题。具体来说，如果决策问题s的答案是“是”，那么算法A可以在时间p(|s|)内确定这个结果。这里，|s|表示问题s的“大小”，而p(·)是一个多项式。\n这意味着，如果我们能找到这样的算法A，那么我们就能说这个决策问题可以在多项式时间内解决，或者说这个问题在计算复杂性理论中属于P类问题。\nComplexity Class P 问题举例\nminimum spanning tree, fractional knapsack, shortest paths in graphs with non-negative edge weights, maximum flows, Euler tours, and task scheduling. Efficient Certification 粗浅的说就是对某个问题给出一个解决方案，可以在多项式时间内检查这个解决方案是否正确。证书指的就是给出的解决方案。\nComplexity class NP 这个集合包含所有能在多项式时间内被“验证”或“证明”其解的决策问题。NP类由决策问题组成，这些决策问题中都存在Efficient Certification。\nP21-41 lec10d\n","permalink":"https://martinspace.top/zh/%E7%AE%97%E6%B3%95comp202/","summary":"时间复杂度和online algorithm (W1) 算法分析中我们常用到三个符号：O，Ω(Omega) 和 Θ(Theta) O表示的是渐进上界，它描述了算法在最坏情况下","title":"算法(COMP202)"},{"content":"复（预）习过程中遇到Petri net，记录一下\nPetri Net是一种系统模型。因为不确定哪个transition会先fire，所以它是非确定性的，通常被用于建立离散分布式系统。\nPetri Net组成元素\nplace: 通常用圆圈表示 token: 在place中，通常用实心点表示 transition: 通常用直角矩形或实心矩形表示 arc: 通常是带箭头的一条线，权重可以通过标数字在线上，或者画出几条arc来表示。 flowchart LR br\u003c--\u003eid1((...)) id1((...))==2==\u003err id2((..))--\u003ebr id2((..))==2==\u003ebb bb--\u003eid2((..)) rr--\u003eid2((..)) 找出上述Petri net中reachable state和deadlock state的数量\nflowchart TD id1(3, 2)==bb/br==\u003eid2(3, 1) id1(3, 2)==rr==\u003eid3(1, 3) id2(3, 1)==rr==\u003eid4(1, 2) id3(1, 3)==bb/br==\u003eid4(1, 2) id3(1, 3)==br==\u003eid5(3, 0) id4(1, 2)==bb/br==\u003eid6(1, 1) id5(3, 0)==rr==\u003eid6(1, 1) id6(1, 1)==br==\u003eid7(1, 0) 由上图可知，这里有7个reachable states，一个死锁状态，当左侧place中没有token并且右侧place只有1个token时，rr, bb, br都不是enabled的状态，因此都不能被fire。所以只有一个死锁状态。\n","permalink":"https://martinspace.top/zh/high-level-petri-netcomp201/","summary":"复（预）习过程中遇到Petri net，记录一下 Petri Net是一种系统模型。因为不确定哪个transition会先fire，所以它是非确定性的，通","title":"High-level Petri Net(COMP201)"},{"content":"将x减到0的最小操作数(1658) 给你一个整数数组 nums 和一个整数 x 。每一次操作时，你应当移除数组 nums 最左边或最右边的元素，然后从 x 中减去该元素的值。请注意，需要 修改 数组以供接下来的操作使用。\n如果可以将 x 恰好 减到 0 ，返回 最小操作数 ；否则，返回 -1 。\n示例1 输入：nums = [1,1,4,2,3], x = 5 输出：2 解释：最佳解决方案是移除后两个元素，将 x 减到 0 。 双指针 这里引用灵神的一句话\n窗口大小不固定的叫做双指针，窗口大小固定的叫做滑动窗口。 我觉得很合理，虽然官方称自己的解法为滑动窗口，我更愿意称其为双指针。\nclass Solution { public int minOperations(int[] nums, int x) { int n = nums.length; //计算出数组中所有元素总和 int sum = Arrays.stream(nums).sum(); if(sum\u0026lt;x){ return -1; } int right = 0; // lsum记录前缀的和，rsum记录后缀的和 int lsum = 0, rsum = sum; int ans = n+1; // 初始使前缀为空，left=-1； // 后缀为整个数组，right不断向右移并做减法。 // 当lsum+rsum\u0026lt;x时，前缀指针向右移动，并做加法。 // 最后的结果计算是根据左右指针的位置，(left+1)+(n-right) // left+1表示左边操作多少次，n-right表示右边操作多少次 for(int left = -1; left\u0026lt;n; ++left){ if(left!=-1){ lsum += nums[left]; } while(right\u0026lt;n \u0026amp;\u0026amp; lsum+rsum\u0026gt;x){ rsum -= nums[right]; ++right; } if(lsum+rsum==x){ ans = Math.min(ans, left+1+n-right); } } return ans \u0026gt; n ? -1 : ans; } } 复杂度分析 时间复杂度：O(n)，其中 n 是数组 nums 的长度。left 和 right 均最多遍历整个数组一次。\n空间复杂度：O(1)\n还原排列的最少操作步数(1806) 这道题有意思，数学解法，实话说高中毕业后就很少接触数学了，但是好在我还看的懂答案，写在博客里记录一下吧。\n给你一个偶数 n​​​​​​ ，已知存在一个长度为 n 的排列 perm ，其中 perm[i] == i​（下标 从 0 开始 计数）。\n一步操作中，你将创建一个新数组 arr ，对于每个 i ：\n如果 i % 2 == 0 ，那么 arr[i] = perm[i / 2] 如果 i % 2 == 1 ，那么 arr[i] = perm[n / 2 + (i - 1) / 2] 然后将 arr​​ 赋值​​给 perm 。\n要想使 perm 回到排列初始值，至少需要执行多少步操作？返回最小的 非零 操作步数。\n欧拉定理\u0026amp;欧拉函数 记录题目之前，要先写一下欧拉定理和欧拉函数，方便更多人看懂这道题。\n欧拉定理：当a和m两个数互素时，即gcd(a, m)=1，则a^(ψ(m)) = 1(mod m) 欧拉函数：ψ(m)就是欧拉函数表示的是小于等于m和m互质的数的个数 数学解法 对于原排列中的第i个元素，设g(i)为进行一次操作后，该元素的新下标，\ng(i)为偶数，arr[g(i)] = perm[g(i)/2]，令x=g(i)/2，则等式转换为arr[2x] = perm[x]，x∈[0, (n−1)/2] g(i)为奇数，arr[g(i)] = perm[n/2+(g(i)-1)/2]，令x=n/2 + (g(i)-1)/2，则等式转换为arr[2x-n-1] = perm[x]，x∈[n/2, n-1] 因此可以总结出\n0 \u0026lt;= i \u0026lt; n/2，g(i) = 2i; n/2 \u0026lt;= i \u0026lt; n，g(i) = 2i-(n-1); 对于除首尾元素外的其他元素，上述表达式可转换为对(n-1)取模，可以转换为g(i) = 2i(mod(n-1))，这个方法很微妙，我这个水平肯定是想不到了。\n令 g^k (i) 表示原排列 perm 中第 i个元素经过 k 次变换后的下标\n举例： g^2 (i) = g(g(i))\n所以，g^k (i) = 2^k i(mod(n-1))\n根据上述推论，我们直接模拟即找到最小的 k 使得满足 2^k ≡ 1(mod(n−1)) 即可。\n代码\nclass Solution { public int reinitializePermutation(int n) { if (n == 2) { return 1; } int step = 1, pow2 = 2; while (pow2 != 1) { step++; pow2 = pow2 * 2 % (n - 1); } return step; } } 复杂度分析 欧拉定理和欧拉函数用来计算时间复杂度\n由于n是偶数，则n-1一定是奇数，n-1和2互质，根据欧拉定理，gcd(2, n-1)=1，所以2^(ψ(n-1)) = 1(mod (n-1))\n由此可知，k = ψ(n-1)，所以k一定小于n-1，因此总的时间复杂度不超过 O(n)。\n破解保险箱(753) 欧拉回路题目。Leetcode的每日一题真的很耗费精神，但是都很有意思，值得记录一下。\n有一个需要密码才能打开的保险箱。密码是 n 位数, 密码的每一位是 k 位序列 0, 1, ..., k-1 中的一个 。\n你可以随意输入密码，保险箱会自动记住最后 n 位输入，如果匹配，则能够打开保险箱。\n举个例子，假设密码是 345，你可以输入 012345 来打开它，只是你输入了 6 个字符。(保险箱依旧识别最后三位输入)\n请返回一个能打开保险箱的最短字符串。\n贪心构造 这道题的方法很巧妙，值得记录一下\n将所有的n-1位数作为节点，每个节点都有k条边，每个节点通过一条边可以走到对应的另一个节点。\n能构造出连续值的最大数目(1798) 给一个长度为 n 的整数数组 coins ，它代表你拥有的 n 个硬币。第 i 个硬币的值为 coins[i] 。如果你从这些硬币中选出一部分硬币，它们的和为 x ，那么称，你可以 构造 出 x 。\n请返回从 0 开始（包括 0 ），你最多能构造出多少个连续整数。(你可能有多个相同值的硬币。)\n总结 这道题我最初的做法是用hashmap存储coins的值，然后遍历coins中的元素，用目标值coins中的元素，然后看该元素在hashmap中是否存在，但是我忽略了可能有相同硬币这个问题。\n看了答案发现这道题思路很巧，理解了半天才绕过这个弯。首先将coins数组升序排列，然后定义一个区间[l, r]，这个区间意思是有一串连续的整数，从l到r。那么现在可以设一个区间[0, x]，找到一个值y，如果y\u0026lt;=x+1，那么就证明[0, x+y]也是一组连续的整数。就是这里，我理解了有一会才反应过来。通俗一点讲，已知0到x这串连续整数，新来一个y，y+0一定要小于等于x+1，才可以保持连续。举个例子：现在有一串连续的整数[0, 2]，那么下一个数(即y)一定要小于等于3。因为前方序列中有0, 1, 2。0+3=3, 1+3 = 4, 2+3 = 5。 代码如下\nclass Solution { public int getMaximumConsecutive(int[] coins) { int res = 1; //先将数组升序排列 Arrays.sort(coins); for (int coin : coins) { //判断y,若\u0026gt;x+1, 则推出循环，说明后面也不会有值满足条件了 //否则，将结果+1 if (coin \u0026gt; res) { break; } res += i; } return res; } } ","permalink":"https://martinspace.top/zh/leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/","summary":"将x减到0的最小操作数(1658) 给你一个整数数组 nums 和一个整数 x 。每一次操作时，你应当移除数组 nums 最左边或最右边的元素，然后从 x 中减去该元素的","title":"Leetcode每日一题"},{"content":"布局小部件(Layout Widgets) Container Row \u0026amp; Column Stack Padding Align \u0026amp; Center Expanded \u0026amp; Flexible Wrap GridView ","permalink":"https://martinspace.top/zh/dart-flutter%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","summary":"布局小部件(Layout Widgets) Container Row \u0026amp; Column Stack Padding Align \u0026amp; Center Expanded \u0026amp; Flexible Wrap GridView","title":"Dart \u0026 Flutter学习笔记"},{"content":"Weekly Assessment 错题知识点复习 Week 1 insert语句语法\ninsert into table_name VALUES (value1, value2) 创建外键语法\nCONSTRAINT fk_name FOREIGN KEY (从表的属性名) REFERENCES 主表(主表的属性名) 有两个表A(a), B(b)。a是主键，b通过外键连向a，即B表索引自A表。此时，只有对B表进行删除操作是肯定可行的。\n其余操作：\n在A表中插入一个值：可能会打破主键约束。 从A表中删除一个值：可能会打破外键约束。 从B表中插入一个值：可能会打破外键约束。 Week 2 week2主要是sql语句查询，没啥好注意的，发现了再说吧。\nWeek 3 事务的四个隔离级别：\n读未提交(Read Uncommitted)：可以读还没被提交的数据（可以说是根本没隔离） 读已提交(Read Committed):读的都是已经提交的数据 可重复读(Repeatable Read)：事务一旦开始，事务进行过程中读取的所有数据不允许被其他事务修改。 序列化(Serializable)：指事务的操作按顺序执行，不可以插队。（这个概念总是忘，写在这记录一下） Week 4 2PL：\n锁+读写操作 解锁+读写操作 注意：\n看清解锁后是否有上锁行为 重点：一定要看清上锁的事务和数据项以及对应解锁的事务及数据项，很容易在这里出问题。 Week 5 Redo日志如何执行？ 和Undo日志相反\n先找出redo日志中所有的COMMIT。 从第一项到最后一项遍历日志。 如果看见\u0026lt;T, x, v\u0026gt;，并且该事务T有COMMIT标记，那么把磁盘上X的值改为v。 对于未完成的事务T，将\u0026lt;abort T\u0026gt;写入磁盘上的日志。 有关checkpoint 现在有一个Undo/Redo log，使用了ARIES checkpoints。当发生崩溃时，我们需要最先看哪一行\n\u0026lt;START T1\u0026gt; \u0026lt;T1,X,1,2\u0026gt; \u0026lt;START T2\u0026gt; \u0026lt;T2,Y,5,1\u0026gt; \u0026lt;START T3\u0026gt; \u0026lt;COMMIT T1\u0026gt; \u0026lt;CHECKPOINT(T2,T3)\u0026gt; \u0026lt;COMMIT T2\u0026gt; \u0026lt;END CHECKPOINT\u0026gt; 显然，checkpoint出现在第7行，并且end checkpoint出现在第9行，T1和T2已经被commit，所以应该返回第5行，即\u0026lt;START T3\u0026gt;，还没有被提交的事务。\nstrict schedule Strict schedule 一定满足哪个隔离级别？\n一定满足Read Committed，因为Strict Schedule要求是无级联，只允许读取已经提交的值。所以它一定满足level2 Read Committed，但是不一定满足level 3，Repeatable Read。因为没有对可重复读作出要求、\n可重复读指：事务一旦开始，事务进行过程中读取的所有数据不允许被其他事务修改。\n时间戳 timestamp属于一种预防死锁的机制，当一个事务对一个数据拥有锁，而另一个事务试图访问这个数据时，才会激发预防死锁的机制，也就是说，时间戳才能派上用场。否则任何事务都不会重新启动。\nWeek6 第六周主要是知识点需要学明白，索引（B树，B+树）\ncreate index on Students using btree (programme, year); B树（B-树） 为什么选择B树这种数据结构？ 众所周知，大部分树状的数据结构都是二叉树，这就是一个致命缺点。当数据量过于庞大的时候，二叉树会变的很深，而想要搜索值，就要进行很多次磁盘I/O，把数据加载到内存中，这是十分浪费时间的。 而B树（B+树）恰好解决了这一个痛点，这种数据结构可以在每一层的每个节点上存放好多数据，这就会明显减少树的深度，也就是减少了磁盘I/O的次数，当数据量十分庞大时，会节省很多的时间。\nB+树 选择n值使得该节点适合单个磁盘块。n的计算方法： 一个节点中有n个value和n+1个指针。所以 Disk block size\u0026gt;=n*(value_size + pointer_size) + pointer_size。\nLeaves 叶子节点中并不是所有的空间都要被使用 只要确保有floor((n+1)/2)个指针被使用就可以了，只有一片叶子这种情况除外。指向下一个叶子节点的指针也算是指针。\nInner nodes 内部节点也并不是所有空间都要被使用 要确保有ceil((n+1)/2)个指针被使用 例外：根节点必须有2个以上节点\n遇到B+树题思考过程： 确定n值，value有几个方块就是几个n 根据n值计算出每个内部节点和叶子节点需要至少有几个pointers 寻找值：\n从树的root节点开始 如果当前节点不是叶子节点 如果要找的值比第一个小，直接传到第一个子节点 否则找到一个最大的i使得ai\u0026lt;=v，然后传到其对应的子节点 如果当前节点是叶子节点 如果v出现在叶子中，则跟随对应的指针输出 若不在叶子中，则返回\u0026quot;v不存在当前索引中\u0026quot; Running time: O(h*log(n)) 其中log以2为底 插入值/指针对\n下面几步我愿称其为法宝（老师的summary并不完整，这里有所补充）\n找到一个应该包含这个值的叶子节点\n如果这个叶子节点没有满，则插入在该节点合适的位置\n如果这个叶子节点满了 (找传递的下一个叶子节点，如果下一个也是满的，则👇)\n拆分叶子节点给即将插入的值创造空间，并且把该节点一半数量的指针移植到新节点。 插入值/指针对 把叶子节点连到对应的父节点上（也可能会出现新的父节点） 如何更新祖先节点？\n应该更新的祖先节点的值是空的 向右侧指向的下一级 接着一直向左侧指向的下一级 直到到达叶子节点，将找到的叶子节点中最左侧（最小）的值更新在祖先节点上。 PS：如果只更新一级就到叶子节点了呢？那就找到当前叶子节点中最左侧的值更新在其父节点上\nRunning time: O(h*log(n)) 其中log以2为底\n删除值/指针对\n找到包含这个值的叶子节点 移除这个值/指针对 假设现在这个节点是C x=2 如果C是根节点 x=ceil((n+1)/2) 如果C是内部节点 x=floor((n+1)/2) 如果C是叶子节点 如果C的指针数量超过x个，修补祖先节点，结束。 如果C是根节点而不是叶节点，移除该节点，并且让其子节点称为新的根节点 否则，检查相邻节点是否有至少x+1个指针 相邻节点：相同深度，共同祖先，且这个共同祖先在这两个子代节点中间没有任何子代。 如果有，拿走一个，修复祖先节点，结束。 否则，合并兄弟姐妹节点，并转到第三行，父节点为当前节点。 这是老师的方法👆，我觉得不是很好理解，下面自己总结一下👇\n找到包含这个值的叶子节点 移除这个值/指针对 看当前叶子节点的指针数量是否满足要求(\u0026gt;=floor((n+1)/2))。 这里要注意：指向下一节点的指针也算是指针,即使是叶子节点中的最后一个节点（最后一个节点是没有指向下一个节点的指针的，但是也要看做是有指向下一个节点的指针）这是老师回答我的，我觉得很不解和震惊，但是只能妥协。毕竟这不是我发明的数据结构:) 如果移除值后，该叶子节点的指针数量不满足要求，则要从相邻节点”偷“指针过来 相邻节点至少有floor((n+1)/2) +1个指针。因为只有这样才能保证被偷走一个指针后，自己的指针还够用。 相邻节点：相同深度，共同祖先，且这个共同祖先在这两个子代节点中间没有任何子代。 修复祖先节点，口诀right-left-left-left直到找到叶子节点结束。 相邻节点的指针数也太少了，偷不过来怎么办？ 选择一个相邻节点进行合并 内部节点更新也是如此，内部节点需要从相邻节点（指针数\u0026gt;=ceil((n+1)/2)+1）偷一半过来，更新。 记得一层一层更新，直到更新到根节点，最后一定检查有没有漏掉的！！！ 注意：对于相同的一些值，也会有不同的B+ tree的写法 (example: insert \u0026amp; delete 42)\n查询过程如图所示 flowchart TB id1[SQL query]--\u003eid2[\"Parse query and preprocess\"]--\"Logical query plan (relational algebra)\"--\u003eid3[Optimise logical query plan] id3--\"Optimised logical query plan\"--\u003eid4[Select physical query plan]--\"Physical query plan\"--\u003eid5[Query Execution] Week7 查询优化的启发式方法（Heuristics for query optimization） 把selections尽可能的向树的底端推移（目的是尽早摆脱不相关的tuple） 把projections尽可能的向树的底端推移 用equijoins符号代替跟在selection+✖️，因为equijoins比selection+✖️要更快 从初始查询集开始，使用这三种启发式方法，如果我们以所有可能的方式重复应用它们，最终会得到什么结果？\n该集合可能包含多个最优化的查询，但由于管道的存在，它们对于物理查询计划是等效的\n物理查询计划 物理查询计划目的是添加执行优化查询计划所需的信息\n如何将信息从一个操作符传到另一个？\nMaterialisation: 将中间结果写进磁盘 Pipelining: 中间selection的结果会被写进内存中的buffer，projection符号将会读取并直接处理这些元组。（Tuple是行，Attributes是列） 2PC/3PC 协议 因为BASE(Basically Available, Soft state, Eventually consistent)理论需要在一致性和可用性方面作出权衡，因此出现了许多关于一致性的协议，其中就有2pc(2 phase commitment protocol)/3PC协议。\n2PC\nDecide when to commit or abort. Commit or abort. 3PC\nDecide when to commit or abort. Prepare to Commit Commit 三段提交就是把2段提交的第二步拆成了两个步骤，可以说是更详细了，解释一下3PC的第二步Prepare to Commit：\n协调器给各个节点发送commit或abort的message。 各个节点进入prepare to commit状态。 协调者：在一些节点上执行，并且决定本地事务是否提交和何时提交。是事务管理器。\n参与者：就是一些节点，也是资源管理器。\n在每个节点本地都会写入日志，无论是发送message还是从其他节点接收message。\nPhase 1: When to commit 协调器发起询问：问各个节点是否想提交，向各个节点发送\u0026lt;prepare T\u0026gt; 各个节点决定是提交(Commit)还是废弃(Abort) 想提交，向协调器发送\u0026lt;ready T\u0026gt;，并且自身进入precommitted状态，一旦进入这个状态，就只有协调器才能废弃这个事务。 不想提交，发送\u0026lt;don't commit\u0026gt;到协调器并且废弃本地事务。 可以delay，但一定要告诉协调器一个准确答案，提交or废弃。 Phase 2: Commit or abort (假设在给定超时前未回复的节点希望中止)\n如果所有节点都希望commit，协调器发送\u0026lt;commit T\u0026gt;给所有的节点，节点收到消息后进行提交。 如果有节点不想提交，那么协调器发送\u0026lt;abort T\u0026gt;到所有节点，节点收到消息后将事务废弃。 有关提交日志 Phase1 flowchart LR id1[send PREPARE T]--\u003eid2[send READY T] id1--\u003eid3[send DON'T COMMIT T]--\u003eid4[ABORT T1] flowchart LR id0[\"\u003c PREPARE T \u003e to log\"]--\u003eid1 id1[send PREPARE T]--\u003eid7[\"Ensure T1 will not be aborted,就把所有日志写入磁盘\"]--\u003eid5[\"\u003c READY T \u003e to log\"]--\u003eid2[send READY T] id1--\u003eid6[\"\u003c DON'T COMMIT T \u003e to log\"]--\u003eid3[send DON'T COMMIT T]--\u003eid4[ABORT T1] style id1 fill:#f9f,stroke:#333,stroke-width:4px style id6 fill:#f9f,stroke:#333,stroke-width:4px style id7 fill:#f9f,stroke:#333,stroke-width:4px style id5 fill:#f9f,stroke:#333,stroke-width:4px 如果日志中\u0026lt; DON'T COMMIT T \u0026gt;是日志的最后一条，就知道事务是要被废弃了\nPhase2 flowchart LR id1[All nodes responded ready T]--\u003eid2[send commit T] id3[\"Some node responded don’t commit T\"]--\u003eid4[send abort T] flowchart LR id1[All nodes responded ready T]--\u003eid5[\"\u003c COMMIT T \u003eto log\"]--\u003eid2[send commit T] id3[\"Some node responded don’t commit T\"]--\u003eid6[\"\u003c ABORT T \u003eto log\"]--\u003eid4[send abort T] style id5 fill:#f9f,stroke:#333,stroke-width:4px style id6 fill:#f9f,stroke:#333,stroke-width:4px 如果日志中\u0026lt; COMMIT T \u0026gt;是日志的最后一条，就知道事务是被提交了，如果登入日志和send commit T之间发生failuer，则进行redo T1（T1是节点上的事务）操作就可以。\u0026lt; ABORT T \u0026gt;同理\n计算题（一次查询中，计算两个site之间传输的数据大小） 双站点题\n已知A站和B站，A站存了一张表R，B站存了一张表S，在B站上进行一次查询（需要用到A站中存的信息），问A向B站传输的数据是多少\n分别计算A-\u0026gt;B和B-\u0026gt;A-\u0026gt;B传输的数据大小，然后进行比较。\nWeek8 2PC协议中，\u0026lt;PRECOMMIT T\u0026gt;不会被写进日志中。第七周的知识点，被出到了第八周的题里，又狠狠被他水到了，可恶\n\u0026hellip;E[i]:现在这个节点是E的第i个子节点\n\u0026hellip;E[*[i]]:拿到E的第i个子节点向下的所有Element\nWeek9 \u0026lt;pair\u0026gt;{$item},{$title}\u0026lt;/pair\u0026gt;，每个title和每个item都要匹配，最终输出\u0026lt;pair\u0026gt;\u0026lt;/pair\u0026gt;ELement的数量为title的数量✖️item的数量\nWeek10 OLAP: Online Analytic Processing, 指的是分析存在数据仓库里的复杂数据的过程。经常会使用where和group by子句。 OLTP: Online Transaction Processing, 传统的DBMS的任务，能被快速执行的查询和更新；影响数据库的一小部分。 Market-basket model (Data mining) Association Rules (Data mining) 经常买尿布的人也经常买啤酒，General form: {尿布} =\u0026gt; 啤酒 置信度(Confidence)：{i1, i2, ..., in}集合中包含j的概率。 support of {i1, ..., in, j}/support of {i1, ..., in} A-priori ALgorithm: 计算支持\u0026gt;=s的所有数据项J 如果J的支持\u0026gt;=s, 那么J的所有子集的支持都\u0026gt;=s。 有不懂的地方欢迎评论 一起讨论👇 ","permalink":"https://martinspace.top/zh/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0comp207/","summary":"Weekly Assessment 错题知识点复习 Week 1 insert语句语法 insert into table_name VALUES (value1, value2) 创建外键语法 CONSTRAINT fk_name FOREIGN KEY (从表的属性名) REFERENCES 主表(主表的属性名) 有两个表A(a), B(b)。","title":"数据库复习笔记(COMP207)"},{"content":"刷leetcode总能看见哈希表，对哈希表有大致的了解，但是一直不知道HashTable，HashMap和HashSet在Java中的区别。今天看到一个博主的文章再结合官方文档的学习，受益良多，记录一下。\n(: 点我进入官方文档 :)\nHashTable 负载因子（load factor） 我觉得有必要对负载因子进行一波解释，方便更多人看懂这篇文章。\n负载因子：构造hashtable时，初始容量和负载因子决定了容器该什么时候扩容。当负载系数过大时，查询效率大幅降低，但是空间利用率上升，因为相同容量下能存的数据更多；而负载系数太小的话，空间利用率会变的非常低，相同的容量可能存不了几个数据就要扩容。所以根据官方文档的建议，负载因子默认为0.75。\n文档定义 public class Hashtable\u0026lt;K,​V\u0026gt; extends Dictionary\u0026lt;K,​V\u0026gt; implements Map\u0026lt;K,​V\u0026gt;, Cloneable, Serializable 继承了Dictionary并且实现了Map接口以及接口的可克隆化和序列化\n可克隆化：可以被克隆一个一样的\n序列化：在进行IO操作时将对象数据转换为字节流，之后将字节流数据转换为特定的对象。\nHashTable的几个常用方法 返回类型 方法 功能 boolean contains(Object value) 检查某个键是否映射到这个hashtable中的指定值。 boolean containsKey(Object key) 检查指定的对象是否是这个hashtable中的一个键。 boolean containsValue(Object value) 如果这个hashtable将一个或多个键映射到这个值，则返回true。 V put​(K key, V value) 将指定的V映射到该hashtable中的指定K。 V get(Object key) 返回指定键所映射的值，如果不存在这个映射，则返回null V remove​(Object key) 从hashtable中删除键（和它对应的值）。 boolean isEmpty() 检查这个hashtable是否有将键映射到值。 V getOrDefault(Object key, V defaultValue) 返回指定的键被映射到的值，如果没有键的映射的值，则返回defaultValue。 PS: V \u0026ndash; 映射值的类型 K \u0026ndash; 映射所维护的键的类型\nHashtable类实现了哈希表，可以将键和值进行对应。Hashtable支持同步，但不支持空值。由于其同步特性，它是线程安全的。\nHashMap 文档定义 public class HashMap\u0026lt;K,​V\u0026gt; extends AbstractMap\u0026lt;K,​V\u0026gt; implements Map\u0026lt;K,​V\u0026gt;, Cloneable, Serializable 显然，HashMap和HashTable功能基本相同，只不过HashMap时继承AbstractMap\nHashMap的几个常用方法 常用的几个方法与HashTable基本相同，查看上面的表格即可。\nHashMap与HashTable的区别 主要区别：线程安全，空值，同步，速度\nHashtable是线程安全，而HashMap非线程安全. HashMap可以使用null作为key，而Hashtable则不允许null作为key HashMap的初始容量为16，Hashtable初始容量为11，两者的填充因子默认都是0.75 HashMap扩容时：new capacity = 2*capacity Hashtable扩容时：new capacity = 2*capacity + 1 两者计算hash的方法不同 HashSet 文档定义 public class HashSet\u0026lt;E\u0026gt; extends AbstractSet\u0026lt;E\u0026gt; implements Set\u0026lt;E\u0026gt;, Cloneable, Serializable 可以看到，HashSet继承了AbstractSet，实现了Set接口。HashSet是基于HashMap来实现的，是一个不允许有重复元素的集合。 因此，HashSet也允许有空值，而且HashSet是线程不安全的。\nHashSet的几个常用方法 返回类型 方法 功能 boolean add(E e) 如果指定的元素还没有出现，则将其添加到这个集合中。 boolean contains(Object o) 如果这个集合包含指定的元素，则返回真。 boolean isEmpty() 如果这个集合不包含任何元素，则返回true。 int size() 返回这个集合中的元素数量。 简单理解：HashSet就是简单的HashMap\n参考文章👇 Java中HashSet、HashMap和HashTable的区别\n","permalink":"https://martinspace.top/zh/%E8%AF%A6%E8%A7%A3java%E4%B8%AD%E7%9A%84%E5%87%A0%E7%A7%8Dhash/","summary":"刷leetcode总能看见哈希表，对哈希表有大致的了解，但是一直不知道HashTable，HashMap和HashSet在Java中的区别。","title":"详解java中的几种hash"},{"content":"低开的上半年 被疫情困在家中的半年，每天都幻想回到学校，甚至做梦都是回到学校见到朋友们。那段时间，经历了整天呆在家的自闭；经历了投简历没人理的难过；经历了在家赶due、复习和考试的折磨；经历了办护照的种种坎坷，不过好在，都挺过来了。不过也要感谢那段时间，我终于拿下了驾照，而且经常练习（基本解封之后是每天都开车，渐渐也不需要我妈在旁边陪我了，完全可以独立驾驶了），这可给我在暑假玩的生龙活虎做下了铺垫。终于在五月，我拿到了讯飞的offer，也算是第一份靠自己拿下的实习了，感觉看到了一些光，很兴奋，因为感觉很快能回到学校在暑假见到朋友们是很美好的事情。所以回想起来，2022年上半年没什么给我印象过于深刻的事情，比较平淡，甚至是有些乏味。\n高走的下半年 苏式绿豆汤般的暑假 终于，回到苏州，光速找到并租好房子就开始了我的打工人生活。租的房子就在北校区旁边，公司离家的距离骑自行车也就20分钟到半个小时的路程，我印象最清晰的就是，当时苏州很热，我基本每天早上都是骑车到公司，在楼下麦当劳蹭会空调顺便解决下早饭再上楼上班。所以暑假过后，我对麦当劳的抗拒是有原因的。上班归上班，下班时间和每个周末我记得都很充实，打网球、游泳、骑车、周末租车出去玩，在西山看日出和日落，风很温柔，景色很美。这段时间很潇洒，很酣畅淋漓，这两个月的快乐的feeling是真实的，是源自心底的。至于实习，宏观上讲，我的收获是了解到国内AI龙头的工作氛围，工作制度或体系（这些是泛泛的，属于是耳濡目染的，用语言难以形容感受）。微观上讲，或者说是从技术提升上来看，没有太大提升，只是让我真实感受到计算机的能力（效率）是远超人类的手和眼，python处理数据，几行代码就可以处理我一周手动处理的数据集，这也确实让我对计算机产生很大的兴趣，我觉得这可能是我第一份实习的重要意义。很快，暑假过去了一大半，我办好离校和离职手续去南京准备签证。接着又不知死活的从四十几度的苏州去四十几度的西安玩了几天，转转钟楼，在酒吧小酌听着陕西话版的遇见，夜爬华山看日出，虽然燥热，但这短暂的记忆无法抹去。\n至于标题为什么叫苏式绿豆汤般的暑假，我想写到这也就不言而喻了，这个暑假正如苏式绿豆汤一般，参杂了很多食材，甘甜，清爽，个性，都是绿豆汤的属性，也是这个暑假给我的感受。喜欢它的人疯狂喜欢，讨厌它的人无敌讨厌。但是无所谓，我没必要看别人怎么想，这是我自己喜欢的绿豆汤，是我自己选择的度过暑假的方式，反正，自己开心最大辣。\n来到英国 这应该是人生中最魔幻的一段时间了，初来乍到的一段时间里，感觉一切都不是很真实，但是现在也慢慢习惯了，不然我也不会平心静气坐在这里打字。这三个月的时间里，看了大本钟、伦敦眼、歌剧魅影，这些小时候在课本上出现的东西，如今真的被我看到了；在安菲尔德看欧冠，看到了高中总提起的萨拉赫；在爱丁堡旅行，看到了周杰伦拍mv的取景地，在苏格兰高地，看到好多之前抖音看到的美丽景色。有时候坐在寝室回想这些经历，都会觉得有一丝不真实，像是在梦里，却又实实在在发生了。觉得这未来几年的留学经历会给整个人生留下浓墨重彩的一笔吧。\n小结 暂且总结到这里吧，还要继续复习，因为一会要和朋友们出去跨年，第一次跨两个年有些不适应。反正我很喜欢2022，甚至是20年来最喜欢的一年。\nlove 2022 sooooo much, it’ll also be well missed in the future! ","permalink":"https://martinspace.top/zh/%E4%BD%8E%E5%BC%80%E9%AB%98%E8%B5%B0%E7%9A%84%E4%B8%80%E5%B9%B4/","summary":"低开的上半年 被疫情困在家中的半年，每天都幻想回到学校，甚至做梦都是回到学校见到朋友们。那段时间，经历了整天呆在家的自闭；经历了投简历没人理的","title":"低开高走的一年"},{"content":"学了一个小技巧实现可以清晰地实现版本控制，记录一下\n1. 列出标签 $ git tag v1.0 v2.0 也可以模糊查询（-l也可以写成\u0026ndash;list）\n$ git tag -l \u0026#34;v1.8.5*\u0026#34; v1.8.5 v1.8.5-rc0 v1.8.5-rc1 v1.8.5-rc2 v1.8.5-rc3 v1.8.5.1 v1.8.5.2 v1.8.5.3 v1.8.5.4 v1.8.5.5 2. 创建标签 附注标签 $ git tag -a v1.4 -m \u0026#34;my version 1.4\u0026#34; $ git tag v0.1 v1.3 v1.4 git show指令可以查看对应的信息\n$ git show v1.4 tag v1.4 Tagger: Ben Straub \u0026lt;ben@straub.cc\u0026gt; Date: Sat May 3 20:19:12 2014 -0700 my version 1.4 commit ca82a6dff817ec66f44342007202690a93763949 Author: Scott Chacon \u0026lt;schacon@gee-mail.com\u0026gt; Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number 轻量标签 $ git tag v1.4-lw $ git tag v0.1 v1.3 v1.4 v1.4-lw v1.5 如果在标签上运行 git show，没有额外的标签信息。 只会显示出提交信息：\n$ git show v1.4-lw commit ca82a6dff817ec66f44342007202690a93763949 Author: Scott Chacon \u0026lt;schacon@gee-mail.com\u0026gt; Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number 后期打标签 命令git log --pretty=oneline 查询提交历史\n$ git log --pretty=oneline 15027957951b64cf874c3557a0f3547bd83b3ff6 Merge branch \u0026#39;experiment\u0026#39; a6b4c97498bd301d84096da251c98a07c7723e65 beginning write support 0d52aaab4479697da7686c15f77a3d64d9165190 one more thing 6d52a271eda8725415634dd79daabbc4d9b6008e Merge branch \u0026#39;experiment\u0026#39; 0b7434d86859cc7b8c3d5e1dddfed66ff742fcbc added a commit function 4682c3261057305bdd616e23b64b0857d832627b added a todo file 166ae0c4d3f420721acbb115cc33848dfcc2121a started write support 9fceb02d0ae598e95dc970b74767f19372d61af8 updated rakefile 964f16d36dfccde844893cac5b347e7b3d44abbc commit the todo 8a5cbc430f1a9c3d00faaeffd07798508422908a updated readme 假设在 v1.2 时你忘记给项目打标签，也就是在 updated rakefile 提交。 你可以在之后补上标签。 要在那个提交上打标签，你需要在命令的末尾指定提交的校验和（或部分校验和）：\n$ git tag -a v1.2 9fceb02 git show可以看到已经为指定的提交打上标签\n$ git tag v0.1 v1.2 v1.3 v1.4 v1.4-lw v1.5 $ git show v1.2 tag v1.2 Tagger: Scott Chacon \u0026lt;schacon@gee-mail.com\u0026gt; Date: Mon Feb 9 15:32:16 2009 -0800 version 1.2 commit 9fceb02d0ae598e95dc970b74767f19372d61af8 Author: Magnus Chacon \u0026lt;mchacon@gee-mail.com\u0026gt; Date: Sun Apr 27 20:43:35 2008 -0700 updated rakefile ... 推送tag到远程分支 默认情况下，git push 命令并不会传送标签到远程仓库服务器上。 在创建完标签后你必须显式地推送标签到共享服务器上。 这个过程就像共享远程分支一样，运行 git push origin \u0026lt;tagname\u0026gt;\n$ git push origin v1.5 Counting objects: 14, done. Delta compression using up to 8 threads. Compressing objects: 100% (12/12), done. Writing objects: 100% (14/14), 2.05 KiB | 0 bytes/s, done. Total 14 (delta 3), reused 0 (delta 0) To git@github.com:schacon/simplegit.git * [new tag] v1.5 -\u0026gt; v1.5 如果想要一次性推送很多标签，也可以使用带有 --tags 选项的 git push 命令。 这将会把所有不在远程仓库服务器上的标签全部传送到那里。\n$ git push origin --tags Counting objects: 1, done. Writing objects: 100% (1/1), 160 bytes | 0 bytes/s, done. Total 1 (delta 0), reused 0 (delta 0) To git@github.com:schacon/simplegit.git * [new tag] v1.4 -\u0026gt; v1.4 * [new tag] v1.4-lw -\u0026gt; v1.4-lw 删除本地标签 $ git tag -d v1.4-lw Deleted tag \u0026#39;v1.4-lw\u0026#39; (was e7d5add) 删除远程标签 $ git push origin --delete \u0026lt;tagname\u0026gt; 以后每次更新都可以狠狠打上标签了，很开心。\n参考文章👇\nhttps://git-scm.com/book/en/v2\n","permalink":"https://martinspace.top/zh/git%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/","summary":"学了一个小技巧实现可以清晰地实现版本控制，记录一下 1. 列出标签 $ git tag v1.0 v2.0 也可以模糊查询（-l也可以写成\u0026ndash;list） $ git tag -l \u0026#34;v1.8.5*\u0026#34; v1.8.5 v1.8.5-rc0 v1.8.5-rc1","title":"git版本控制"},{"content":"最近发现了一款巨好用的生成reference的神器——Zotero\n首先进入官网👇（科学上网，速度更快）\nhttps://www.zotero.org/\n进入官网后点击Download会看到如下页面\n两个都要下载，zotero connector下载的是浏览器插件，这里只支持chrome，下载好后顶部右侧导航栏的插件会显示出zotero的图标。然后下载安装zotero软件，切记，在chrome使用插件时，一定要保证软件在运行状态。\n接下来打开word，如果word已经打开则需要重启，在顶部找到Zotero字样并点击，点击后可以选择Add/Edit Citation和Add/Edit Bibliography。\n接下来在互联网随便找一篇文章，点击插件保存文章，然后回到Word点击Add/Edit Citation会自动生成in text citation\n点击Add/Edit Bibliography，选择格式（如下图所示）点击OK后会出现输入栏，在栏中输入文章作者的名字，就可以自动生成reference了。\n这个软件最强大的功能不仅在于他会自动排序，还会在自己的文章变动时，对reference作出改变，比如我删掉了一句带有in text citation的句子，那么它对应的reference也会被删除并全部重新排序。\n非常好上手好用的东西，点赞！(虽然感觉之后大部分时间不会再写论文了\u0026hellip;\n","permalink":"https://martinspace.top/zh/zotero%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/","summary":"最近发现了一款巨好用的生成reference的神器——Zotero 首先进入官网👇（科学上网，速度更快） https://www.zotero.org/ 进入官网后点击Download会看到","title":"Zotero快速上手"},{"content":"图片太大了，很占服务器的资源，想看的去朋友圈自取吧\nThe images are so big that take up a lot of resources on the server, so if you\u0026rsquo;d like to see them, go to ins and get them yourself.\n","permalink":"https://martinspace.top/zh/scotland/","summary":"图片太大了，很占服务器的资源，想看的去朋友圈自取吧 The images are so big that take up a lot of resources on the server, so if you\u0026rsquo;d like to see them, go to ins and get them yourself.","title":"Scotland"},{"content":" 大家都是幸运者，世界上只有幸运者。有朝一日，所有的其他人无一例外，都会被判死刑，他自己也会被判死刑，幸免不了。\n这两天在赶due，总会心烦意乱，于是找到了《局外人》这本书，花了两天的空闲时间读完了这个哲学（？）著作。\n第一次看完这本书的感觉就是有些莫名其妙，即荒诞。当我看到中间部分，默尔索对着雷蒙的仇人开了五枪， 而且文中的描述是这样的：是因为当时阳光太强烈，让我感到不适，所以我补了四枪（大致是这个意思）。到这里我觉得默尔索这个人荒诞至极，像是不受道德和法律约束的另外一类不具备社会属性的人。读到后面，更令我大开眼界的是法官（or 检察官，记不清了）。他并没有根据默尔索开枪杀人并且对着一个死人补了四枪这一违背法律的行为作出判决，反而是对他在他妈妈的葬礼中没有哭泣作出大量批判，法官和陪审团等众人的依据不再是杀人，而是不在自己母亲葬礼上哭泣，那这个人就一定是一个残忍的冷血动物。在一桩谋杀案中，道德却成为了判罚的标准。另外，从头到尾他们都在劝说默尔索去相信他们所相信的神，并且对默尔索的无动于衷感到同情甚至是恼火。\n尼采一直强调道德是群居社会的产物，是扼杀个性和个人意识的。根据前文的描述，默尔索显然是一个个人意识大于社会意识的人。社会意识的目的从来不是毁灭异己而是同化异己，虽然默尔索的肉体被判处死刑，这并不代表他的“存在”也跟随生命一起消失。对他而言，判处死刑恰恰使他找到了真理。真理就是，默尔索对于自己存在过的这件事情有十足的把握。在结尾有这样一段话：“我曾以某种方式生活过，我也可能以另一种方式生活。我做过这件事，没有做过那件事。我干了某一件事而没有干另一件事\u0026hellip;\u0026hellip;面对着充满信息和星斗的夜，我第一次向这个世界的动人的冷漠敞开了心扉。我体验到这个世界如此像我，入戏有爱，我觉得我过去曾经是幸福的，我现在仍然是幸福的。为了把一切都做的完善，为了使我感到不那么孤独，我还希望处决我的那一天有很多人来观看，希望他们对我报以仇恨的叫喊声。”\n难以否认这是一个近乎完美的结局，是默尔索活过一生，终于在死亡的面前发现自己存在的真理。他曾经以局外人的身份活着，又将以局外人的身份死去，对默尔索而言，生命的本身就是荒诞的，但是存在的幸福感却是真实的。\n","permalink":"https://martinspace.top/zh/%E8%8D%92%E8%AF%9E%E4%B8%8E%E7%9C%9F%E7%90%86/","summary":"大家都是幸运者，世界上只有幸运者。有朝一日，所有的其他人无一例外，都会被判死刑，他自己也会被判死刑，幸免不了。 这两天在赶due，总会心烦意乱","title":"荒诞与真理"},{"content":" 先在github建立一个远程仓库，可以全部是默认设置\n全局声明git的用户认证信息\ngit config --global user.name \u0026#34;name\u0026#34; git config --global user.email \u0026#34;email@gmail.com\u0026#34; 其中name写自己的名字，email@gmail.com写自己的email\n初始化本地git仓库 cd 本地项目根目录 git init 将项目文件添加到git仓库中 git add . .意思是将当前文件夹下的文件全部添加到仓库中，如果想添加某一个问价，将 .替换成对应的文件名就可以了 5. 将文件commit到仓库\ngit commit -m \u0026#34;提交的消息内容\u0026#34; 将本地仓库关联到github git remote add origin 自己仓库的url地址 上传本地仓库代码至github远程仓库 git push -u origin +main git pull git push origin main 由于新项目github的默认分支从master变成了main，所以之前的博客中提到的master对于新项目来说都会报错，用上述指令就可以成功上传啦（记得敲完一行执行一行）\n上传成功之后可以回到github查看自己的仓库是否多了一些文件，记得刷新！ 之后每次代码更新后只要执行第七步就可以啦。\n","permalink":"https://martinspace.top/zh/%E4%BB%A3%E7%A0%81%E4%B8%8A%E4%BC%A0%E8%87%B3github/","summary":"先在github建立一个远程仓库，可以全部是默认设置 全局声明git的用户认证信息 git config --global user.name \u0026#34;name\u0026#34; git config --global user.email \u0026#34;email@gmail.com\u0026#34; 其中name写自己的名字，email@g","title":"代码上传至github"},{"content":"芜湖！刚刚把CYK算法搞懂了。之前不理解的点在于table cells,我一直认为一个cell是一个三角形，结果始终搞不懂算法的原理。 实际上，一个cell就是一个数字，下面我以input=4举例进行说明。\nCYK table 14 13 24 12 23 34 11 22 33 44 x1 x2 x3 x4 伪代码如下\nfor all cells in last row if there is a production A -\u0026gt; x[i] put A in table cell ii for cells st in other rows // 自下而上推导 if there is a production A -\u0026gt; BC where B is in cell sj and C is in cell (j+1)t put A in cell st 首先，cell表示的是数字，上图中11，22，33，44这样的数字，每个数字都是一个cell。接下来分析伪代码，首先是第一个循环，是对于最下面一行：11，22，33，44。即找出所有能推出xi的字符，这是最好理解的一行，不多赘述。 接下来是第二个循环，从倒数第二行开始，st即表中的12，23，34，13，24，14（st的意思是xs -\u0026gt; xt）。这里定义了三个变量来表示cell，分别是s, j, t。我简单概括下就是st = sj + (j+1)t，13 = 11 + 23 = 12 + 33 = 13 + 43, 因为43不存在，所以13涉及的字符对有(11, 23) 和 (12, 33)。 因此，可以把11和23对应的字符联系起来，在语法中寻找对应的rule，如果有对应的A存在，就把A写入13那个位置；如果有多个，逗号隔开，有几个写几个；如果没有，就写空集的符号。每一个位置都进行类似的推理，耐心足够就可以推出来的。填完全部的空，再从上到下画出parse tree就可以啦！ 一些小技巧：寻找sj和(j+1)t的时候，可以先确定两边的s和t，中间的数字依次递增，直到大于两边的数字。比如👇 1\u0026hellip;\u0026hellip;.3 -\u0026gt; 11+23 -\u0026gt; 12+33 -\u0026gt;13+43 (43不存在，舍弃并停止)\n额外的嘱咐：CYK算法要求使用的grammar rule必须遵守CNF范式，因为这个范式保证了其对应的parsing tree是二叉树。\n","permalink":"https://martinspace.top/zh/cykcomp218/","summary":"芜湖！刚刚把CYK算法搞懂了。之前不理解的点在于table cells,我一直认为一个cell是一个三角形，结果始终搞不懂算法的原理。 实际上，","title":"CYK(COMP218)"},{"content":" 🧑🏻‍💻 可以叫我Martin / Martingale 🌊 专注于底层逻辑思考的学习者 ✏️ CS本科大四，就读于利物浦大学@LivUni 💭 热爱开源，学习无止境 我最近在干什么？ 🪜 现在正在\u0026hellip; 24fall CS硕士申请 贡献开源项目Casibase feat: support texts and images in the response at the same time (进行中\u0026hellip;) feat: support generating images via dalle-3 model (2024-02-20) feat: support sending images to gpt4vision model (2024-02-19) feat: fix reply display for huggingface (2024-01-28) feat: add Cohere Command model provider (2024-01-25) 🌟 同步正在\u0026hellip; Go 高性能计算 算法刷题 🤔 不久之后将会 学习成为hacker（附带正义感的 继续贡献开源项目 (坚持做自己喜欢的事情✊) ","permalink":"https://martinspace.top/zh/about/","summary":"🧑🏻‍💻 可以叫我Martin / Martingale 🌊 专注于底层逻辑思考的学习者 ✏️ CS本科大四，就读于利物浦大学@LivUni 💭 热爱开源，学习无止境 我最近在干","title":"💭 关于"},{"content":"SYDNEY \u003c!DOCTYPE html\u003e City walk Bondi beach With baby😍 Sydney opera house GOLD COAST \u003c!DOCTYPE html\u003e Francis Favourite😆 surfers paradise With baby😍 Parasail 😍 MELBOURNE \u003c!DOCTYPE html\u003e With Baby 😍 ","permalink":"https://martinspace.top/zh/loverdiary/","summary":"SYDNEY \u003c!DOCTYPE html\u003e City walk Bondi beach With baby😍 Sydney opera house GOLD COAST \u003c!DOCTYPE html\u003e Francis Favourite😆 surfers paradise With baby😍 Parasail 😍 MELBOURNE \u003c!DOCTYPE html\u003e With Baby 😍","title":"loverDiary"},{"content":"重置服务器系统后。当使用远程ssh连接后，会显示连接失败。 这里直接给出解决办法(仅以macOS为例):\n打开终端 输入 vim ~/.ssh/known_hosts vim会打开该文件，向下翻，找到服务器对应的IP 按下v进入visual模式，使用方向键选择要删掉的内容（服务器IP对应的内容都要删掉）, 按下d 或者，按下i进入insert模式，一个一个删除 按下esc键，输入:wq+回车，任务完成 切记：一定要在英文输入法情况下按esc切换状态，中文输入模式下按esc不管用的！！！ ","permalink":"https://martinspace.top/zh/%E9%87%8D%E7%BD%AE%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%B3%BB%E7%BB%9F%E5%90%8Essh%E8%BF%9E%E6%8E%A5%E5%A4%B1%E8%B4%A5/","summary":"重置服务器系统后。当使用远程ssh连接后，会显示连接失败。 这里直接给出解决办法(仅以macOS为例): 打开终端 输入 vim ~/.ssh/known_hosts vim会打开该文件，向","title":"重置服务器系统后，ssh连接失败"},{"content":"图片太大了，很占服务器的资源，想看的去朋友圈自取吧\nThe images are so big that take up a lot of resources on the server, so if you\u0026rsquo;d like to see them, go to ins and get them yourself.\n\u003c!DOCTYPE html\u003e London ","permalink":"https://martinspace.top/zh/london/","summary":"图片太大了，很占服务器的资源，想看的去朋友圈自取吧 The images are so big that take up a lot of resources on the server, so if you\u0026rsquo;d like to see them, go to ins and get them yourself. \u003c!DOCTYPE html\u003e London","title":"London"},{"content":"看了几篇博客，感觉博主们把问题描述的好专业，导致本菜看不太懂qwq，这里写下我的个人理解，还望各位大佬多多指正，谢谢！\n首先，DF作为标志寄存器的一个位，其值只能是0或1。\n若DF = 0 每次操作后si, di递增\n若DF = 1 每次操作后si, di递减\n好了，我们现在只要知道这些就够了，至于怎么改变DF的值，使其变成0或1，后面我会讨论到。\nsi, di作为变址寄存器，其值递增或递减的本质就是要逐个取得ds:si/ds:di地址上的数据。\n于是我们很难不联想到对于各种串传送的操作，因为串传送就要一个一个地将数据传送到想要的位置，接下来我介绍两个串传送指令：movsb和movsw。\n我以movsb为例（movsw同理 只不过其单位是字word）：\n;movsb指令相当于做如下操作： ((es)*16 + (di)) = ((ds)*16 + (di)) ;将ds:di指向的数据传送到es:si指向的地址 if(df == 0) inc si inc di if(df == 1) dec si dec di 很明显，movsb指令是将一处内存单元中的字节byte送入另一处，然后判断DF的值来决定是递增还是递减。回归到递增递减的本质可以发现，递增就是将这个数据串中的数据正向传入，递减就是反向传入。\n举例：一个数据串’12345678‘，正向传入的结果是’12345678‘，反向传入的结果是’87654321‘。\n那么作为程序员我们如何控制其方向，总不能是机器说了算吧，于是我们有了CLD和STD指令\nCLD：将DF值设为0\nSTD：将DF值设为1\n于是 当写出CLD时，DF为0，si和di递增，则正向传递\n当写出STD时，DF为1，si和di递减，则反向传递\n所以CLD和STD可被理解为用来设置传递方向，这也是为什么DF叫做Direction Flag。\n此外！！教给大家一个指令 用来直接传送数据串，\n即rep movsb，可通过循环实现（cx）个字符的传送。\nPS: 转载我自己的文章，无所吊谓了\n","permalink":"https://martinspace.top/zh/%E5%AF%B9%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80%E4%B8%ADdf%E6%A0%87%E5%BF%97cldstdrep%E7%AD%89%E6%8C%87%E4%BB%A4%E7%9A%84%E7%90%86%E8%A7%A3/","summary":"看了几篇博客，感觉博主们把问题描述的好专业，导致本菜看不太懂qwq，这里写下我的个人理解，还望各位大佬多多指正，谢谢！ 首先，DF作为标志寄存","title":"对汇编语言中DF标志，CLD，STD，REP等指令的理解"}]
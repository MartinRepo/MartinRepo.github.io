<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>ä»RNNåˆ°Transformers | Martin&#39;s space</title>
<meta name="keywords" content="NLP">
<meta name="description" content="é€’å½’ç¥ç»ç½‘ç»œ ç®€å•çš„ç¥ç»ç½‘ç»œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šè¾“å…¥ï¼Œéšè—å±‚ï¼Œè¾“å‡º æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ä¸€èˆ¬æ˜¯å¢åŠ éšè—å±‚çš„æ•°é‡ã€‚ é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ›´åŠ å…³æ³¨çš„æ˜¯éš">
<meta name="author" content="Martin">
<link rel="canonical" href="https://martinspace.top/zh/nluplus/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bc3e848411f8e2e3b39fd084e8d998d9f2d9782118c440525c90992eaecdc9f0.css" integrity="sha256-vD6EhBH44uOzn9CE6NmY2fLZeCEYxEBSXJCZLq7NyfA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://martinspace.top/icebear.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="https://martinspace.top/icebear.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="https://martinspace.top/icebear.jpg">
<link rel="apple-touch-icon" href="https://martinspace.top/icebear.jpg">
<link rel="mask-icon" href="https://martinspace.top/icebear.jpg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://martinspace.top/zh/nluplus/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css" />
<script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
          },
          svg: {
            fontCache: 'global'
          }
        };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-J8TFFL1ZS7"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-J8TFFL1ZS7', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="ä»RNNåˆ°Transformers" />
<meta property="og:description" content="é€’å½’ç¥ç»ç½‘ç»œ ç®€å•çš„ç¥ç»ç½‘ç»œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šè¾“å…¥ï¼Œéšè—å±‚ï¼Œè¾“å‡º æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ä¸€èˆ¬æ˜¯å¢åŠ éšè—å±‚çš„æ•°é‡ã€‚ é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ›´åŠ å…³æ³¨çš„æ˜¯éš" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://martinspace.top/zh/nluplus/" />
<meta property="og:image" content="https://martinspace.top/zh/nluplus/img/nluplus/whatisrnn.jpg" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2025-02-01T10:39:49+00:00" />
<meta property="article:modified_time" content="2025-02-01T10:39:49+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://martinspace.top/zh/nluplus/img/nluplus/whatisrnn.jpg" />
<meta name="twitter:title" content="ä»RNNåˆ°Transformers"/>
<meta name="twitter:description" content="é€’å½’ç¥ç»ç½‘ç»œ ç®€å•çš„ç¥ç»ç½‘ç»œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šè¾“å…¥ï¼Œéšè—å±‚ï¼Œè¾“å‡º æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ä¸€èˆ¬æ˜¯å¢åŠ éšè—å±‚çš„æ•°é‡ã€‚ é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ›´åŠ å…³æ³¨çš„æ˜¯éš"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "ğŸ“’ æ–‡ç« ",
      "item": "https://martinspace.top/zh/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "ğŸ”¬ ç ”ç©¶è®°å½•",
      "item": "https://martinspace.top/zh/post/2research/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "ä»RNNåˆ°Transformers",
      "item": "https://martinspace.top/zh/nluplus/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "ä»RNNåˆ°Transformers",
  "name": "ä»RNNåˆ°Transformers",
  "description": "é€’å½’ç¥ç»ç½‘ç»œ ç®€å•çš„ç¥ç»ç½‘ç»œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šè¾“å…¥ï¼Œéšè—å±‚ï¼Œè¾“å‡º æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ä¸€èˆ¬æ˜¯å¢åŠ éšè—å±‚çš„æ•°é‡ã€‚ é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ›´åŠ å…³æ³¨çš„æ˜¯éš",
  "keywords": [
    "NLP"
  ],
  "articleBody": "é€’å½’ç¥ç»ç½‘ç»œ ç®€å•çš„ç¥ç»ç½‘ç»œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šè¾“å…¥ï¼Œéšè—å±‚ï¼Œè¾“å‡º\næ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ä¸€èˆ¬æ˜¯å¢åŠ éšè—å±‚çš„æ•°é‡ã€‚\né€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ›´åŠ å…³æ³¨çš„æ˜¯éšè—å±‚ä¸­æ¯ä¸ªç¥ç»å…ƒåœ¨æ—¶é—´ä¸Šçš„æˆé•¿ä¸è¿›æ­¥ã€‚\nBack Propogation Through Time åœ¨æ™®é€šçš„å‰é¦ˆç¥ç»ç½‘ç»œ (feed-forward nn) ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åº”ç”¨åå‘ä¼ æ’­æ¥è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°æƒé‡ã€‚ä½†æ˜¯åœ¨ RNN è¿™ç§å…·æœ‰æ—¶é—´ä¾èµ–çš„ç½‘ç»œä¸­ï¼Œå½“å‰çš„è¾“å‡ºä¸ä»…ä¾èµ–äºå½“å‰è¾“å…¥ï¼Œè¿˜ä¾èµ–äºè¿‡å»çš„éšè—çŠ¶æ€ã€‚è¿™å°±å¯¼è‡´äº†å‚æ•°çš„æ¢¯åº¦è®¡ç®—éœ€è¦æ²¿æ—¶é—´ç»´åº¦è¿›è¡Œä¼ æ’­ï¼Œè€Œ BPTT æ­£æ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„æ–¹æ³•ã€‚ æ•°å­¦æ¨å¯¼å¦‚ä¸‹ï¼š\n(1) è®¾å®šç¬¦å·\nè¾“å…¥åºåˆ—ï¼š$ X = ${$x_1, x_2, â€¦, x_T$} è¾“å‡ºåºåˆ—ï¼š$ Y = ${$y_1, y_2, â€¦, y_T$} éšè—çŠ¶æ€ï¼š$ h_t $ è¡¨ç¤ºæ—¶é—´æ­¥ $ t $ çš„éšè—çŠ¶æ€ å‚æ•°ï¼š $ W_{xh} $ï¼šè¾“å…¥åˆ°éšè—å±‚çš„æƒé‡ $ W_{hh} $ï¼šéšè—å±‚åˆ°éšè—å±‚çš„æƒé‡ $ W_{hy} $ï¼šéšè—å±‚åˆ°è¾“å‡ºçš„æƒé‡ æŸå¤±å‡½æ•°ï¼š$ L(Y, \\hat{Y}) $ï¼ˆå¦‚ MSE æˆ–äº¤å‰ç†µï¼‰ (2) RNN çš„å‰å‘ä¼ æ’­ åœ¨ RNN ä¸­ï¼Œæ¯ä¸ªæ—¶é—´æ­¥çš„è®¡ç®—å¦‚ä¸‹ï¼š $$ h_t = f(W_{hh} h_{t-1} + W_{xh} x_t) $$ $$ \\hat{y} _ t = g(W_{hy} h_t) $$ å…¶ä¸­ï¼š\n$ f $ æ˜¯éšè—çŠ¶æ€çš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ tanh æˆ– ReLUï¼‰ $ g $ æ˜¯è¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ softmaxï¼‰ æ€»æŸå¤±ä¸ºï¼š $ L = \\sum_{t=1}^{T} L_t(y_t, \\hat{y}_t) $\n(3) åå‘ä¼ æ’­ Through Time\nè®¡ç®—æŸå¤±å¯¹è¾“å‡ºçš„æ¢¯åº¦ $ \\frac{\\partial L}{\\partial \\hat{y} _ t} = \\nabla_{\\hat{y}_t} L_t $ è®¡ç®—æŸå¤±ç›¸å¯¹äºè¾“å‡ºçš„æ¢¯åº¦ï¼Œè¿™éƒ¨åˆ†å’Œæ™®é€š BP è®¡ç®—ç±»ä¼¼ã€‚\nè®¡ç®—æŸå¤±å¯¹éšè—çŠ¶æ€çš„æ¢¯åº¦ $ \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_t}{\\partial h_t} + \\frac{\\partial L_{t+1}}{\\partial h_t} + \\frac{\\partial L_{t+2}}{\\partial h_t} + \\dots $ ç”±äºéšè—çŠ¶æ€ $ h_t $ ä¼šå½±å“åç»­çš„æ‰€æœ‰æ—¶é—´æ­¥ï¼Œå› æ­¤æ¢¯åº¦è¦æ²¿æ—¶é—´æ–¹å‘åå‘ç´¯ç§¯ã€‚\nè®¡ç®—å‚æ•°çš„æ¢¯åº¦\nå¯¹éšè—çŠ¶æ€çš„æƒé‡ $ W_{hh} $ï¼š $ \\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_{hh}} $ å¯¹è¾“å…¥æƒé‡ $ W_{xh} $ï¼š $ \\frac{\\partial L}{\\partial W_{xh}} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_{xh}} $ å¯¹è¾“å‡ºæƒé‡ $ W_{hy} $ï¼š $ \\frac{\\partial L}{\\partial W _ {hy}} = \\sum _ {t=1}^{T} \\frac{\\partial L}{\\partial \\hat{y} _ t} \\cdot \\frac{\\partial \\hat{y} _ t}{\\partial W_{hy}} $ åå‘ä¼ æ’­å›ä¼ æ¢¯åº¦\nè®¡ç®— æ¢¯åº¦ä¼ æ’­è·¯å¾„ï¼Œä»æŸå¤± $ L $ åå‘é€šè¿‡æ—¶é—´æ­¥ $ T \\to 1 $ é€æ­¥æ›´æ–°å‚æ•°ã€‚ ä½¿ç”¨ æ¢¯åº¦ä¸‹é™ï¼ˆSGD, Adamï¼‰ æ¥æ›´æ–°æƒé‡ã€‚ æ˜¾è€Œæ˜“è§ï¼ŒBPTTå­˜åœ¨ä¸¤ä¸ªè‡´å‘½é—®é¢˜ï¼š\nç¬¬ä¸€ä¸ªæ˜¯æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ï¼šå½“æƒé‡çŸ©é˜µçš„èŒƒæ•°$||W_{hh}||$å°äº1æ—¶ï¼Œéšç€æ—¶é—´æ­¥çš„å¢åŠ ï¼Œæ¢¯åº¦æŒ‡æ•°çº§è¡°å‡ï¼Œå¯¼è‡´æ—©æœŸæ—¶é—´æ­¥çš„æ¢¯åº¦å‡ ä¹ä¸º 0ï¼Œæ— æ³•æœ‰æ•ˆè®­ç»ƒï¼› å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰ æ¥é™åˆ¶æ¢¯åº¦å¤§å°ï¼›ä½¿ç”¨é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰æˆ– é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰ ä»£æ›¿ RNNã€‚ ç¬¬äºŒä¸ªé—®é¢˜åœ¨äºè®¡ç®—å¼€é”€ï¼šBPTT éœ€è¦å­˜å‚¨æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå› æ­¤å½“åºåˆ—å¾ˆé•¿æ˜¯ï¼Œè®­ç»ƒæ•ˆç‡ä½ã€‚ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼šTruncated BPTTï¼Œä»…åœ¨å›ºå®šçª—å£å¤§å°å†…è¿›è¡Œæ¢¯åº¦ä¼ æ’­ï¼Œè€Œä¸å›æº¯æ•´ä¸ªåºåˆ—ã€‚ BPTT Variation Truncted BPTT ç­–ç•¥ï¼šä¸å±•å¼€æ•´ä¸ªæ—¶é—´åºåˆ—ï¼Œè€Œæ˜¯ä»…åœ¨ å›ºå®šçª—å£å¤§å° $ k $ å†…è¿›è¡Œåå‘ä¼ æ’­ï¼ˆå¦‚$ k=10 $ï¼‰ã€‚ ä¼˜ç‚¹ï¼šå‡å°‘è®¡ç®—é‡ï¼Œé€‚ç”¨äºé•¿åºåˆ—è®­ç»ƒã€‚ ç¼ºç‚¹ï¼šå¯èƒ½å¯¼è‡´é•¿ç¨‹ä¾èµ–ä¿¡æ¯ä¸¢å¤±ã€‚ Online BPTT è®¡ç®—ä¸€ä¸ªæ—¶é—´æ­¥çš„æ¢¯åº¦ ç«‹å³æ›´æ–°å‚æ•°ï¼ˆç±»ä¼¼åœ¨çº¿å­¦ä¹ ï¼‰ã€‚ é€‚ç”¨äº æµå¼æ•°æ®å¤„ç†ï¼ˆå¦‚è¯­éŸ³è¯†åˆ«ï¼‰ã€‚ RNN Variation LSTM å’Œ GRU æ˜¯RNNçš„ä¸¤ç§ä¸»è¦å˜ä½“ï¼Œå®ƒä»¬è¢«è®¾è®¡ç”¨äºè§£å†³ä¼ ç»ŸRNNçš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰é•¿ç¨‹ä¾èµ–ï¼ˆlong-term dependenciesï¼‰ã€‚LSTMå’ŒGRUå¯ä»¥é€šè¿‡é—¨æ§æœºåˆ¶ï¼ˆGatesï¼‰ æ§åˆ¶ä¿¡æ¯æµåŠ¨ï¼Œèƒ½å¤Ÿé€‰æ‹©æ€§ä¿ç•™æˆ–é—å¿˜ä¿¡æ¯ï¼Œæœ‰æ•ˆç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä»è€Œæ›´å¥½åœ°å­¦ä¹ é•¿ç¨‹ä¾èµ–ã€‚\nLSTM (1) ç»“æ„\nLSTM ä¸»è¦å¼•å…¥äº†ä¸‰ä¸ªGateså’Œä¸€ä¸ªè®°å¿†å•å…ƒï¼ˆCell State, $ C_t $ï¼‰ï¼š\né—å¿˜é—¨ï¼ˆForget Gateï¼‰ï¼šå†³å®šä¸¢å¼ƒå¤šå°‘è¿‡å»çš„ä¿¡æ¯ã€‚ è¾“å…¥é—¨ï¼ˆInput Gateï¼‰ï¼šå†³å®šæ›´æ–°å¤šå°‘æ–°ä¿¡æ¯åˆ°è®°å¿†å•å…ƒã€‚ è¾“å‡ºé—¨ï¼ˆOutput Gateï¼‰ï¼šå†³å®šè¾“å‡ºå¤šå°‘éšè—çŠ¶æ€ã€‚ å®Œæ•´è®¡ç®—æµç¨‹ï¼š $$ f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f) $$ $$ i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i) $$ $$ \\tilde{C} _ t = \\tanh(W_c [h_{t-1}, x_t] + b_c) $$ $$ C _ t = f _ t \\odot C _ {t-1} + i _ t \\odot \\tilde{C} _ t $$ $$ o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o) $$ $$ h_t = o_t \\odot \\tanh(C_t) $$\n(2) è¯¦ç»†è§£é‡Š\né—å¿˜é—¨ $ f_t $ï¼šæ§åˆ¶æ˜¯å¦é—å¿˜è¿‡å»çš„è®°å¿†ã€‚ è¾“å…¥é—¨ $ i_t $ï¼šæ§åˆ¶æ˜¯å¦å†™å…¥æ–°çš„ä¿¡æ¯ã€‚ å€™é€‰è®°å¿† $ \\tilde{C}_t $ï¼šå½“å‰æ—¶é—´æ­¥çš„æ–°ä¿¡æ¯ã€‚ è®°å¿†å•å…ƒ $ C_t $ï¼šæ›´æ–°åçš„é•¿ç¨‹è®°å¿†ï¼Œä¿¡æ¯å¯ä»¥ç›´æ¥åœ¨æ—¶é—´è½´ä¸ŠæµåŠ¨ï¼Œä¸æ˜“ä¸¢å¤±ã€‚ è¾“å‡ºé—¨ $ o_t $ï¼šå†³å®šéšè—çŠ¶æ€ $ h_t $ çš„è¾“å‡ºã€‚ $ C_t $ ç›´æ¥é€šè¿‡åŠ æ³•è¿æ¥ï¼Œä½¿å¾—æ¢¯åº¦æµåŠ¨ä¸ä¼šå› å¤šæ¬¡ä¹˜æ³•è€Œæ¶ˆå¤±ã€‚\nGRU GRU æ˜¯ LSTM çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå»æ‰äº†ç‹¬ç«‹çš„è®°å¿†å•å…ƒ $ C_t $ï¼Œä»…ä½¿ç”¨éšè—çŠ¶æ€ $ h_t $ ä½œä¸ºå­˜å‚¨ä¿¡æ¯çš„ä»‹è´¨ã€‚\n(1) ç»“æ„ GRU ä¸»è¦æœ‰ä¸¤ä¸ªé—¨ï¼š\né‡ç½®é—¨ï¼ˆReset Gateï¼‰ï¼šæ§åˆ¶å¦‚ä½•ç»“åˆæ–°çš„è¾“å…¥å’Œè¿‡å»çš„éšè—çŠ¶æ€ã€‚ æ›´æ–°é—¨ï¼ˆUpdate Gateï¼‰ï¼šæ§åˆ¶å¤šå°‘è¿‡å»çš„ä¿¡æ¯ä¿ç•™ï¼Œå¤šå°‘æ–°ä¿¡æ¯åŠ å…¥ã€‚ å®Œæ•´è®¡ç®—æµç¨‹ï¼š $$ r_t = \\sigma(W_r [h_{t-1}, x_t] + b_r) $$ $$ z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z) $$ $$ \\tilde{h} _ t = \\tanh(W_h [r_t \\odot h_{t-1}, x_t] + b_h) $$ $$ h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h} _ t $$\n(2) è¯¦ç»†è§£é‡Š\né‡ç½®é—¨ $ r_t $ï¼šå†³å®šæ˜¯å¦é—å¿˜è¿‡å»çš„éšè—çŠ¶æ€ã€‚ æ›´æ–°é—¨ $ z_t $ï¼šå†³å®šæ–°æ—§ä¿¡æ¯çš„æ··åˆæ¯”ä¾‹ï¼Œç±»ä¼¼ LSTM çš„è¾“å…¥é—¨å’Œé—å¿˜é—¨çš„ç»“åˆã€‚ å€™é€‰çŠ¶æ€ $ \\tilde{h}_t $ï¼šæ–°çš„ä¿¡æ¯ã€‚ æœ€ç»ˆéšè—çŠ¶æ€ $ h_t $ï¼š å½“ $ z_t $ é€¼è¿‘ 1ï¼šå½“å‰çŠ¶æ€ æ¥è¿‘æ–°ä¿¡æ¯ã€‚ å½“ $ z_t $ é€¼è¿‘ 0ï¼šå½“å‰çŠ¶æ€ ä¿ç•™è¿‡å»çš„éšè—çŠ¶æ€ã€‚ GRU ç”¨æ›´æ–°é—¨åˆå¹¶äº† LSTM çš„è¾“å…¥é—¨å’Œé—å¿˜é—¨ï¼Œå› æ­¤è®¡ç®—æ›´ç®€å•ã€‚\nTransformers è™½ç„¶ LSTM å’Œ GRU ä»ç„¶è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†ç°ä»£NLPä»»åŠ¡å¤§å¤šé‡‡ç”¨ Transformerï¼ˆåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼Œå› ä¸ºï¼š\nTransformer å¹¶è¡Œè®¡ç®— æ€§èƒ½æ›´å¼ºï¼Œè€Œ LSTM/GRU ä¾èµ–åºåˆ—å¤„ç†ï¼Œæ— æ³•å¹¶è¡Œã€‚ Transformer é€šè¿‡ Self-Attention ç›´æ¥å»ºæ¨¡é•¿ç¨‹ä¾èµ–ï¼Œè€ŒLSTM/GRUä»ç„¶æœ‰ä¸€å®šä¿¡æ¯è¡°å‡é—®é¢˜ã€‚ ä½†åœ¨è¯­éŸ³å¤„ç†ã€æ—¶é—´åºåˆ—é¢„æµ‹ç­‰ä»»åŠ¡ä¸­ï¼ŒLSTM/GRUä»ç„¶å…·æœ‰è¾ƒå¥½çš„è¡¨ç°ã€‚\nSelf-Attention The fundamental operation of any transformer architecture is the self-attention operation 1.\nè‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ä¸ªä»åºåˆ—åˆ°åºåˆ—çš„æ“ä½œï¼Œå®ƒçš„è¾“å…¥æ˜¯ä¸€ç»„å‘é‡ $x_1, x_2, â€¦, x_t$ï¼Œå¯¹åº”çš„è¾“å‡ºåºåˆ—æ˜¯ $y_1, y_2, â€¦, y_t$ã€‚ï¼ˆæ¯ä¸ªå‘é‡éƒ½æ˜¯kç»´ï¼‰\næ¯ä¸€ä¸ª $y_i$ æ˜¯æ€ä¹ˆè®¡ç®—çš„ï¼Ÿ$y_i$æ˜¯æ ¹æ®å½“å‰ä½ç½®çš„$x_i$å’Œå…¶ä»–æ‰€æœ‰ä½ç½®çš„$x_j$çš„ç›¸å…³æ€§è®¡ç®—å‡ºæ¥çš„ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹ $$y_{\\color{red}{i}} = \\sum_{\\color{blue}{j}} w_{\\color{red}{i}\\color{blue}{j}} x_{\\color{blue}{j}}$$\nå¯¹äº$w_{\\color{red}{i}\\color{blue}{j}}$, å®ƒæ˜¯ç”±$x_{\\color{red}{i}}, x_{\\color{blue}{j}}$çš„ç‚¹ç§¯å‡½æ•°æ¨å¯¼å‡ºæ¥çš„ã€‚ $$w_{\\color{red}{i}\\color{blue}{j}} = \\text{softmax}(w^{\\prime}_{\\color{red}{i}\\color{blue}{j}})$$\nå…¶ä¸­ $$w^{\\prime}_{\\color{red}{i}\\color{blue}{j}} = x^T_ix_j$$\nåŸç†å¯è§†åŒ–å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯æ•´ä¸ªä½“ç³»ç»“æ„ä¸­å”¯ä¸€åœ¨å‘é‡ä¹‹é—´ä¼ æ’­ä¿¡æ¯çš„æ“ä½œã€‚ ä¸ºä»€ä¹ˆself-attentionæœºåˆ¶å¦‚æ­¤æœ‰æ•ˆï¼Ÿæˆ‘ä»¬æ‹¿ç”µå½±æ¨èç³»ç»Ÿæ¥ä¸¾ä¾‹å­ã€‚å‡è®¾ä½ ç»è¥ä¸€å®¶ç”µå½±ç§Ÿèµå…¬å¸ï¼Œä½ æœ‰ä¸€äº›ç”µå½±å’Œä¸€äº›ç”¨æˆ·ï¼Œä½ æƒ³å‘ä½ çš„ç”¨æˆ·æ¨èä»–ä»¬å¯èƒ½ä¼šå–œæ¬¢çš„ç”µå½±ã€‚ä¸€ç§æ–¹æ³•æ˜¯ä¸ºç”µå½±æ‰‹åŠ¨åˆ›å»ºç‰¹å¾ï¼Œä¾‹å¦‚ç”µå½±ä¸­æœ‰å¤šå°‘æµªæ¼«æƒ…èŠ‚ï¼Œæœ‰å¤šå°‘åŠ¨ä½œåœºé¢ï¼Œç„¶åä¸ºç”¨æˆ·è®¾è®¡ç›¸åº”çš„ç‰¹å¾ï¼šä»–ä»¬æœ‰å¤šå–œæ¬¢æµªæ¼«ç”µå½±ï¼Œæœ‰å¤šå–œæ¬¢åŠ¨ä½œç‰‡ã€‚å¦‚æœè¿™æ ·åšï¼Œä¸¤ä¸ªç‰¹å¾å‘é‡ä¹‹é—´çš„ç‚¹ç§¯å°±ä¼šä¸ºç”µå½±å±æ€§ä¸ç”¨æˆ·å–œå¥½çš„åŒ¹é…ç¨‹åº¦æ‰“åˆ†ã€‚å¦‚æœç”¨æˆ·å’Œç”µå½±çš„æŸä¸ªç‰¹å¾çš„ç¬¦å·ç›¸åŒ¹é…â€“ç”µå½±å¾ˆæµªæ¼«ï¼Œç”¨æˆ·å–œæ¬¢æµªæ¼«ï¼Œæˆ–è€…ç”µå½±ä¸æµªæ¼«ï¼Œç”¨æˆ·è®¨åŒæµªæ¼«â€“é‚£ä¹ˆå¾—åˆ°çš„ç‚¹ç§¯å°±ä¼šå¾—åˆ°è¯¥ç‰¹å¾çš„æ­£å€¼ã€‚å¦‚æœç¬¦å·ä¸åŒ¹é…â€“ç”µå½±æµªæ¼«è€Œç”¨æˆ·è®¨åŒæµªæ¼«ï¼Œåˆ™ç›¸åº”é¡¹ä¸ºè´Ÿå€¼ã€‚ å½“ç„¶è¿™æ ·åšå¹¶ä¸ç°å®ï¼Œå› ä¸ºè¿™æ¶‰åŠåˆ°å¤§é‡çš„æ•°æ®æ ‡æ³¨å·¥ä½œã€‚ä¸€ç§è§£å†³åŠæ³•æ˜¯æˆ‘ä»¬å°†ç”µå½±ç‰¹å¾å’Œç”¨æˆ·ç‰¹å¾ä½œä¸ºæ¨¡å‹çš„å‚æ•°ï¼Œè®©ç”¨æˆ·é€‰æ‹©ä¸€äº›å–œæ¬¢çš„ç”µå½±ï¼Œç„¶åé€šè¿‡ä¼˜åŒ–ç®—æ³•è°ƒæ•´ç”¨æˆ·ç‰¹å¾å’Œç”µå½±ç‰¹å¾ï¼Œä½¿å®ƒä»¬çš„ç‚¹ç§¯èƒ½å¤ŸåŒ¹é…ç”¨æˆ·çš„å·²çŸ¥å–œå¥½ã€‚å³ä½¿æˆ‘ä»¬æ²¡æœ‰å‘Šè¯‰æ¨¡å‹è¿™äº›ç‰¹å¾çš„å…·ä½“å«ä¹‰ï¼Œä½†åœ¨è®­ç»ƒåï¼Œè¿™äº›ç‰¹å¾å¾€å¾€ä¼šè‡ªå‘åœ°æ˜ å°„åˆ°æœ‰æ„ä¹‰çš„ç”µå½±å†…å®¹ä¿¡æ¯ã€‚ ä¾‹å¦‚ï¼ŒæŸä¸ªç»´åº¦å¯èƒ½å¯¹åº”â€œæµªæ¼«â€ï¼Œå¦ä¸€ä¸ªç»´åº¦å¯èƒ½å¯¹åº”â€œåŠ¨ä½œâ€æˆ–â€œå–œå‰§â€ï¼Œå³ä½¿æˆ‘ä»¬äº‹å…ˆå¹¶æœªæ˜ç¡®è¿™äº›ç»´åº¦çš„å«ä¹‰ã€‚\nè¿™å°±æ˜¯self-attentionæœºåˆ¶ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ç§è®¡ç®—åºåˆ—å†…éƒ¨å…³ç³»çš„æ–¹æ³•ã€‚åœ¨å¤„ç†æ–‡æœ¬æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆä¸ºæ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ªå¯å­¦ä¹ çš„åµŒå…¥å‘é‡ï¼ˆembedding vectorï¼‰ï¼Œä¾‹å¦‚å¥å­ â€œthe cat walks on the streetâ€ ä¼šè¢«æ˜ å°„ä¸ºå‘é‡åºåˆ— $ v_{\\text{the}}, v_{\\text{cat}}, v_{\\text{walks}}, v_{\\text{on}}, v_{\\text{the}}, v_{\\text{street}} $ã€‚ç„¶åï¼Œè¿™äº›å‘é‡è¿›å…¥self-attention layerï¼Œè¾“å‡ºæ–°çš„å‘é‡åºåˆ— $ y_{\\text{the}}, y_{\\text{cat}}, y_{\\text{walks}}, y_{\\text{on}}, y_{\\text{the}}, y_{\\text{street}} $ã€‚å…¶ä¸­ï¼Œæ¯ä¸ª $ y_t $ éƒ½æ˜¯è¾“å…¥å‘é‡çš„åŠ æƒå’Œï¼Œè€Œæƒé‡æ˜¯æ ¹æ®å½’ä¸€åŒ– (softmax) çš„ç‚¹ç§¯ï¼ˆdot productï¼‰ è®¡ç®—å¾—åˆ°çš„ã€‚ç‚¹ç§¯è¡¡é‡äº†å•è¯ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¦‚æœä¸¤ä¸ªå•è¯åœ¨ä¸Šä¸‹æ–‡ä¸­ç´§å¯†ç›¸å…³ï¼Œå®ƒä»¬çš„ç‚¹ç§¯å°±ä¼šè¾ƒé«˜ï¼Œä»è€Œè·å¾—æ›´é«˜çš„æ³¨æ„åŠ›æƒé‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¥å­ä¸­ï¼Œâ€œwalksâ€ è¿™ä¸ªåŠ¨è¯éœ€è¦ä¸â€œcatâ€å»ºç«‹è”ç³»ï¼Œæ‰€ä»¥ $ v_{\\text{walks}} $ å’Œ $ v_{\\text{cat}} $ çš„ç‚¹ç§¯ä¼šè¾ƒé«˜ï¼Œè€Œâ€œtheâ€ è¿™æ ·çš„å† è¯ç”±äºå¯¹æ•´ä½“è¯­ä¹‰å½±å“è¾ƒå°ï¼Œé€šå¸¸ä¸ä¼šä¸å…¶ä»–è¯æœ‰å¾ˆå¤§çš„ç‚¹ç§¯å€¼ã€‚\næœ‰è¶£çš„æ˜¯ï¼ŒåŸºç¡€çš„self-attentionå¹¶ä¸å…³å¿ƒè¾“å…¥çš„é¡ºåºï¼Œå®ƒåªå…³æ³¨å•è¯ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ã€‚å¦‚æœæˆ‘ä»¬å¯¹è¾“å…¥åºåˆ—è¿›è¡Œé‡æ–°æ’åˆ—ï¼Œè¾“å‡ºçš„å‘é‡åºåˆ—ä¹Ÿä¼šä»¥ç›¸åŒçš„æ–¹å¼è¢«è°ƒæ•´ï¼Œè€Œä¸ä¼šæ”¹å˜å†…å®¹æœ¬èº«ã€‚è¿™æ„å‘³ç€è‡ªæ³¨æ„åŠ›æœ¬è´¨ä¸Šæ˜¯ä¸€ç§é›†åˆæ“ä½œï¼ˆSet Operationï¼‰ï¼Œå®ƒä¸ä¼šä¾èµ–è¯çš„å‰åé¡ºåºï¼Œè€Œæ˜¯è®©æ¨¡å‹è‡ªå·±å­¦ä¹ å“ªäº›å•è¯ä¹‹é—´åº”è¯¥æœ‰æ›´å¼ºçš„è”ç³»ã€‚æ­¤å¤–ï¼ŒåŸºç¡€çš„è‡ªæ³¨æ„åŠ›å±‚æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼Œå®ƒçš„è¡Œä¸ºå®Œå…¨å–å†³äºè¾“å…¥å‘é‡ï¼Œè€Œè¿™äº›å‘é‡æ¥è‡ªäºåµŒå…¥å±‚ï¼Œè€ŒåµŒå…¥å±‚æ˜¯å¯å­¦ä¹ çš„ã€‚åœ¨å®Œæ•´çš„ Transformer ç»“æ„ä¸­ï¼Œæˆ‘ä»¬ä¼šé€šè¿‡ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ è®©æ¨¡å‹æ„ŸçŸ¥å•è¯çš„é¡ºåºä¿¡æ¯ï¼Œä½†åœ¨çº¯è‡ªæ³¨æ„åŠ›å±‚ä¸­ï¼Œå®ƒæ˜¯å®Œå…¨é¡ºåºä¸æ•æ„Ÿçš„ã€‚\nQuery, Key, Value å½“ä»£transformerä¸­ä½¿ç”¨çš„self-attentionè¿˜ä¾èµ–ä¸‰ä¸ªé¢å¤–çš„æŠ€å·§ - Queries, Values, Keysã€‚åœ¨ è‡ªæ³¨æ„åŠ› è®¡ç®—ä¸­ï¼Œæ¯ä¸ªè¾“å…¥å‘é‡ $ x_i $ éœ€è¦åœ¨ä¸‰ç§ä¸åŒçš„è§’è‰²ä¹‹é—´åˆ‡æ¢ï¼š\næŸ¥è¯¢ï¼ˆQueryï¼Œ$ q_i $ï¼‰ï¼šæ¯ä¸ªè¯å‘é‡éƒ½éœ€è¦ä¸å…¶ä»–è¯å‘é‡è¿›è¡Œæ¯”è¾ƒï¼Œä»¥ç¡®å®šå®ƒå¯¹è‡ªå·±çš„è¾“å‡º $ y_i $ çš„å½±å“æƒé‡ã€‚ é”®ï¼ˆKeyï¼Œ$ k_i $ï¼‰ï¼šæ¯ä¸ªè¯å‘é‡ä¹Ÿè¢«å…¶ä»–å•è¯ç”¨æ¥è®¡ç®—æƒé‡ï¼Œå³å®ƒå¯¹å…¶ä»–å•è¯è¾“å‡º $ y_j $ çš„å½±å“ã€‚ å€¼ï¼ˆValueï¼Œ$ v_i $ï¼‰ï¼šåœ¨è®¡ç®—å®Œæƒé‡åï¼Œæ¯ä¸ªå•è¯çš„æœ€ç»ˆè¡¨ç¤ºæ˜¯æ‰€æœ‰è¯å‘é‡çš„åŠ æƒå’Œï¼Œå…¶ä¸­åŠ æƒæ–¹å¼ç”±æŸ¥è¯¢å’Œé”®è®¡ç®—å¾—åˆ°çš„æ³¨æ„åŠ›æƒé‡å†³å®šã€‚ ä¸ºäº†æ›´æ–¹ä¾¿è®¡ç®—ï¼Œæˆ‘ä»¬ä¸ä¼šç›´æ¥ä½¿ç”¨åŸå§‹è¾“å…¥å‘é‡ $ x_i $ï¼Œè€Œæ˜¯å¯¹å…¶è¿›è¡Œä¸‰æ¬¡ä¸åŒçš„ çº¿æ€§å˜æ¢ï¼Œå¾—åˆ°ï¼š $$ q_i = W_q x_i, \\quad k_i = W_k x_i, \\quad v_i = W_v x_i $$ å…¶ä¸­ $ W_q, W_k, W_v $ æ˜¯å¯è®­ç»ƒçš„ æƒé‡çŸ©é˜µï¼Œç”¨äºå­¦ä¹ é€‚åˆä»»åŠ¡çš„æ³¨æ„åŠ›è¡¨ç¤ºã€‚\næ¥ä¸‹æ¥ï¼Œè®¡ç®—ä¸¤ä¸ªè¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ˆç›¸å…³æ€§ï¼‰: $$ wâ€™ = q_i^T k_j $$ ç„¶åè¿›è¡Œ softmax å½’ä¸€åŒ– ä»¥è·å¾—æœ€ç»ˆçš„æ³¨æ„åŠ›æƒé‡ï¼š $$ w_{ij} = \\text{softmax}(wâ€™) $$ æœ€åï¼Œæ¯ä¸ªå•è¯çš„è¾“å‡ºå‘é‡$ y_i $æ˜¯æ‰€æœ‰å€¼å‘é‡çš„åŠ æƒå’Œï¼š $$ y_i = \\sum_j w_{ij} v_j $$ è¿™ä¸ªè¿‡ç¨‹è®©æ¨¡å‹èƒ½å¤ŸåŠ¨æ€è°ƒæ•´å•è¯ä¹‹é—´çš„ç›¸äº’å½±å“ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£åºåˆ—ä¸­çš„è¯­ä¹‰å…³ç³»ã€‚\nç‚¹ç§¯ç¼©æ”¾ åˆšæ‰æˆ‘ä»¬ä½¿ç”¨ dot product è®¡ç®—æŸ¥è¯¢ï¼ˆQueryï¼‰å’Œé”®ï¼ˆKeyï¼‰ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶å°†å…¶è¾“å…¥ Softmax è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚ç„¶è€Œï¼Œå½“åµŒå…¥ç»´åº¦ $ k $ è¾ƒå¤§æ—¶ï¼Œç‚¹ç§¯çš„å€¼ä¼šå˜å¾—å¾ˆå¤§ï¼Œå¯¼è‡´ Softmax çš„æ¢¯åº¦å˜å¾—æå°ï¼Œç”šè‡³é€ æˆæ¢¯åº¦æ¶ˆå¤±ï¼Œå½±å“æ¨¡å‹è®­ç»ƒã€‚\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹ç‚¹ç§¯è¿›è¡Œç¼©æ”¾ ï¼ˆscaling dot productï¼‰ï¼Œå³é™¤ä»¥ $ \\sqrt{k} $: $$ wâ€™_{ij} = \\frac{q_i^T k_j}{\\sqrt{k}} $$ ä»è€Œé˜²æ­¢è¾“å…¥å€¼è¿‡å¤§ï¼Œä½¿ softmax å‡½æ•°è¾“å‡ºçš„æ¢¯åº¦ä¿æŒç¨³å®šï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚è¿™æ ·ç¼©æ”¾çš„åŸå› åœ¨äº é«˜ç»´å‘é‡çš„æ¬§å‡ é‡Œå¾—é•¿åº¦éšç€ç»´åº¦ $ k $ å¢å¤§è€Œå¢å¤§ï¼Œé€šè¿‡é™¤ä»¥ $\\sqrt{k}$ï¼Œå¯ä»¥æ¶ˆé™¤è¿™ç§æ”¾å¤§æ•ˆåº”ï¼Œä½¿ä¸åŒç»´åº¦çš„è®¡ç®—ç»“æœä¿æŒåœ¨åˆç†èŒƒå›´å†…ï¼Œä»è€Œæ›´ç¨³å®šåœ°å­¦ä¹ æ³¨æ„åŠ›æƒé‡ã€‚\nMulti-head Attention åœ¨Self-Attentionæœºåˆ¶ä¸­ï¼Œæ¯ä¸ªå•è¯éƒ½å¯ä»¥ä¸å¥å­ä¸­çš„å…¶ä»–å•è¯å»ºç«‹å…³ç³»ã€‚ç„¶è€Œï¼Œåœ¨æ ‡å‡†çš„å•å¤´æ³¨æ„åŠ›ï¼ˆSingle-Head Attentionï¼‰æœºåˆ¶ä¸‹ï¼Œä¸€ä¸ªå•è¯åªèƒ½ç”¨ä¸€ä¸ªæ³¨æ„åŠ›åˆ†å¸ƒæ¥è¡¨ç¤ºå®ƒä¸æ‰€æœ‰å…¶ä»–å•è¯çš„å…³ç³»ã€‚è¿™å°±å¯¼è‡´äº†ä¸€ä¸ªé—®é¢˜ï¼šå•ä¸ªè¯å¯èƒ½åœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­æ‰¿æ‹…ä¸åŒçš„è¯­ä¹‰è§’è‰²ï¼Œä½†å•å¤´æ³¨æ„åŠ›æ— æ³•åŒºåˆ†è¿™äº›è§’è‰²ã€‚ä¾‹å¦‚Mary gave roses to susan. åœ¨ single-head attention æœºåˆ¶ä¸­ï¼Œæ‰€æœ‰è¿™äº›ä¿¡æ¯éƒ½ä¼šè¢«ç®€å•åœ°åŠ æƒæ±‚å’Œï¼Œè€Œä¸ä¼šåˆ†åˆ«å¤„ç†ä¸åŒçš„è¯­ä¹‰å…³ç³»ã€‚è¿™æ„å‘³ç€ï¼Œâ€œMaryâ€ å’Œ â€œSusanâ€ éƒ½ä¼šå½±å“ â€œgaveâ€ çš„è¡¨ç¤ºï¼Œä½†å®ƒä»¬çš„å½±å“æ–¹å¼æ˜¯ç›¸åŒçš„ï¼Œæ— æ³•åŒºåˆ†\"æ–½äº‹è€…\" å’Œ â€œå—ç›Šè€…â€ çš„ä¸åŒè¯­ä¹‰ã€‚\nå¤šå¤´æ³¨æ„åŠ›å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰ é€šè¿‡ä½¿ç”¨å¤šä¸ªç‹¬ç«‹çš„æ³¨æ„åŠ›å¤´ï¼ˆattention headsï¼‰ æ¥æä¾›æ›´é«˜çš„è¡¨è¾¾èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´,æ¯ä¸ªæ³¨æ„åŠ›å¤´ï¼ˆheadï¼‰å…·æœ‰è‡ªå·±ç‹¬ç«‹çš„æŸ¥è¯¢ã€é”®å’Œå€¼å˜æ¢çŸ©é˜µï¼š$W_q^r, W_k^r, W_v^r$ï¼Œå…¶ä¸­$r$è¡¨ç¤ºä¸åŒçš„æ³¨æ„åŠ›å¤´ã€‚å¯¹äºè¾“å…¥$x_i$ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªè¾“å‡ºå‘é‡$y_i^r$ï¼ŒæŠŠè¿™äº›å‘é‡æ‹¼æ¥èµ·æ¥ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢æŠŠç»´åº¦é™åˆ°kã€‚\nä»€ä¹ˆæ˜¯Transformer? Any architecture designed to process a connected set of unitsâ€”such as the tokens in a sequence or the pixels in an imageâ€”where the only interaction between units is through self-attention.\nä¸‹é¢æ˜¯ä¸€ä¸ªæ¯”è¾ƒåŸºç¡€çš„æ¶æ„ BERT BERT - Bidirectional Embedding Representation Transformer\nBERTä¸GPTæ¶æ„çš„åŒºåˆ«\nParsing Decoding with LLMs One naturally wonders if the problem of translation could conceivably be treated as a problem in cryptography. When I look at an article in Russian, I say: â€™This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.\nTerminology: continuation - è¡¥å…¨\næˆ‘ä»¬å¦‚ä½•ç”Ÿæˆthe most probable continuation? $$ \\arg\\max_{w_1, â€¦, w_n} p(w_1, â€¦, w_n | \\text{prefix}) $$ å¦‚ä½•æ±‚è§£ï¼Ÿ\néå†æ‰€æœ‰å¯èƒ½çš„ç»­å†™ï¼Œè®¡ç®—å®ƒä»¬çš„æ¦‚ç‡ï¼Œç„¶åé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªã€‚ ä½†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œå¯èƒ½çš„åºåˆ—æ•°é‡æ˜¯æŒ‡æ•°çº§å¢é•¿çš„ï¼Œæ‰€ä»¥è¿™ç§æ–¹æ³•é€šå¸¸ä¸å¯è¡Œã€‚ å¦ä¸€ç§ç­–ç•¥æ˜¯é€æ­¥ç”Ÿæˆï¼Œå³æ¯ä¸€æ­¥é€‰æ‹©å½“å‰æœ€ä¼˜çš„ä¸‹ä¸€ä¸ªå•è¯ï¼ˆæˆ– top-k é€‰é¡¹ï¼‰ã€‚è¿™ç§æ–¹æ³•æ›´å®ç”¨ï¼Œæ¯”å¦‚è´ªå¿ƒè§£ç ï¼ˆgreedy decodingï¼‰ Greedy Decoding ä¸‹é¢æ˜¯greedy decodingçš„å…·ä½“æµç¨‹ è¿™ç§è§£ç æ–¹å¼çš„é—®é¢˜ï¼š\næˆ‘ä»¬åªåšä¸€æ¬¡continuationï¼Œä¸€æ—¦å‘ç”Ÿé”™è¯¯æ— æ³•å›æº¯æ›´æ”¹ã€‚ Beam Searching Decoding æ ¹æ®greedy decodingçš„æ•™è®­ï¼Œæˆ‘ä»¬é€‰æ‹©ä¿ç•™å‡ ä¸ªcontinuationï¼Œå¹¶ä¸”æ¯æ¬¡éƒ½æ‰©å±•most promisingçš„é‚£ä¸ªã€‚æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤º Beam Searchingå¦‚ä½•å®ç°ï¼Ÿ\nç”¨ä¼˜å…ˆé˜Ÿåˆ—è¿™æ ·çš„æ•°æ®ç»“æ„æ¥å­˜å‚¨æ¯ä¸ªcontinuation è¿™ä¸ªé˜Ÿåˆ—ä¸­ä¿æŒæœ€å¤škä¸ªè§£æ å½“è¦è§£ç ä¸€ä¸ªå•è¯æ—¶ï¼Œä»é˜Ÿåˆ—ä¸­popå‡ºä¸€ä¸ªè§£æï¼Œæ‰©å±•å®Œå†pushå›é˜Ÿåˆ— ç”¨priority queueè¿™ç§ç»“æ„æ¥åšï¼Œæ—¶é—´å¤æ‚åº¦ä¸ä¼šè¶…è¿‡O(logk)ï¼Œç”šè‡³èƒ½è¾¾åˆ°O(1) é˜Ÿåˆ—è§„æ¨¡kçš„å½±å“\nå½“k=1ï¼Œå°±æ˜¯greedy decodingã€‚æ‰€ä»¥å½“kè¾ƒå°æ—¶ï¼Œæˆ‘ä»¬ä¼šå‡ºç°ç±»ä¼¼çš„é—®é¢˜ å½“kå¾ˆå¤§æ—¶ï¼Œä¸€æ–¹é¢æ˜¯è®¡ç®—æ˜‚è´µï¼Œä¸€æ–¹é¢æ˜¯æœ‰å·¥ä½œè¡¨æ˜è¾ƒå¤§çš„kå¾€å¾€å¯¼è‡´worse translation 2, 3ã€‚ è€Œä¸”å½“kå¾ˆå¤§æ—¶ï¼Œå¾€å¾€ç”Ÿæˆçš„continationéƒ½å¾ˆçŸ­ï¼Œå› ä¸ºè¾ƒçŸ­çš„continuationä¸€èˆ¬éƒ½æ˜¯ä¼˜é€‰ å› æ­¤ï¼Œé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„kéå¸¸é‡è¦ è§£å†³æ–¹æ³•ï¼š\nSampling ä¸å…¶æ¯æ¬¡é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ï¼Œä¸å¦‚ä»è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ã€‚ä¾‹å¦‚Top-k Samplingå’ŒNucleus sampling\nHoltzman et al. å¯¹æ¯”äº†å‡ ç§æ–¹æ³•çš„æ•ˆæœï¼Œå¦‚å›¾æ‰€ç¤º4 é™¤æ­¤ä»¥å¤–ï¼Œè¿˜æœ‰Locally Typical Sampling Top-k Sampling vs. Nucleus Sampling vs. Locally Typical Sampling è¿™äº›éƒ½æ˜¯æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ç”¨äºè§£ç ï¼ˆdecodingï¼‰çš„ç­–ç•¥ï¼Œæ—¨åœ¨å¹³è¡¡å¤šæ ·æ€§å’Œåˆç†æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTã€LLMï¼‰ä¸­åº”ç”¨ã€‚\nå®ƒä»¬çš„æ ¸å¿ƒç›®æ ‡æ˜¯é¿å…å•ä¸€ç¡®å®šæ€§è¾“å‡ºï¼ˆå¦‚ Greedy Searchï¼‰ï¼Œè®©ç”Ÿæˆæ–‡æœ¬æ›´åŠ è‡ªç„¶å’Œå¯Œæœ‰å˜åŒ–ã€‚\næ ¸å¿ƒæ€æƒ³ï¼š\né€‰æ‹©é‚£äº›ä½¿å¾—æ¨¡å‹ç”Ÿæˆçš„ä¸‹ä¸€ä¸ªå•è¯åœ¨ä¸Šä¸‹æ–‡ä¸­æœ€ç¬¦åˆâ€œå±€éƒ¨å…¸å‹æ€§â€çš„å•è¯ã€‚ ä¸åªæ˜¯çœ‹å•è¯æ¦‚ç‡æœ¬èº«ï¼Œè€Œæ˜¯çœ‹å®ƒæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­â€œå¬èµ·æ¥â€åˆç†ï¼ˆå±€éƒ¨ç†µæœ€å°ï¼‰ã€‚ æµç¨‹ï¼š\nè®¡ç®—æ‰€æœ‰å•è¯çš„æ¦‚ç‡ $ p(w \\mid \\text{context}) $ã€‚ è®¡ç®—è¿™äº›å•è¯çš„ ä¿¡æ¯ç†µï¼ˆself-informationï¼‰ï¼š $$ s(w) = -\\log p(w \\mid \\text{context}) $$ é€‰æ‹©ä¿¡æ¯ç†µæœ€æ¥è¿‘å¹³å‡ä¿¡æ¯ç†µï¼ˆå…¸å‹ç†µï¼‰ çš„ä¸€éƒ¨åˆ†å•è¯ã€‚ åœ¨è¿™ä¸ªå­é›†ä¸­è¿›è¡Œé‡‡æ ·ã€‚ ä¼˜ç‚¹ï¼š\né¿å…äº† Top-k å’Œ Nucleus Sampling çš„å±€é™ï¼Œä½¿å¾—è¾“å‡ºæ›´åŠ ç¬¦åˆä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯ä»…ä»…åŸºäºå…¨å±€æ¦‚ç‡é«˜ä½ã€‚ é€‚ç”¨äºå¯¹å±€éƒ¨è¯­å¢ƒè¦æ±‚é«˜çš„ä»»åŠ¡ï¼Œæ¯”å¦‚å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç­‰ã€‚ ç¼ºç‚¹ï¼š\nè®¡ç®—å¤æ‚åº¦ç•¥é«˜ï¼Œéœ€è¦é¢å¤–è®¡ç®—ä¿¡æ¯ç†µã€‚ é‡‡æ ·çš„ç›®çš„å°±æ˜¯å¹³è¡¡å¤šæ ·æ€§å’Œåˆç†æ€§ã€‚é¿å…è¯­è¨€æ¨¡å‹å•ä¸€ç¡®å®šæ€§çš„è¾“å‡ºã€‚\nNeural Parsing å…ˆå¤ä¹ ä¸‹Encoder-Decoderæ¶æ„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒEncoderä½œç”¨æ˜¯å°†è¾“å…¥åºåˆ—ï¼ˆå¦‚å¾·è¯­å¥å­ï¼‰ç¼–ç æˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡å‘é‡ï¼ˆcontext vectorï¼‰ã€‚\nè¾“å…¥è¯ï¼ˆå¦‚ â€œnatÃ¼rlichâ€, â€œhatâ€, â€œjohnâ€, â€œspaÃŸâ€ï¼‰é¦–å…ˆè¢«æ˜ å°„æˆè¯å‘é‡ï¼ˆ$ x_1, x_2, x_3, x_4 $ï¼‰ã€‚ è¿™äº›è¯å‘é‡ä¾æ¬¡è¾“å…¥åˆ°ä¸€ä¸ªRNN ç¼–ç å™¨ï¼ˆé€šå¸¸æ˜¯ LSTM æˆ– GRUï¼‰ï¼Œäº§ç”Ÿéšè—çŠ¶æ€ $ h_1, h_2, h_3, h_4 $ã€‚ è¿™äº›éšè—çŠ¶æ€æ•æ‰äº†å¥å­çš„ä¿¡æ¯ï¼Œæœ€ç»ˆæœ€åä¸€ä¸ªéšè—çŠ¶æ€ï¼ˆè¿™é‡Œæ˜¯ $ h_4 $ï¼‰è¢«ç”¨ä½œæ•´ä¸ªè¾“å…¥å¥å­çš„ä¸Šä¸‹æ–‡å‘é‡ï¼ˆcontext vectorï¼‰ã€‚ Decoderæ¥æ”¶ç¼–ç å™¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç„¶åé€æ­¥ç”Ÿæˆç›®æ ‡è¯­è¨€çš„å¥å­ï¼ˆå¦‚è‹±æ–‡ï¼‰ã€‚\nç¼–ç å™¨æœ€åçš„éšè—çŠ¶æ€è¢«ä¼ å…¥è§£ç å™¨çš„ RNN ä½œä¸ºåˆå§‹çŠ¶æ€ã€‚ è§£ç å™¨ä½¿ç”¨ RNNï¼ˆLSTM/GRUï¼‰é€æ­¥ç”Ÿæˆç›®æ ‡è¯­è¨€çš„å•è¯ï¼š åˆå§‹çŠ¶æ€ $ s_1 $ ä¾èµ–äºç¼–ç å™¨çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚ æ¯ä¸ªæ—¶é—´æ­¥ï¼Œè§£ç å™¨çš„ RNN ç”Ÿæˆä¸€ä¸ªæ–°çš„éšè—çŠ¶æ€ $ s_t $ã€‚ é€šè¿‡ä¸€ä¸ª softmax å±‚ï¼Œ$ s_t $ è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºè¯ $ y_t $ï¼ˆå¦‚ â€œofâ€, â€œcourseâ€, â€œjohnâ€, â€œhasâ€, â€œfunâ€ï¼‰ã€‚ è§£ç å™¨çš„è¾“å‡ºä½œä¸ºè¾“å…¥ä¼ é€’åˆ°ä¸‹ä¸€æ­¥ï¼Œç›´åˆ°ç”Ÿæˆå®Œæ•´å¥å­ã€‚ Parsing is the task of turning a sequence of words\né—®é¢˜æ¥äº†ï¼Œå¥å­è§£æçš„è¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œä½†è¾“å‡ºä¸æ˜¯ï¼Œé‚£æˆ‘ä»¬å¦‚ä½•ç”¨encoder-decoderæ¨¡å‹æ¥è¿›è¡Œparsingï¼Ÿ\næˆ‘ä»¬å¯ä»¥linearize the syntax tree. è¿™æ ·æˆ‘ä»¬å°±æœ‰ä¸€ä¸ªåºåˆ—æ¥è¡¨ç¤ºè¾“å‡ºäº†ã€‚ä¾‹å¦‚ï¼šI saw a man with a telescopeçš„å¥å­è§£æå°±æ˜¯ (S (NP (Pro You ) ) (VP (V saw ) (NP (Det a ) (N man ) (PP (P with ) (Det a ) (N telescope ) ) ) ) )\næ­¤å¤–ï¼Œè¿˜éœ€è¦ä¸€äº›ä¼˜åŒ–\næ·»åŠ EOSã€‚ä½¿è§£ç å™¨çŸ¥é“ä½•æ—¶åœæ­¢ç”Ÿæˆï¼Œè€Œä¸ä¼šä¸€ç›´æ— é™åˆ¶åœ°é¢„æµ‹å•è¯ã€‚ åè½¬è¾“å…¥å­—ç¬¦ä¸²å¯ä»¥å¸¦æ¥å°å¹…çš„æ€§èƒ½æå‡ã€‚ åŠ æ·±ç½‘ç»œå±‚æ•°ã€‚ä¾‹å¦‚å¯¹encoderå’Œdecoderéƒ½ä½¿ç”¨ä¸‰å±‚LSTM5ã€‚ æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶ ä½¿ç”¨word2vecä½œä¸ºè¾“å…¥ï¼ˆé¢„è®­ç»ƒçš„è¯åµŒå…¥ï¼‰ è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œè€Œä¸éœ€è¦äººå·¥æ ‡æ³¨ã€‚Vinyals et al. (2015) 5 é‡‡ç”¨ Berkeley Parserï¼ˆä¸€ä¸ªå·²æœ‰çš„å¥æ³•è§£æå™¨ï¼‰æ¥è§£æå¤§é‡æ–‡æœ¬ï¼Œå¹¶ç”¨è¿™äº›è§£æç»“æœä½œä¸ºè®­ç»ƒæ•°æ®ã€‚ å¯èƒ½å‘ç”Ÿçš„é—®é¢˜ï¼ˆæ¯”ä¾‹å¾ˆå°ï¼Œæ— éœ€æ‹…å¿ƒï¼‰ï¼šæ¯”å¦‚æˆ‘ä»¬å¦‚ä½•ç¡®å®šopening and closing brackets match? å¦‚ä½•å¯¹åº”è¾“å…¥å’Œè¾“å‡ºï¼Ÿå¦‚ä½•ç¡®ä¿æ¨¡å‹è¾“å‡ºçš„æ˜¯æ•´ä¸ªåºåˆ—çš„æœ€ä¼˜parsingè€Œä¸æ˜¯ä»…ä»…é¢„æµ‹æ¯ä¸ªtime stepä¸Šçš„best symbol?\nParsing with Transformers Kitaevç­‰äººç”¨transformersè¿›è¡Œparsingã€‚\nUse a transformer to encode the input This results in a â€œcontext aware summary vectorâ€ (embedding) for each input word. The embedding encodes word, PoS tag, and position information. The embedding layers are combined to obtain span scores But they also try factored attention heads, which separate position and content information. ä»€ä¹ˆæ˜¯span scores? åœ¨NLPä¸­ï¼Œå°¤å…¶æ˜¯å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€å¥æ³•è§£æï¼ˆParsingï¼‰ å’Œ ä¿¡æ¯æŠ½å–ï¼ˆIEï¼‰ ç­‰ä»»åŠ¡ä¸­ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚ Span scores é€šå¸¸åŸºäº Precisionï¼ˆç²¾ç¡®ç‡ï¼‰ã€Recallï¼ˆå¬å›ç‡ï¼‰ å’Œ F1-scoreï¼ˆF1 åˆ†æ•°ï¼‰ æ¥è¯„ä¼°æ¨¡å‹åœ¨ span-level çš„è¡¨ç°ï¼š\nSpan scoresåœ¨Transformerè§£æä»»åŠ¡ä¸­çš„è®¡ç®—æ–¹æ³•ï¼šè®¡ç®—spanç›¸å…³çš„å‘é‡å·®å€¼ï¼Œç»è¿‡çº¿æ€§å˜æ¢ã€å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰ã€éçº¿æ€§æ¿€æ´»ï¼ˆReLUï¼‰å’Œå†æ¬¡å˜æ¢ï¼Œå¾—åˆ° span scoresã€‚æœ€ç»ˆï¼Œè¿™äº›å¾—åˆ†ç”¨äº è§£ç ï¼ˆdecode the outputï¼‰ï¼Œå³æ‰¾åˆ°æœ€ä¼˜çš„è§£ææ ‘ã€‚\nUnsupervised Parsing Unsupervised Parsing6ä¸»è¦æ˜¯ä»æœªæ ‡è®°çš„æ–‡æœ¬ä¸­å½’çº³å‡ºè¯­æ³•ç»“æ„ã€‚\nUnsupervised Parsingçš„SOTAçš„F-scoreåªæœ‰60ï¼Œå¯¹æ¯”supervised parsingæ¥è¯´å¾ˆéš¾ï¼Œsupervised parsingçš„F-scoreå¯ä»¥è¾¾åˆ°95ä»¥ä¸Š\nLLMå‰æ²¿ç ”ç©¶ Scaling Law ä¾‹å¦‚ï¼Œç»™å®šå›ºå®šçš„è®¡ç®—èµ„æºé¢„ç®—ï¼Œè®­ç»ƒTransformerè¯­è¨€æ¨¡å‹çš„æœ€ä½³æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ•°æ®é›†å¤§å°æ˜¯å¤šå°‘ï¼Ÿ\nè¿™ç±»é—®é¢˜å°±å¯ä»¥ç”¨Scaling lawsæ¥å›ç­”ã€‚å®ƒæä¾›äº†Compute Budget(C)ï¼Œsize of model(N)ä»¥åŠnumber of training tokens(D)ä¹‹é—´çš„ä¸€ç§å…³ç³»ã€‚æœ‰ä¸€ä¸ªç²—ç•¥çš„å…±è¯†æ˜¯ï¼Œå®ƒä»¬å¯ä»¥ä½¿ç”¨power lawsæˆ–ç±»ä¼¼çš„å®šå¾‹ï¼ˆä»¥åŠå®ƒä»¬çš„ç»„åˆï¼‰æ¥å»ºæ¨¡ã€‚\nPower Law Power Lawæè¿°çš„æ˜¯å˜é‡xå’Œå®ƒæŸäº›è¡Œä¸ºä¹‹é—´çš„å…³ç³»ï¼Œå…¬å¼è¡¨ç¤ºä¸º$f(x) = \\alpha x^\\mathcal{-K}$\nå®ƒæœ‰ä¸€ä¸ªé‡è¦çš„æ€§è´¨ï¼šå½“$x$ä¹˜ä»¥ä¸€ä¸ªå› å­æ—¶ï¼Œ$f(x)$ä¹ŸåŒæ ·ä½œä¸º$Îº$çš„å‡½æ•°ä¹˜ä»¥ä¸€ä¸ªå› å­ã€‚å³ï¼š $$ f(cx) = \\alpha (cx)^\\mathcal{-K} = c^\\mathcal{-K}f(x) $$\nKaplan et al. (2020) Kaplanç­‰äººçš„ç ”ç©¶7æ€»ç»“\næ¨¡å‹çš„æ€§èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºscaleï¼ˆæ¨¡å‹å‚æ•°çš„æ•°é‡ï¼Œæ•°æ®é›†å¤§å°ï¼Œè®¡ç®—é¢„ç®—ï¼‰è€Œä¸æ˜¯ç½‘ç»œæ‹“æ‰‘ï¼ˆå±‚æ•°ï¼Œæ¯å±‚çš„å®½åº¦ï¼‰ã€‚ æ•°æ®é›†å¤§å°å’Œæ¨¡å‹å¤§å°çš„ä¸€èµ·å¢é•¿æ˜¯å¾ˆé‡è¦çš„ã€‚ æ€§èƒ½å¯ä»¥ç”¨power lawæ¥å»ºæ¨¡ã€‚ è¿˜æœ‰ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨floating-point operations per second (FLOPs) æ¥è¡¡é‡computeï¼Œè€Œä¸æ˜¯å•çº¯ä¾èµ–æ—¶é—´æ¥è¡¡é‡ã€‚\nå¯¹ä»–ä»¬çš„å·¥ä½œï¼ˆKaplan Scaling Lawï¼‰çš„ä¸»è¦æ‰¹åˆ¤ç‚¹åœ¨äºï¼Œä»–ä»¬åœ¨è®­ç»ƒä¸­ä½¿ç”¨äº†ç›¸åŒçš„å­¦ä¹ ç‡ã€‚\nHoffmann et al. (2020) ä»¥å‰çš„scaling lawè®¤ä¸ºï¼Œå¢åŠ è®¡ç®—é¢„ç®—ï¼ˆcompute budgetï¼‰æ—¶ï¼Œä¼˜å…ˆå¢åŠ æ¨¡å‹å¤§å°ï¼ˆmodel sizeï¼‰ï¼Œå…¶æ¬¡æ‰å¢åŠ æ•°æ®é‡ï¼ˆdata sizeï¼‰ã€‚ æ¯”å¦‚è®¡ç®—é¢„ç®— Ã—100 æ—¶ï¼Œæ¨¡å‹å‚æ•°é‡ Ã—25ï¼Œæ•°æ®é‡ Ã—4ã€‚å› ä¸ºç›´è§‰ä¸Šæ›´å¤§çš„æ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®ï¼Œæ‰€ä»¥æ•°æ®è§„æ¨¡çš„å¢é•¿ä¸éœ€è¦å¤ªå¿«ã€‚ ä½†æ˜¯ï¼Œè¿‡åº¦å¢åŠ æ¨¡å‹å¤§å°å¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆæˆ–è®¡ç®—èµ„æºæµªè´¹ã€‚\nHoffmannç­‰äºº2022å¹´çš„ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„ç»“è®º8ï¼š\nè®¡ç®—é¢„ç®—å¢åŠ æ—¶ï¼Œæ¨¡å‹å¤§å°å’Œæ•°æ®é‡åº”è¯¥åŒæ¯”ä¾‹å¢é•¿ã€‚ æ¯”å¦‚è®¡ç®—é¢„ç®— Ã—100 æ—¶ï¼Œæ¨¡å‹å¤§å° Ã—10ï¼Œæ•°æ®é‡ Ã—10ã€‚å› ä¸ºä¹‹å‰çš„ç­–ç•¥å¯¼è‡´å¾ˆå¤šå¤§æ¨¡å‹åœ¨å°æ•°æ®ä¸Šè®­ç»ƒï¼Œå‡ºç°æ¬ è®­ç»ƒï¼ˆundertrained modelsï¼‰ çš„é—®é¢˜ã€‚ Hoffmann å‘ç°ï¼Œæ›´å¤§æ¨¡å‹å¹¶ä¸ä¸€å®šèƒ½å®Œå…¨åˆ©ç”¨å›ºå®šæ•°é‡çš„æ•°æ®ï¼Œåˆç†å¢åŠ æ•°æ®é‡å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚è¿™ä¸ªå‘ç°ä¹Ÿå½±å“äº†åç»­ LLM è®­ç»ƒç­–ç•¥ï¼Œä¾‹å¦‚ GPT-4ã€Geminiã€PaLM ç­‰æ›´å…³æ³¨ æ•°æ®ä¸æ¨¡å‹è§„æ¨¡çš„å‡è¡¡å¢é•¿ã€‚\nå¦‚ä½•æ¨å¯¼Hoffmannç­‰äººæå‡ºçš„Chinchillaâ€™s Scaling Law?\nL: LM average test loss (entropy loss) D: dataset size, number of tokens N: number of parameters C: compute budget, C=C(N, D) å·²çŸ¥ä¸€ä¸ªå›ºå®šçš„compute budget C*, æ‰¾åˆ° $$ \\arg\\min_{N, D}L(N, D) = C^* $$ ç”¨power lawæ¥å»ºæ¨¡ï¼š $$ L(N, D) = \\frac{a}{N^\\alpha} + \\frac{b}{D^\\beta} + c $$ $c$æ˜¯ç†æƒ³çš„test loss å› æ­¤ï¼ŒHoffmannç­‰äººè®­ç»ƒå‡ºæ¥äº†ä¸¤ä¸ªæ¨¡å‹ï¼š\nGopher: 280 billion parameters, 300 billion tokens, L(N, D) = 1.993 Chinchilla: 70 billion parameters, 1.4 trillion tokens, L(N, D) = 1.936 Chinchilla éµå¾ªè¿™ä¸€ç­–ç•¥ï¼ˆè¾ƒå°æ¨¡å‹ + æ›´å¤šæ•°æ®ï¼‰ï¼Œç»“æœè¯æ˜å®ƒåœ¨test losså’Œä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚\nå¢åŠ æ•ˆç‡ è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œä¸»è¦åšçš„è®¡ç®—å°±æ˜¯çŸ©é˜µä¹˜æ³•å’Œæ±‚å’Œå½’ä¸€åŒ–ï¼ˆSoftmaxï¼‰ã€‚ç”¨ä¸€ä¸ªå…¬å¼æ¦‚æ‹¬Transformeråšçš„äº‹æƒ…å°±æ˜¯$\\text{Softmax}(QK^T)V$\nGPUæ˜¯å¹¶è¡Œå¤„ç†å™¨ï¼Œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º åŸºç¡€çš„è®¡ç®—å•å…ƒæ˜¯çº¿ç¨‹ çº¿ç¨‹è¢«åˆ†ç»„æˆBlockï¼ŒBlockä¸­çš„æ‰€æœ‰çº¿ç¨‹éƒ½æœ‰å”¯ä¸€çš„IDï¼Œä½†è¿è¡Œç›¸åŒçš„ä»£ç  è¿™ä½¿å¾—GPUå¯ä»¥åšæç«¯çš„å¹¶è¡ŒåŒ– Gridæ˜¯ä¸€ç»„Blockã€‚æ¯ä¸ªBlockå¯ä»¥è¿è¡Œä¸åŒçš„ä»£ç æ®µ GPUæœ‰å¾ˆå¤§çš„global memoryï¼Œå¾ˆå¤§ï¼Œä½†æ˜¯å¾ˆæ…¢ æ¯ä¸ªBlockéƒ½æœ‰ä¸€ä¸ªå…±äº«å†…å­˜-Blockä¸­çš„æ‰€æœ‰çº¿ç¨‹ä¹‹é—´å…±äº«è¿™ä¸ªå†…å­˜ï¼Œå®ƒæ•ˆç‡å¾ˆé«˜ï¼Œä½†å¾ˆå° å†™ä»£ç æ—¶è¦å°½å¯èƒ½å°‘ç”¨å…¨å±€å†…å­˜ï¼Œå¤šç”¨Blockçš„å…±äº«å†…å­˜ Double Descent Double Descentæ˜¯æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°çš„ä¸€ç§ç°è±¡ï¼Œå®ƒæè¿°äº†æ¨¡å‹çš„test erroréšç€model sizeæˆ–dataset sizeå˜åŒ–çš„éå•è°ƒè¶‹åŠ¿ã€‚ Double Descent ç°è±¡è¡¨æ˜ï¼Œåœ¨â€œè¿‡æ‹ŸåˆåŒºåŸŸâ€ä¹‹åï¼Œç»§ç»­å¢åŠ æ¨¡å‹è§„æ¨¡ï¼Œtest erroråè€Œä¼šå†æ¬¡ä¸‹é™ï¼Œè¿›å…¥â€œç¬¬äºŒæ¬¡æ³›åŒ–åŒºåŸŸâ€ã€‚\nå› æ­¤ï¼Œä¼ ç»Ÿç»Ÿè®¡å­¦ä¹ ç†è®ºé¢„æµ‹è¿‡æ‹Ÿåˆä¼šå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸‹é™ï¼Œä½†Double Descentè¯´æ˜åœ¨è¶…å¤§æ¨¡å‹ä¸‹ï¼Œæ³›åŒ–èƒ½åŠ›åè€Œä¼šå›å‡ã€‚\nLLMs as Formal Machines LLMå¾®è°ƒ Evaluating Generation and Machine Translation Ethics Evaluation of LLM and Alignment QA and RAG Tutorials Language Models Softmax Function What is the purpose of Softmax function? The softmax converts an arbitrary vector of |v| dimensions into a valid categorical probability distribution over |v| possible outcomes. In particular it ensures that all individual elements (probabilities) are non-negative and sum to one. What is the purpose of the expression in the numerator? The numerator ensures that all values are positive. Note that this is stronger than needed: the axioms of probability simply require all values to be non-negative. But exponentiation is only zero in the (negative) limit. What is the purpose of the expression in the denominator? The denominator normalises the distribution so that all individual probabilities sum to one. Now consider how a neural language model with a softmax output layer compares with a classic n-gram language model. Typically, we use techniques like smoothing or backoff in conjunction with n-gram models. Does this problem arise in the neural model? Why or why not? Noâ€”softmax ensures that the model will always return a non-zero probability for any n-gram. Feedforward Language Models In feedforward language model, the probability $P(w_i\\mid w_{i-n+1}, â€¦, w_{i-1}) = \\text{softmax}(Vh_2 + b_2)$, where $h_2 = \\text{tanh}(Wh_1 + b_1)$, $h_1 = \\text{concatenate}(Cw_{i-n+1}, â€¦, Cw_{i-1})$, $w_i = \\text{onehot}(w_i)$ for all i. Now consider the number of parameters required to represent this model. This number is determined by the size of the vocabulary (given to you by the data), the order $n$, and the dimension of the two hidden layers, $h_1$ and $h_2$, which we will denote $d_1$ and $d_2$, respectively (Note that the first dimension must be divisible by n âˆ’ 1, but you can ignore this detail in your calculations). Dimensions $d_1$ and $d_2$ are modeling choices, though the practical consideration is how they impact the modelâ€™s accuracy. How to express the number of model parameters in terms of $|V|, n, d_1$ and $d_2$? For $V$: $d_2|V|$ For $W$: $d_1d_2$ because $d_1$ is predefined, and will be equal to $(n-1) * \\text{embedding}$ For $C$: $|V|d_1/(n-1)$ because $|\\text{embedding}| = \\frac{d_1}{n-1}$ For $b_1$: $d_2$ For the complete model, add up the aboveL $(1+\\frac{d_1}{n-1} + d_2){V} + d_1d_2 + d_2$ An effective size for the hidden dimension of a neural NLP model is often in the hundreds. For n from 2 to 5, how many parameters would your model have if $\\frac{d_1}{n-1} = d_2 = 100$, what if $\\frac{d_1}{n-1} = d_2 = 1000$ The key here is that $(1 + \\frac{d_1}{n-1} + d_2)|V|$ dominates, and this is determined by the mapping between the one-hot vocabulary vectors and the hidden dimensions. A reasonable guess for |V| in most neural network language models is 20000 and $\\frac{d_1}{n-1} = d_2 = 100$ should give parameters of about 4M parameters. If $\\frac{d_1}{n-1} = d_2 = 1000$ then you have about 40M parameters. However if we try to model an open vocabulary in this model we will struggle to fit this in memory on a GPU whose memory is generally far smaller than a CPU machine. What do you conclude about the relative memory efficiency of classic n-gram and feedforward neural language models? If you increased n even further, what would happen? The number of parameters in the n-gram model is highly sensitive to changes in n, while the number of parameters in the neural model is almost unchanged. Hence, the feedforward model can be easily extended to larger n, which might be advantageous. How would you expect the number of parameters in an RNN model to scale with $n$ and $|V|$ An RNN scales in the same way as the feedforward model: the dominant factor is the vocabulary size. Itâ€™s entirely insensitive to n since it (theoretically) models $n = \\infty$ Think of any strategies to substantially reduce the number of parameters? For RNN models, the key to reducing the number of parameters is to reduce vocabulary size. This can be done with subword modeling. Notice that this is inappropriate for the n-gram model, since it would be conditioning on less information! Note that the feedforward model has a similar limitation, though it is easier to increase the order $n$ of the feedforward model. Model Design Design a feedforward neural network to model $P(y_i\\mid x)$. . Identify any independence assumptions you make. Draw a diagram that illustrates how the model computes probabilities for the tag of the word â€œwithâ€: What is the input, and how is the output distribution computed from the input? Write out the basic equations of the model, and explain the choice. An effective design for the feedforward network is to model $P(y_i \\mid x_{i-k}, â€¦, x_{i+k})$ for some fixed window size 2k+1. For example, use something like: $P(y_i \\mid x_{i-k}, â€¦, x_{i+k}) = \\text{softmax}(Wh+b_2)$, where $h = \\text{tanh}(Vx+b_1)$, and $x$ is onehot encoded like $x = \\text{onehot}(x_{i-k});â€¦;\\text{onehot}(x_{i+k})$. The choice of non-linearity is not important for this question, but since it asks for a feedforward network, you should have a hidden layer. This is about the simplest possible model. Design a RNN to model $P(y_i\\mid x)$. . Identify any independence assumptions you make. Draw a diagram that illustrates how the model computes probabilities for the tag of the word â€œwithâ€: What is the input, and how is the output distribution computed from the input? Write out the basic equations of the model, and explain your choices. One design for the RNN is to model $P(y_i\\mid x_1, â€¦, x_i)$. That is, the RNN reads $x_1$ through $x_i$ one step at a time, and at the ith step produces a distribution for possible tags $y_i$. For simplicity, letâ€™s use RNN to denote a unit that receives an input and a previous hidden state, and produces a new hidden state; it can easily be replaced with an LSTM or other recurrent unit of your choice: $P(y_i\\mid x_1, â€¦, x_i) = \\text{softmax}(Wh_i+b)$, where $h_i = \\text{RNN}(\\text{onehot}(x_i), h_{i-1})$. References â€˜Transformers from scratchâ€™ Available: https://peterbloem.nl/blog/transformersÂ â†©ï¸\nTu, Zhaopeng, et al. â€œNeural machine translation with reconstruction.â€ Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 31. No. 1. 2017.Â â†©ï¸\nKoehn, Philipp, and Rebecca Knowles. â€œSix challenges for neural machine translation.â€ arXiv preprint arXiv:1706.03872 (2017).Â â†©ï¸\nHoltzman, Ari, et al. â€œThe curious case of neural text degeneration.â€ arXiv preprint arXiv:1904.09751 (2019).Â â†©ï¸\nVinyals, Oriol, et al. â€œGrammar as a foreign language.â€ Advances in neural information processing systems 28 (2015).Â â†©ï¸Â â†©ï¸\nCao, Steven, Nikita Kitaev, and Dan Klein. â€œUnsupervised parsing via constituency tests.â€ arXiv preprint arXiv:2010.03146 (2020).Â â†©ï¸\nKaplan, Jared, et al. â€œScaling laws for neural language models.â€ arXiv preprint arXiv:2001.08361 (2020).Â â†©ï¸\nHoffmann, Jordan, et al. â€œTraining compute-optimal large language models.â€ arXiv preprint arXiv:2203.15556 (2022).Â â†©ï¸\n",
  "wordCount" : "10056",
  "inLanguage": "zh",
  "image":"https://martinspace.top/zh/nluplus/img/nluplus/whatisrnn.jpg","datePublished": "2025-02-01T10:39:49Z",
  "dateModified": "2025-02-01T10:39:49Z",
  "author":[{
    "@type": "Person",
    "name": "Martin"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://martinspace.top/zh/nluplus/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Martin's space",
    "logo": {
      "@type": "ImageObject",
      "url": "https://martinspace.top/icebear.jpg"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>



<script async src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://martinspace.top/zh/" accesskey="h" title="Martin&#39;s space (Alt + H)">
                <img src="https://martinspace.top/icebear.jpg" alt="" aria-label="logo"
                    height="35">Martin&#39;s space</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://martinspace.top/en/" title="English"
                            aria-label="English">English</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://martinspace.top/zh/" title="ğŸ  ä¸»é¡µ">
                    <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/archives/" title="ğŸ“ å½’æ¡£">
                    <span>ğŸ“ å½’æ¡£</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/tags" title="ğŸ·ï¸ æ ‡ç­¾">
                    <span>ğŸ·ï¸ æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                    <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/about" title="ğŸ’­ å…³äº">
                    <span>ğŸ’­ å…³äº</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://martinspace.top/zh/">ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://martinspace.top/zh/post/">ğŸ“’ æ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://martinspace.top/zh/post/2research/">ğŸ”¬ ç ”ç©¶è®°å½•</a></div>
    <h1 class="post-title">
      ä»RNNåˆ°Transformers
    </h1>
    <div class="post-meta">










åˆ›å»º: 2025-02-01 | æ›´æ–°: 2025-02-01 | å­—æ•°: 10056å­— | é˜…è¯»æ—¶é•¿: 21åˆ†é’Ÿ | 
ä½œè€…:Martin&nbsp;|&nbsp;æ ‡ç­¾: &nbsp;
    <ul class="post-tags-meta">
        <a href="https://martinspace.top/zh/tags/nlp/">NLP</a>
    </ul>


    
    </div>
  </header> 
<figure class="entry-cover1"><img loading="lazy" src="https://martinspace.top/img/nluplus/whatisrnn.jpg" alt="">
        
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">ç›®å½•</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e9%80%92%e5%bd%92%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" aria-label="é€’å½’ç¥ç»ç½‘ç»œ">é€’å½’ç¥ç»ç½‘ç»œ</a><ul>
                            
                    <li>
                        <a href="#back-propogation-through-time" aria-label="Back Propogation Through Time">Back Propogation Through Time</a></li>
                    <li>
                        <a href="#bptt-variation" aria-label="BPTT Variation">BPTT Variation</a><ul>
                            
                    <li>
                        <a href="#truncted-bptt" aria-label="Truncted BPTT">Truncted BPTT</a></li>
                    <li>
                        <a href="#online-bptt" aria-label="Online BPTT">Online BPTT</a></li></ul>
                    </li>
                    <li>
                        <a href="#rnn-variation" aria-label="RNN Variation">RNN Variation</a><ul>
                            
                    <li>
                        <a href="#lstm" aria-label="LSTM">LSTM</a></li>
                    <li>
                        <a href="#gru" aria-label="GRU">GRU</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#transformers" aria-label="Transformers">Transformers</a><ul>
                            
                    <li>
                        <a href="#self-attention" aria-label="Self-Attention">Self-Attention</a></li>
                    <li>
                        <a href="#query-key-value" aria-label="Query, Key, Value">Query, Key, Value</a></li>
                    <li>
                        <a href="#%e7%82%b9%e7%a7%af%e7%bc%a9%e6%94%be" aria-label="ç‚¹ç§¯ç¼©æ”¾">ç‚¹ç§¯ç¼©æ”¾</a></li>
                    <li>
                        <a href="#multi-head-attention" aria-label="Multi-head Attention">Multi-head Attention</a></li>
                    <li>
                        <a href="#%e4%bb%80%e4%b9%88%e6%98%aftransformer" aria-label="ä»€ä¹ˆæ˜¯Transformer?">ä»€ä¹ˆæ˜¯Transformer?</a></li>
                    <li>
                        <a href="#bert" aria-label="BERT">BERT</a></li></ul>
                    </li>
                    <li>
                        <a href="#parsing" aria-label="Parsing">Parsing</a><ul>
                            
                    <li>
                        <a href="#decoding-with-llms" aria-label="Decoding with LLMs">Decoding with LLMs</a><ul>
                            
                    <li>
                        <a href="#greedy-decoding" aria-label="Greedy Decoding">Greedy Decoding</a></li>
                    <li>
                        <a href="#beam-searching-decoding" aria-label="Beam Searching Decoding">Beam Searching Decoding</a></li>
                    <li>
                        <a href="#sampling" aria-label="Sampling">Sampling</a></li></ul>
                    </li>
                    <li>
                        <a href="#neural-parsing" aria-label="Neural Parsing">Neural Parsing</a></li>
                    <li>
                        <a href="#unsupervised-parsing" aria-label="Unsupervised Parsing">Unsupervised Parsing</a></li></ul>
                    </li>
                    <li>
                        <a href="#llm%e5%89%8d%e6%b2%bf%e7%a0%94%e7%a9%b6" aria-label="LLMå‰æ²¿ç ”ç©¶">LLMå‰æ²¿ç ”ç©¶</a><ul>
                            
                    <li>
                        <a href="#scaling-law" aria-label="Scaling Law">Scaling Law</a><ul>
                            
                    <li>
                        <a href="#power-law" aria-label="Power Law">Power Law</a></li>
                    <li>
                        <a href="#kaplan-et-al-2020" aria-label="Kaplan et al. (2020)">Kaplan et al. (2020)</a></li>
                    <li>
                        <a href="#hoffmann-et-al-2020" aria-label="Hoffmann et al. (2020)">Hoffmann et al. (2020)</a></li>
                    <li>
                        <a href="#%e5%a2%9e%e5%8a%a0%e6%95%88%e7%8e%87" aria-label="å¢åŠ æ•ˆç‡">å¢åŠ æ•ˆç‡</a></li>
                    <li>
                        <a href="#double-descent" aria-label="Double Descent">Double Descent</a></li></ul>
                    </li>
                    <li>
                        <a href="#llms-as-formal-machines" aria-label="LLMs as Formal Machines">LLMs as Formal Machines</a></li></ul>
                    </li>
                    <li>
                        <a href="#llm%e5%be%ae%e8%b0%83" aria-label="LLMå¾®è°ƒ">LLMå¾®è°ƒ</a></li>
                    <li>
                        <a href="#evaluating-generation-and-machine-translation" aria-label="Evaluating Generation and Machine Translation">Evaluating Generation and Machine Translation</a></li>
                    <li>
                        <a href="#ethics" aria-label="Ethics">Ethics</a></li>
                    <li>
                        <a href="#evaluation-of-llm-and-alignment" aria-label="Evaluation of LLM and Alignment">Evaluation of LLM and Alignment</a></li>
                    <li>
                        <a href="#qa-and-rag" aria-label="QA and RAG">QA and RAG</a></li>
                    <li>
                        <a href="#tutorials" aria-label="Tutorials">Tutorials</a><ul>
                            
                    <li>
                        <a href="#language-models" aria-label="Language Models">Language Models</a></li></ul>
                    </li>
                    <li>
                        <a href="#references" aria-label="References">References</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>
  <div class="post-content"><h1 id="é€’å½’ç¥ç»ç½‘ç»œ">é€’å½’ç¥ç»ç½‘ç»œ<a hidden class="anchor" aria-hidden="true" href="#é€’å½’ç¥ç»ç½‘ç»œ">#</a></h1>
<p>ç®€å•çš„ç¥ç»ç½‘ç»œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šè¾“å…¥ï¼Œéšè—å±‚ï¼Œè¾“å‡º</p>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ä¸€èˆ¬æ˜¯å¢åŠ éšè—å±‚çš„æ•°é‡ã€‚</p>
<p>é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ›´åŠ å…³æ³¨çš„æ˜¯éšè—å±‚ä¸­æ¯ä¸ªç¥ç»å…ƒåœ¨æ—¶é—´ä¸Šçš„æˆé•¿ä¸è¿›æ­¥ã€‚</p>
<h2 id="back-propogation-through-time">Back Propogation Through Time<a hidden class="anchor" aria-hidden="true" href="#back-propogation-through-time">#</a></h2>
<p>åœ¨æ™®é€šçš„å‰é¦ˆç¥ç»ç½‘ç»œ (feed-forward nn) ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åº”ç”¨åå‘ä¼ æ’­æ¥è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°æƒé‡ã€‚ä½†æ˜¯åœ¨ RNN è¿™ç§å…·æœ‰æ—¶é—´ä¾èµ–çš„ç½‘ç»œä¸­ï¼Œå½“å‰çš„è¾“å‡ºä¸ä»…ä¾èµ–äºå½“å‰è¾“å…¥ï¼Œè¿˜ä¾èµ–äºè¿‡å»çš„éšè—çŠ¶æ€ã€‚è¿™å°±å¯¼è‡´äº†å‚æ•°çš„æ¢¯åº¦è®¡ç®—éœ€è¦æ²¿æ—¶é—´ç»´åº¦è¿›è¡Œä¼ æ’­ï¼Œè€Œ BPTT æ­£æ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„æ–¹æ³•ã€‚
æ•°å­¦æ¨å¯¼å¦‚ä¸‹ï¼š</p>
<p><strong>(1) è®¾å®šç¬¦å·</strong></p>
<ul>
<li>è¾“å…¥åºåˆ—ï¼š$ X = ${$x_1, x_2, &hellip;, x_T$}</li>
<li>è¾“å‡ºåºåˆ—ï¼š$ Y = ${$y_1, y_2, &hellip;, y_T$}</li>
<li>éšè—çŠ¶æ€ï¼š$ h_t $ è¡¨ç¤ºæ—¶é—´æ­¥ $ t $ çš„éšè—çŠ¶æ€</li>
<li>å‚æ•°ï¼š
<ul>
<li>$ W_{xh} $ï¼šè¾“å…¥åˆ°éšè—å±‚çš„æƒé‡</li>
<li>$ W_{hh} $ï¼šéšè—å±‚åˆ°éšè—å±‚çš„æƒé‡</li>
<li>$ W_{hy} $ï¼šéšè—å±‚åˆ°è¾“å‡ºçš„æƒé‡</li>
</ul>
</li>
<li>æŸå¤±å‡½æ•°ï¼š$ L(Y, \hat{Y}) $ï¼ˆå¦‚ MSE æˆ–äº¤å‰ç†µï¼‰</li>
</ul>
<p><strong>(2) RNN çš„å‰å‘ä¼ æ’­</strong>
åœ¨ RNN ä¸­ï¼Œæ¯ä¸ªæ—¶é—´æ­¥çš„è®¡ç®—å¦‚ä¸‹ï¼š
$$
h_t = f(W_{hh} h_{t-1} + W_{xh} x_t)
$$
$$
\hat{y} _ t = g(W_{hy} h_t)
$$
å…¶ä¸­ï¼š</p>
<ul>
<li>$ f $ æ˜¯éšè—çŠ¶æ€çš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ tanh æˆ– ReLUï¼‰</li>
<li>$ g $ æ˜¯è¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ softmaxï¼‰</li>
</ul>
<p>æ€»æŸå¤±ä¸ºï¼š
$
L = \sum_{t=1}^{T} L_t(y_t, \hat{y}_t)
$</p>
<p><strong>(3) åå‘ä¼ æ’­ Through Time</strong></p>
<ol>
<li>
<p><strong>è®¡ç®—æŸå¤±å¯¹è¾“å‡ºçš„æ¢¯åº¦</strong>
$
\frac{\partial L}{\partial \hat{y} _ t} = \nabla_{\hat{y}_t} L_t
$
è®¡ç®—æŸå¤±ç›¸å¯¹äºè¾“å‡ºçš„æ¢¯åº¦ï¼Œè¿™éƒ¨åˆ†å’Œæ™®é€š BP è®¡ç®—ç±»ä¼¼ã€‚</p>
</li>
<li>
<p><strong>è®¡ç®—æŸå¤±å¯¹éšè—çŠ¶æ€çš„æ¢¯åº¦</strong>
$
\frac{\partial L}{\partial h_t} = \frac{\partial L_t}{\partial h_t} + \frac{\partial L_{t+1}}{\partial h_t} + \frac{\partial L_{t+2}}{\partial h_t} + \dots
$
ç”±äºéšè—çŠ¶æ€ $ h_t $ ä¼šå½±å“åç»­çš„æ‰€æœ‰æ—¶é—´æ­¥ï¼Œå› æ­¤æ¢¯åº¦è¦æ²¿æ—¶é—´æ–¹å‘<strong>åå‘ç´¯ç§¯</strong>ã€‚</p>
</li>
<li>
<p><strong>è®¡ç®—å‚æ•°çš„æ¢¯åº¦</strong></p>
<ul>
<li><strong>å¯¹éšè—çŠ¶æ€çš„æƒé‡ $ W_{hh} $</strong>ï¼š
$
\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^{T} \frac{\partial L}{\partial h_t} \cdot \frac{\partial h_t}{\partial W_{hh}}
$</li>
<li><strong>å¯¹è¾“å…¥æƒé‡ $ W_{xh} $</strong>ï¼š
$
\frac{\partial L}{\partial W_{xh}} = \sum_{t=1}^{T} \frac{\partial L}{\partial h_t} \cdot \frac{\partial h_t}{\partial W_{xh}}
$</li>
<li><strong>å¯¹è¾“å‡ºæƒé‡ $ W_{hy} $</strong>ï¼š
$
\frac{\partial L}{\partial W _ {hy}} = \sum _ {t=1}^{T} \frac{\partial L}{\partial \hat{y} _ t} \cdot \frac{\partial \hat{y} _ t}{\partial W_{hy}}
$</li>
</ul>
</li>
<li>
<p><strong>åå‘ä¼ æ’­å›ä¼ æ¢¯åº¦</strong></p>
<ul>
<li>è®¡ç®— <strong>æ¢¯åº¦ä¼ æ’­è·¯å¾„</strong>ï¼Œä»æŸå¤± $ L $ åå‘é€šè¿‡æ—¶é—´æ­¥ $ T \to 1 $ é€æ­¥æ›´æ–°å‚æ•°ã€‚</li>
<li>ä½¿ç”¨ <strong>æ¢¯åº¦ä¸‹é™ï¼ˆSGD, Adamï¼‰</strong> æ¥æ›´æ–°æƒé‡ã€‚</li>
</ul>
</li>
</ol>
<p>æ˜¾è€Œæ˜“è§ï¼ŒBPTTå­˜åœ¨ä¸¤ä¸ªè‡´å‘½é—®é¢˜ï¼š</p>
<ul>
<li>ç¬¬ä¸€ä¸ªæ˜¯æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ï¼šå½“æƒé‡çŸ©é˜µçš„èŒƒæ•°$||W_{hh}||$å°äº1æ—¶ï¼Œéšç€æ—¶é—´æ­¥çš„å¢åŠ ï¼Œæ¢¯åº¦æŒ‡æ•°çº§è¡°å‡ï¼Œå¯¼è‡´æ—©æœŸæ—¶é—´æ­¥çš„æ¢¯åº¦å‡ ä¹ä¸º 0ï¼Œæ— æ³•æœ‰æ•ˆè®­ç»ƒï¼›
<ul>
<li>å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰ æ¥é™åˆ¶æ¢¯åº¦å¤§å°ï¼›ä½¿ç”¨é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰æˆ– é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰ ä»£æ›¿ RNNã€‚</li>
</ul>
</li>
<li>ç¬¬äºŒä¸ªé—®é¢˜åœ¨äºè®¡ç®—å¼€é”€ï¼šBPTT éœ€è¦å­˜å‚¨æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå› æ­¤å½“åºåˆ—å¾ˆé•¿æ˜¯ï¼Œè®­ç»ƒæ•ˆç‡ä½ã€‚
<ul>
<li>å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼šTruncated BPTTï¼Œä»…åœ¨å›ºå®šçª—å£å¤§å°å†…è¿›è¡Œæ¢¯åº¦ä¼ æ’­ï¼Œè€Œä¸å›æº¯æ•´ä¸ªåºåˆ—ã€‚</li>
</ul>
</li>
</ul>
<h2 id="bptt-variation">BPTT Variation<a hidden class="anchor" aria-hidden="true" href="#bptt-variation">#</a></h2>
<h3 id="truncted-bptt">Truncted BPTT<a hidden class="anchor" aria-hidden="true" href="#truncted-bptt">#</a></h3>
<ul>
<li><strong>ç­–ç•¥</strong>ï¼šä¸å±•å¼€æ•´ä¸ªæ—¶é—´åºåˆ—ï¼Œè€Œæ˜¯ä»…åœ¨ <strong>å›ºå®šçª—å£å¤§å° $ k $</strong> å†…è¿›è¡Œåå‘ä¼ æ’­ï¼ˆå¦‚$ k=10 $ï¼‰ã€‚</li>
<li><strong>ä¼˜ç‚¹</strong>ï¼šå‡å°‘è®¡ç®—é‡ï¼Œé€‚ç”¨äºé•¿åºåˆ—è®­ç»ƒã€‚</li>
<li><strong>ç¼ºç‚¹</strong>ï¼šå¯èƒ½å¯¼è‡´é•¿ç¨‹ä¾èµ–ä¿¡æ¯ä¸¢å¤±ã€‚</li>
</ul>
<h3 id="online-bptt">Online BPTT<a hidden class="anchor" aria-hidden="true" href="#online-bptt">#</a></h3>
<ul>
<li>è®¡ç®—ä¸€ä¸ªæ—¶é—´æ­¥çš„æ¢¯åº¦ <strong>ç«‹å³æ›´æ–°å‚æ•°</strong>ï¼ˆç±»ä¼¼åœ¨çº¿å­¦ä¹ ï¼‰ã€‚</li>
<li>é€‚ç”¨äº <strong>æµå¼æ•°æ®å¤„ç†ï¼ˆå¦‚è¯­éŸ³è¯†åˆ«ï¼‰</strong>ã€‚</li>
</ul>
<h2 id="rnn-variation">RNN Variation<a hidden class="anchor" aria-hidden="true" href="#rnn-variation">#</a></h2>
<p>LSTM å’Œ GRU æ˜¯RNNçš„ä¸¤ç§ä¸»è¦å˜ä½“ï¼Œå®ƒä»¬è¢«è®¾è®¡ç”¨äºè§£å†³ä¼ ç»ŸRNNçš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰é•¿ç¨‹ä¾èµ–ï¼ˆlong-term dependenciesï¼‰ã€‚LSTMå’ŒGRUå¯ä»¥é€šè¿‡é—¨æ§æœºåˆ¶ï¼ˆGatesï¼‰ æ§åˆ¶ä¿¡æ¯æµåŠ¨ï¼Œèƒ½å¤Ÿé€‰æ‹©æ€§ä¿ç•™æˆ–é—å¿˜ä¿¡æ¯ï¼Œæœ‰æ•ˆç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä»è€Œæ›´å¥½åœ°å­¦ä¹ é•¿ç¨‹ä¾èµ–ã€‚</p>
<h3 id="lstm">LSTM<a hidden class="anchor" aria-hidden="true" href="#lstm">#</a></h3>
<p><strong>(1) ç»“æ„</strong></p>
<p>LSTM ä¸»è¦å¼•å…¥äº†ä¸‰ä¸ªGateså’Œä¸€ä¸ªè®°å¿†å•å…ƒï¼ˆCell State, $ C_t $ï¼‰ï¼š</p>
<ul>
<li><strong>é—å¿˜é—¨ï¼ˆForget Gateï¼‰</strong>ï¼šå†³å®š<strong>ä¸¢å¼ƒ</strong>å¤šå°‘è¿‡å»çš„ä¿¡æ¯ã€‚</li>
<li><strong>è¾“å…¥é—¨ï¼ˆInput Gateï¼‰</strong>ï¼šå†³å®š<strong>æ›´æ–°</strong>å¤šå°‘æ–°ä¿¡æ¯åˆ°è®°å¿†å•å…ƒã€‚</li>
<li><strong>è¾“å‡ºé—¨ï¼ˆOutput Gateï¼‰</strong>ï¼šå†³å®š<strong>è¾“å‡º</strong>å¤šå°‘éšè—çŠ¶æ€ã€‚</li>
</ul>
<p><strong>å®Œæ•´è®¡ç®—æµç¨‹</strong>ï¼š
$$
f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)
$$
$$
i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)
$$
$$
\tilde{C} _ t = \tanh(W_c [h_{t-1}, x_t] + b_c)
$$
$$
C _ t = f _ t \odot C _ {t-1} + i _ t \odot \tilde{C} _ t
$$
$$
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)
$$
$$
h_t = o_t \odot \tanh(C_t)
$$</p>
<p><strong>(2) è¯¦ç»†è§£é‡Š</strong></p>
<ul>
<li><strong>é—å¿˜é—¨ $ f_t $</strong>ï¼šæ§åˆ¶æ˜¯å¦é—å¿˜è¿‡å»çš„è®°å¿†ã€‚</li>
<li><strong>è¾“å…¥é—¨ $ i_t $</strong>ï¼šæ§åˆ¶æ˜¯å¦å†™å…¥æ–°çš„ä¿¡æ¯ã€‚</li>
<li><strong>å€™é€‰è®°å¿† $ \tilde{C}_t $</strong>ï¼šå½“å‰æ—¶é—´æ­¥çš„æ–°ä¿¡æ¯ã€‚</li>
<li><strong>è®°å¿†å•å…ƒ $ C_t $</strong>ï¼šæ›´æ–°åçš„é•¿ç¨‹è®°å¿†ï¼Œ<strong>ä¿¡æ¯å¯ä»¥ç›´æ¥åœ¨æ—¶é—´è½´ä¸ŠæµåŠ¨ï¼Œä¸æ˜“ä¸¢å¤±</strong>ã€‚</li>
<li><strong>è¾“å‡ºé—¨ $ o_t $</strong>ï¼šå†³å®šéšè—çŠ¶æ€ $ h_t $ çš„è¾“å‡ºã€‚</li>
</ul>
<blockquote>
<p>$ C_t $ ç›´æ¥é€šè¿‡åŠ æ³•è¿æ¥ï¼Œä½¿å¾—æ¢¯åº¦æµåŠ¨ä¸ä¼šå› å¤šæ¬¡ä¹˜æ³•è€Œæ¶ˆå¤±ã€‚</p>
</blockquote>
<h3 id="gru">GRU<a hidden class="anchor" aria-hidden="true" href="#gru">#</a></h3>
<p>GRU æ˜¯ LSTM çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå»æ‰äº†ç‹¬ç«‹çš„è®°å¿†å•å…ƒ $ C_t $ï¼Œä»…ä½¿ç”¨éšè—çŠ¶æ€ $ h_t $ ä½œä¸ºå­˜å‚¨ä¿¡æ¯çš„ä»‹è´¨ã€‚</p>
<p><strong>(1) ç»“æ„</strong>
GRU ä¸»è¦æœ‰<strong>ä¸¤ä¸ªé—¨</strong>ï¼š</p>
<ul>
<li><strong>é‡ç½®é—¨ï¼ˆReset Gateï¼‰</strong>ï¼šæ§åˆ¶å¦‚ä½•ç»“åˆæ–°çš„è¾“å…¥å’Œè¿‡å»çš„éšè—çŠ¶æ€ã€‚</li>
<li><strong>æ›´æ–°é—¨ï¼ˆUpdate Gateï¼‰</strong>ï¼šæ§åˆ¶å¤šå°‘è¿‡å»çš„ä¿¡æ¯ä¿ç•™ï¼Œå¤šå°‘æ–°ä¿¡æ¯åŠ å…¥ã€‚</li>
</ul>
<p><strong>å®Œæ•´è®¡ç®—æµç¨‹</strong>ï¼š
$$
r_t = \sigma(W_r [h_{t-1}, x_t] + b_r)
$$
$$
z_t = \sigma(W_z [h_{t-1}, x_t] + b_z)
$$
$$
\tilde{h} _ t = \tanh(W_h [r_t \odot h_{t-1}, x_t] + b_h)
$$
$$
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h} _ t
$$</p>
<p><strong>(2) è¯¦ç»†è§£é‡Š</strong></p>
<ul>
<li><strong>é‡ç½®é—¨ $ r_t $</strong>ï¼šå†³å®š<strong>æ˜¯å¦é—å¿˜</strong>è¿‡å»çš„éšè—çŠ¶æ€ã€‚</li>
<li><strong>æ›´æ–°é—¨ $ z_t $</strong>ï¼šå†³å®š<strong>æ–°æ—§ä¿¡æ¯çš„æ··åˆæ¯”ä¾‹</strong>ï¼Œç±»ä¼¼ LSTM çš„è¾“å…¥é—¨å’Œé—å¿˜é—¨çš„ç»“åˆã€‚</li>
<li><strong>å€™é€‰çŠ¶æ€ $ \tilde{h}_t $</strong>ï¼šæ–°çš„ä¿¡æ¯ã€‚</li>
<li><strong>æœ€ç»ˆéšè—çŠ¶æ€ $ h_t $</strong>ï¼š
<ul>
<li>å½“ $ z_t $ é€¼è¿‘ 1ï¼šå½“å‰çŠ¶æ€ <strong>æ¥è¿‘æ–°ä¿¡æ¯</strong>ã€‚</li>
<li>å½“ $ z_t $ é€¼è¿‘ 0ï¼šå½“å‰çŠ¶æ€ <strong>ä¿ç•™è¿‡å»çš„éšè—çŠ¶æ€</strong>ã€‚</li>
</ul>
</li>
</ul>
<blockquote>
<p>GRU ç”¨æ›´æ–°é—¨åˆå¹¶äº† LSTM çš„è¾“å…¥é—¨å’Œé—å¿˜é—¨ï¼Œå› æ­¤è®¡ç®—æ›´ç®€å•ã€‚</p>
</blockquote>
<h1 id="transformers">Transformers<a hidden class="anchor" aria-hidden="true" href="#transformers">#</a></h1>
<p>è™½ç„¶ LSTM å’Œ GRU ä»ç„¶è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†ç°ä»£NLPä»»åŠ¡å¤§å¤šé‡‡ç”¨ <strong>Transformer</strong>ï¼ˆåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼Œå› ä¸ºï¼š</p>
<ul>
<li>Transformer <strong>å¹¶è¡Œè®¡ç®—</strong> æ€§èƒ½æ›´å¼ºï¼Œè€Œ LSTM/GRU ä¾èµ–åºåˆ—å¤„ç†ï¼Œæ— æ³•å¹¶è¡Œã€‚</li>
<li>Transformer é€šè¿‡ <strong>Self-Attention</strong> ç›´æ¥å»ºæ¨¡é•¿ç¨‹ä¾èµ–ï¼Œè€ŒLSTM/GRUä»ç„¶æœ‰ä¸€å®šä¿¡æ¯è¡°å‡é—®é¢˜ã€‚</li>
</ul>
<p>ä½†åœ¨è¯­éŸ³å¤„ç†ã€æ—¶é—´åºåˆ—é¢„æµ‹ç­‰ä»»åŠ¡ä¸­ï¼ŒLSTM/GRUä»ç„¶å…·æœ‰è¾ƒå¥½çš„è¡¨ç°ã€‚</p>
<h2 id="self-attention">Self-Attention<a hidden class="anchor" aria-hidden="true" href="#self-attention">#</a></h2>
<blockquote>
<p>The fundamental operation of any transformer architecture is the self-attention operation <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
</blockquote>
<p>è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ä¸ªä»åºåˆ—åˆ°åºåˆ—çš„æ“ä½œï¼Œå®ƒçš„è¾“å…¥æ˜¯ä¸€ç»„å‘é‡ $x_1, x_2, &hellip;, x_t$ï¼Œå¯¹åº”çš„è¾“å‡ºåºåˆ—æ˜¯ $y_1, y_2, &hellip;, y_t$ã€‚ï¼ˆæ¯ä¸ªå‘é‡éƒ½æ˜¯kç»´ï¼‰</p>
<p>æ¯ä¸€ä¸ª $y_i$ æ˜¯æ€ä¹ˆè®¡ç®—çš„ï¼Ÿ$y_i$æ˜¯æ ¹æ®å½“å‰ä½ç½®çš„$x_i$å’Œå…¶ä»–æ‰€æœ‰ä½ç½®çš„$x_j$çš„ç›¸å…³æ€§è®¡ç®—å‡ºæ¥çš„ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹
$$y_{\color{red}{i}} = \sum_{\color{blue}{j}} w_{\color{red}{i}\color{blue}{j}} x_{\color{blue}{j}}$$</p>
<p>å¯¹äº$w_{\color{red}{i}\color{blue}{j}}$, å®ƒæ˜¯ç”±$x_{\color{red}{i}}, x_{\color{blue}{j}}$çš„ç‚¹ç§¯å‡½æ•°æ¨å¯¼å‡ºæ¥çš„ã€‚
$$w_{\color{red}{i}\color{blue}{j}} = \text{softmax}(w^{\prime}_{\color{red}{i}\color{blue}{j}})$$</p>
<p>å…¶ä¸­
$$w^{\prime}_{\color{red}{i}\color{blue}{j}} = x^T_ix_j$$</p>
<p>åŸç†å¯è§†åŒ–å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯æ•´ä¸ªä½“ç³»ç»“æ„ä¸­å”¯ä¸€åœ¨å‘é‡ä¹‹é—´ä¼ æ’­ä¿¡æ¯çš„æ“ä½œã€‚
<img loading="lazy" src="/img/nluplus/self-attention.png" alt="self-attention"  />
</p>
<p>ä¸ºä»€ä¹ˆself-attentionæœºåˆ¶å¦‚æ­¤æœ‰æ•ˆï¼Ÿæˆ‘ä»¬æ‹¿ç”µå½±æ¨èç³»ç»Ÿæ¥ä¸¾ä¾‹å­ã€‚å‡è®¾ä½ ç»è¥ä¸€å®¶ç”µå½±ç§Ÿèµå…¬å¸ï¼Œä½ æœ‰ä¸€äº›ç”µå½±å’Œä¸€äº›ç”¨æˆ·ï¼Œä½ æƒ³å‘ä½ çš„ç”¨æˆ·æ¨èä»–ä»¬å¯èƒ½ä¼šå–œæ¬¢çš„ç”µå½±ã€‚ä¸€ç§æ–¹æ³•æ˜¯ä¸ºç”µå½±æ‰‹åŠ¨åˆ›å»ºç‰¹å¾ï¼Œä¾‹å¦‚ç”µå½±ä¸­æœ‰å¤šå°‘æµªæ¼«æƒ…èŠ‚ï¼Œæœ‰å¤šå°‘åŠ¨ä½œåœºé¢ï¼Œç„¶åä¸ºç”¨æˆ·è®¾è®¡ç›¸åº”çš„ç‰¹å¾ï¼šä»–ä»¬æœ‰å¤šå–œæ¬¢æµªæ¼«ç”µå½±ï¼Œæœ‰å¤šå–œæ¬¢åŠ¨ä½œç‰‡ã€‚å¦‚æœè¿™æ ·åšï¼Œä¸¤ä¸ªç‰¹å¾å‘é‡ä¹‹é—´çš„ç‚¹ç§¯å°±ä¼šä¸ºç”µå½±å±æ€§ä¸ç”¨æˆ·å–œå¥½çš„åŒ¹é…ç¨‹åº¦æ‰“åˆ†ã€‚å¦‚æœç”¨æˆ·å’Œç”µå½±çš„æŸä¸ªç‰¹å¾çš„ç¬¦å·ç›¸åŒ¹é…&ndash;ç”µå½±å¾ˆæµªæ¼«ï¼Œç”¨æˆ·å–œæ¬¢æµªæ¼«ï¼Œæˆ–è€…ç”µå½±ä¸æµªæ¼«ï¼Œç”¨æˆ·è®¨åŒæµªæ¼«&ndash;é‚£ä¹ˆå¾—åˆ°çš„ç‚¹ç§¯å°±ä¼šå¾—åˆ°è¯¥ç‰¹å¾çš„æ­£å€¼ã€‚å¦‚æœç¬¦å·ä¸åŒ¹é…&ndash;ç”µå½±æµªæ¼«è€Œç”¨æˆ·è®¨åŒæµªæ¼«ï¼Œåˆ™ç›¸åº”é¡¹ä¸ºè´Ÿå€¼ã€‚
<img loading="lazy" src="/img/nluplus/movie_recommendation.png" alt="movie_recommendation"  />
</p>
<p>å½“ç„¶è¿™æ ·åšå¹¶ä¸ç°å®ï¼Œå› ä¸ºè¿™æ¶‰åŠåˆ°å¤§é‡çš„æ•°æ®æ ‡æ³¨å·¥ä½œã€‚ä¸€ç§è§£å†³åŠæ³•æ˜¯æˆ‘ä»¬å°†ç”µå½±ç‰¹å¾å’Œç”¨æˆ·ç‰¹å¾ä½œä¸ºæ¨¡å‹çš„å‚æ•°ï¼Œè®©ç”¨æˆ·é€‰æ‹©ä¸€äº›å–œæ¬¢çš„ç”µå½±ï¼Œç„¶åé€šè¿‡ä¼˜åŒ–ç®—æ³•è°ƒæ•´ç”¨æˆ·ç‰¹å¾å’Œç”µå½±ç‰¹å¾ï¼Œä½¿å®ƒä»¬çš„ç‚¹ç§¯èƒ½å¤ŸåŒ¹é…ç”¨æˆ·çš„å·²çŸ¥å–œå¥½ã€‚å³ä½¿æˆ‘ä»¬æ²¡æœ‰å‘Šè¯‰æ¨¡å‹è¿™äº›ç‰¹å¾çš„å…·ä½“å«ä¹‰ï¼Œä½†åœ¨è®­ç»ƒåï¼Œè¿™äº›ç‰¹å¾å¾€å¾€ä¼šè‡ªå‘åœ°æ˜ å°„åˆ°æœ‰æ„ä¹‰çš„ç”µå½±å†…å®¹ä¿¡æ¯ã€‚ ä¾‹å¦‚ï¼ŒæŸä¸ªç»´åº¦å¯èƒ½å¯¹åº”â€œæµªæ¼«â€ï¼Œå¦ä¸€ä¸ªç»´åº¦å¯èƒ½å¯¹åº”â€œåŠ¨ä½œâ€æˆ–â€œå–œå‰§â€ï¼Œå³ä½¿æˆ‘ä»¬äº‹å…ˆå¹¶æœªæ˜ç¡®è¿™äº›ç»´åº¦çš„å«ä¹‰ã€‚</p>
<p>è¿™å°±æ˜¯self-attentionæœºåˆ¶ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ç§è®¡ç®—åºåˆ—å†…éƒ¨å…³ç³»çš„æ–¹æ³•ã€‚åœ¨å¤„ç†æ–‡æœ¬æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆä¸ºæ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ªå¯å­¦ä¹ çš„<strong>åµŒå…¥å‘é‡ï¼ˆembedding vectorï¼‰</strong>ï¼Œä¾‹å¦‚å¥å­ &ldquo;the cat walks on the street&rdquo; ä¼šè¢«æ˜ å°„ä¸ºå‘é‡åºåˆ— $ v_{\text{the}}, v_{\text{cat}}, v_{\text{walks}}, v_{\text{on}}, v_{\text{the}}, v_{\text{street}} $ã€‚ç„¶åï¼Œè¿™äº›å‘é‡è¿›å…¥self-attention layerï¼Œè¾“å‡ºæ–°çš„å‘é‡åºåˆ— $ y_{\text{the}}, y_{\text{cat}}, y_{\text{walks}}, y_{\text{on}}, y_{\text{the}}, y_{\text{street}} $ã€‚å…¶ä¸­ï¼Œæ¯ä¸ª $ y_t $ éƒ½æ˜¯è¾“å…¥å‘é‡çš„åŠ æƒå’Œï¼Œè€Œæƒé‡æ˜¯æ ¹æ®<strong>å½’ä¸€åŒ– (softmax) çš„ç‚¹ç§¯ï¼ˆdot productï¼‰</strong> è®¡ç®—å¾—åˆ°çš„ã€‚ç‚¹ç§¯è¡¡é‡äº†å•è¯ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¦‚æœä¸¤ä¸ªå•è¯åœ¨ä¸Šä¸‹æ–‡ä¸­ç´§å¯†ç›¸å…³ï¼Œå®ƒä»¬çš„ç‚¹ç§¯å°±ä¼šè¾ƒé«˜ï¼Œä»è€Œè·å¾—æ›´é«˜çš„æ³¨æ„åŠ›æƒé‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¥å­ä¸­ï¼Œâ€œwalksâ€ è¿™ä¸ªåŠ¨è¯éœ€è¦ä¸â€œcatâ€å»ºç«‹è”ç³»ï¼Œæ‰€ä»¥ $ v_{\text{walks}} $ å’Œ $ v_{\text{cat}} $ çš„ç‚¹ç§¯ä¼šè¾ƒé«˜ï¼Œè€Œâ€œtheâ€ è¿™æ ·çš„å† è¯ç”±äºå¯¹æ•´ä½“è¯­ä¹‰å½±å“è¾ƒå°ï¼Œé€šå¸¸ä¸ä¼šä¸å…¶ä»–è¯æœ‰å¾ˆå¤§çš„ç‚¹ç§¯å€¼ã€‚</p>
<p>æœ‰è¶£çš„æ˜¯ï¼ŒåŸºç¡€çš„self-attentionå¹¶ä¸å…³å¿ƒè¾“å…¥çš„é¡ºåºï¼Œå®ƒåªå…³æ³¨å•è¯ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ã€‚å¦‚æœæˆ‘ä»¬å¯¹è¾“å…¥åºåˆ—è¿›è¡Œé‡æ–°æ’åˆ—ï¼Œè¾“å‡ºçš„å‘é‡åºåˆ—ä¹Ÿä¼šä»¥ç›¸åŒçš„æ–¹å¼è¢«è°ƒæ•´ï¼Œè€Œä¸ä¼šæ”¹å˜å†…å®¹æœ¬èº«ã€‚è¿™æ„å‘³ç€è‡ªæ³¨æ„åŠ›æœ¬è´¨ä¸Šæ˜¯ä¸€ç§<strong>é›†åˆæ“ä½œï¼ˆSet Operationï¼‰</strong>ï¼Œå®ƒä¸ä¼šä¾èµ–è¯çš„å‰åé¡ºåºï¼Œè€Œæ˜¯è®©æ¨¡å‹è‡ªå·±å­¦ä¹ å“ªäº›å•è¯ä¹‹é—´åº”è¯¥æœ‰æ›´å¼ºçš„è”ç³»ã€‚æ­¤å¤–ï¼ŒåŸºç¡€çš„è‡ªæ³¨æ„åŠ›å±‚æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼Œå®ƒçš„è¡Œä¸ºå®Œå…¨å–å†³äºè¾“å…¥å‘é‡ï¼Œè€Œè¿™äº›å‘é‡æ¥è‡ªäºåµŒå…¥å±‚ï¼Œè€ŒåµŒå…¥å±‚æ˜¯å¯å­¦ä¹ çš„ã€‚åœ¨å®Œæ•´çš„ Transformer ç»“æ„ä¸­ï¼Œæˆ‘ä»¬ä¼šé€šè¿‡<strong>ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰</strong> è®©æ¨¡å‹æ„ŸçŸ¥å•è¯çš„é¡ºåºä¿¡æ¯ï¼Œä½†åœ¨çº¯è‡ªæ³¨æ„åŠ›å±‚ä¸­ï¼Œå®ƒæ˜¯<strong>å®Œå…¨é¡ºåºä¸æ•æ„Ÿ</strong>çš„ã€‚</p>
<h2 id="query-key-value">Query, Key, Value<a hidden class="anchor" aria-hidden="true" href="#query-key-value">#</a></h2>
<p>å½“ä»£transformerä¸­ä½¿ç”¨çš„self-attentionè¿˜ä¾èµ–ä¸‰ä¸ªé¢å¤–çš„æŠ€å·§ - Queries, Values, Keysã€‚åœ¨ <strong>è‡ªæ³¨æ„åŠ›</strong> è®¡ç®—ä¸­ï¼Œæ¯ä¸ªè¾“å…¥å‘é‡ $ x_i $ éœ€è¦åœ¨ä¸‰ç§ä¸åŒçš„è§’è‰²ä¹‹é—´åˆ‡æ¢ï¼š</p>
<ol>
<li><strong>æŸ¥è¯¢ï¼ˆQueryï¼Œ$ q_i $ï¼‰</strong>ï¼šæ¯ä¸ªè¯å‘é‡éƒ½éœ€è¦ä¸å…¶ä»–è¯å‘é‡è¿›è¡Œæ¯”è¾ƒï¼Œä»¥ç¡®å®šå®ƒå¯¹è‡ªå·±çš„è¾“å‡º $ y_i $ çš„å½±å“æƒé‡ã€‚</li>
<li><strong>é”®ï¼ˆKeyï¼Œ$ k_i $ï¼‰</strong>ï¼šæ¯ä¸ªè¯å‘é‡ä¹Ÿè¢«å…¶ä»–å•è¯ç”¨æ¥è®¡ç®—æƒé‡ï¼Œå³å®ƒå¯¹å…¶ä»–å•è¯è¾“å‡º $ y_j $ çš„å½±å“ã€‚</li>
<li><strong>å€¼ï¼ˆValueï¼Œ$ v_i $ï¼‰</strong>ï¼šåœ¨è®¡ç®—å®Œæƒé‡åï¼Œæ¯ä¸ªå•è¯çš„æœ€ç»ˆè¡¨ç¤ºæ˜¯æ‰€æœ‰è¯å‘é‡çš„åŠ æƒå’Œï¼Œå…¶ä¸­åŠ æƒæ–¹å¼ç”±æŸ¥è¯¢å’Œé”®è®¡ç®—å¾—åˆ°çš„æ³¨æ„åŠ›æƒé‡å†³å®šã€‚</li>
</ol>
<p>ä¸ºäº†æ›´æ–¹ä¾¿è®¡ç®—ï¼Œæˆ‘ä»¬ä¸ä¼šç›´æ¥ä½¿ç”¨åŸå§‹è¾“å…¥å‘é‡ $ x_i $ï¼Œè€Œæ˜¯å¯¹å…¶è¿›è¡Œä¸‰æ¬¡ä¸åŒçš„ <strong>çº¿æ€§å˜æ¢</strong>ï¼Œå¾—åˆ°ï¼š
$$
q_i = W_q x_i, \quad k_i = W_k x_i, \quad v_i = W_v x_i
$$
å…¶ä¸­ $ W_q, W_k, W_v $ æ˜¯å¯è®­ç»ƒçš„ <strong>æƒé‡çŸ©é˜µ</strong>ï¼Œç”¨äºå­¦ä¹ é€‚åˆä»»åŠ¡çš„æ³¨æ„åŠ›è¡¨ç¤ºã€‚</p>
<p>æ¥ä¸‹æ¥ï¼Œè®¡ç®—ä¸¤ä¸ªè¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ˆç›¸å…³æ€§ï¼‰:
$$
w&rsquo; = q_i^T k_j
$$
ç„¶åè¿›è¡Œ <strong>softmax å½’ä¸€åŒ–</strong> ä»¥è·å¾—æœ€ç»ˆçš„æ³¨æ„åŠ›æƒé‡ï¼š
$$
w_{ij} = \text{softmax}(w&rsquo;)
$$
æœ€åï¼Œæ¯ä¸ªå•è¯çš„è¾“å‡ºå‘é‡$ y_i $æ˜¯æ‰€æœ‰å€¼å‘é‡çš„åŠ æƒå’Œï¼š
$$
y_i = \sum_j w_{ij} v_j
$$
è¿™ä¸ªè¿‡ç¨‹è®©æ¨¡å‹èƒ½å¤ŸåŠ¨æ€è°ƒæ•´å•è¯ä¹‹é—´çš„ç›¸äº’å½±å“ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£åºåˆ—ä¸­çš„è¯­ä¹‰å…³ç³»ã€‚</p>
<p><img loading="lazy" src="/img/nluplus/qvk.png" alt="movie_recommendation"  />
</p>
<h2 id="ç‚¹ç§¯ç¼©æ”¾">ç‚¹ç§¯ç¼©æ”¾<a hidden class="anchor" aria-hidden="true" href="#ç‚¹ç§¯ç¼©æ”¾">#</a></h2>
<p>åˆšæ‰æˆ‘ä»¬ä½¿ç”¨ <strong>dot product</strong> è®¡ç®—æŸ¥è¯¢ï¼ˆQueryï¼‰å’Œé”®ï¼ˆKeyï¼‰ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶å°†å…¶è¾“å…¥ <strong>Softmax</strong> è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚ç„¶è€Œï¼Œå½“<strong>åµŒå…¥ç»´åº¦ $ k $ è¾ƒå¤§æ—¶</strong>ï¼Œç‚¹ç§¯çš„å€¼ä¼šå˜å¾—å¾ˆå¤§ï¼Œå¯¼è‡´ <strong>Softmax çš„æ¢¯åº¦å˜å¾—æå°ï¼Œç”šè‡³é€ æˆæ¢¯åº¦æ¶ˆå¤±ï¼Œå½±å“æ¨¡å‹è®­ç»ƒ</strong>ã€‚</p>
<p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹ç‚¹ç§¯è¿›è¡Œç¼©æ”¾ <strong>ï¼ˆscaling dot productï¼‰</strong>ï¼Œå³é™¤ä»¥ $ \sqrt{k} $:
$$
w&rsquo;_{ij} = \frac{q_i^T k_j}{\sqrt{k}}
$$
ä»è€Œé˜²æ­¢è¾“å…¥å€¼è¿‡å¤§ï¼Œä½¿ softmax å‡½æ•°è¾“å‡ºçš„æ¢¯åº¦ä¿æŒç¨³å®šï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚è¿™æ ·ç¼©æ”¾çš„åŸå› åœ¨äº <strong>é«˜ç»´å‘é‡çš„æ¬§å‡ é‡Œå¾—é•¿åº¦éšç€ç»´åº¦ $ k $ å¢å¤§è€Œå¢å¤§</strong>ï¼Œé€šè¿‡é™¤ä»¥ $\sqrt{k}$ï¼Œå¯ä»¥æ¶ˆé™¤è¿™ç§æ”¾å¤§æ•ˆåº”ï¼Œä½¿ä¸åŒç»´åº¦çš„è®¡ç®—ç»“æœä¿æŒåœ¨åˆç†èŒƒå›´å†…ï¼Œä»è€Œæ›´ç¨³å®šåœ°å­¦ä¹ æ³¨æ„åŠ›æƒé‡ã€‚</p>
<h2 id="multi-head-attention">Multi-head Attention<a hidden class="anchor" aria-hidden="true" href="#multi-head-attention">#</a></h2>
<p>åœ¨Self-Attentionæœºåˆ¶ä¸­ï¼Œæ¯ä¸ªå•è¯éƒ½å¯ä»¥ä¸å¥å­ä¸­çš„å…¶ä»–å•è¯å»ºç«‹å…³ç³»ã€‚ç„¶è€Œï¼Œåœ¨æ ‡å‡†çš„å•å¤´æ³¨æ„åŠ›ï¼ˆSingle-Head Attentionï¼‰æœºåˆ¶ä¸‹ï¼Œä¸€ä¸ªå•è¯åªèƒ½ç”¨ä¸€ä¸ªæ³¨æ„åŠ›åˆ†å¸ƒæ¥è¡¨ç¤ºå®ƒä¸æ‰€æœ‰å…¶ä»–å•è¯çš„å…³ç³»ã€‚è¿™å°±å¯¼è‡´äº†ä¸€ä¸ªé—®é¢˜ï¼šå•ä¸ªè¯å¯èƒ½åœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­æ‰¿æ‹…ä¸åŒçš„è¯­ä¹‰è§’è‰²ï¼Œä½†å•å¤´æ³¨æ„åŠ›æ— æ³•åŒºåˆ†è¿™äº›è§’è‰²ã€‚ä¾‹å¦‚Mary gave roses to susan. åœ¨ single-head attention æœºåˆ¶ä¸­ï¼Œæ‰€æœ‰è¿™äº›ä¿¡æ¯éƒ½ä¼šè¢«ç®€å•åœ°åŠ æƒæ±‚å’Œï¼Œè€Œä¸ä¼šåˆ†åˆ«å¤„ç†ä¸åŒçš„è¯­ä¹‰å…³ç³»ã€‚è¿™æ„å‘³ç€ï¼Œ&ldquo;Mary&rdquo; å’Œ &ldquo;Susan&rdquo; éƒ½ä¼šå½±å“ &ldquo;gave&rdquo; çš„è¡¨ç¤ºï¼Œä½†å®ƒä»¬çš„å½±å“æ–¹å¼æ˜¯ç›¸åŒçš„ï¼Œæ— æ³•åŒºåˆ†&quot;æ–½äº‹è€…&quot; å’Œ &ldquo;å—ç›Šè€…&rdquo; çš„ä¸åŒè¯­ä¹‰ã€‚</p>
<p>å¤šå¤´æ³¨æ„åŠ›å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ
å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰ é€šè¿‡ä½¿ç”¨å¤šä¸ªç‹¬ç«‹çš„æ³¨æ„åŠ›å¤´ï¼ˆattention headsï¼‰ æ¥æä¾›æ›´é«˜çš„è¡¨è¾¾èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´,æ¯ä¸ªæ³¨æ„åŠ›å¤´ï¼ˆheadï¼‰å…·æœ‰è‡ªå·±ç‹¬ç«‹çš„æŸ¥è¯¢ã€é”®å’Œå€¼å˜æ¢çŸ©é˜µï¼š$W_q^r, W_k^r, W_v^r$ï¼Œå…¶ä¸­$r$è¡¨ç¤ºä¸åŒçš„æ³¨æ„åŠ›å¤´ã€‚å¯¹äºè¾“å…¥$x_i$ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªè¾“å‡ºå‘é‡$y_i^r$ï¼ŒæŠŠè¿™äº›å‘é‡æ‹¼æ¥èµ·æ¥ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢æŠŠç»´åº¦é™åˆ°kã€‚</p>
<h2 id="ä»€ä¹ˆæ˜¯transformer">ä»€ä¹ˆæ˜¯Transformer?<a hidden class="anchor" aria-hidden="true" href="#ä»€ä¹ˆæ˜¯transformer">#</a></h2>
<blockquote>
<p>Any architecture designed to process a connected set of unitsâ€”such as the tokens in a sequence or the pixels in an imageâ€”where the only interaction between units is through self-attention.</p>
</blockquote>
<p>ä¸‹é¢æ˜¯ä¸€ä¸ªæ¯”è¾ƒåŸºç¡€çš„æ¶æ„
<img loading="lazy" src="/img/nluplus/transformer_block.png" alt="Transoformer"  />
</p>
<h2 id="bert">BERT<a hidden class="anchor" aria-hidden="true" href="#bert">#</a></h2>
<blockquote>
<p>BERT - Bidirectional Embedding Representation Transformer</p>
</blockquote>
<p>BERTä¸GPTæ¶æ„çš„åŒºåˆ«</p>
<h1 id="parsing">Parsing<a hidden class="anchor" aria-hidden="true" href="#parsing">#</a></h1>
<h2 id="decoding-with-llms">Decoding with LLMs<a hidden class="anchor" aria-hidden="true" href="#decoding-with-llms">#</a></h2>
<blockquote>
<p>One naturally wonders if the problem of translation could conceivably be treated as a problem in cryptography. When I look at an article in Russian, I say: â€™This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.</p>
</blockquote>
<p><strong>Terminology: continuation - è¡¥å…¨</strong></p>
<p>æˆ‘ä»¬å¦‚ä½•ç”Ÿæˆthe most probable continuation?
$$
\arg\max_{w_1, &hellip;, w_n} p(w_1, &hellip;, w_n | \text{prefix})
$$
å¦‚ä½•æ±‚è§£ï¼Ÿ</p>
<ul>
<li><strong>éå†æ‰€æœ‰å¯èƒ½çš„ç»­å†™</strong>ï¼Œè®¡ç®—å®ƒä»¬çš„æ¦‚ç‡ï¼Œç„¶åé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªã€‚  ä½†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œå¯èƒ½çš„åºåˆ—æ•°é‡æ˜¯<strong>æŒ‡æ•°çº§å¢é•¿</strong>çš„ï¼Œæ‰€ä»¥è¿™ç§æ–¹æ³•é€šå¸¸<strong>ä¸å¯è¡Œ</strong>ã€‚</li>
<li>å¦ä¸€ç§ç­–ç•¥æ˜¯<strong>é€æ­¥ç”Ÿæˆ</strong>ï¼Œå³æ¯ä¸€æ­¥é€‰æ‹©<strong>å½“å‰æœ€ä¼˜</strong>çš„ä¸‹ä¸€ä¸ªå•è¯ï¼ˆæˆ– top-k é€‰é¡¹ï¼‰ã€‚è¿™ç§æ–¹æ³•æ›´å®ç”¨ï¼Œæ¯”å¦‚<strong>è´ªå¿ƒè§£ç ï¼ˆgreedy decodingï¼‰</strong></li>
</ul>
<h3 id="greedy-decoding">Greedy Decoding<a hidden class="anchor" aria-hidden="true" href="#greedy-decoding">#</a></h3>
<p>ä¸‹é¢æ˜¯greedy decodingçš„å…·ä½“æµç¨‹
<img loading="lazy" src="/img/nluplus/greedy_decoding.png" alt="beam search"  />

è¿™ç§è§£ç æ–¹å¼çš„é—®é¢˜ï¼š</p>
<ul>
<li>æˆ‘ä»¬åªåšä¸€æ¬¡continuationï¼Œä¸€æ—¦å‘ç”Ÿé”™è¯¯æ— æ³•å›æº¯æ›´æ”¹ã€‚</li>
</ul>
<h3 id="beam-searching-decoding">Beam Searching Decoding<a hidden class="anchor" aria-hidden="true" href="#beam-searching-decoding">#</a></h3>
<p>æ ¹æ®greedy decodingçš„æ•™è®­ï¼Œæˆ‘ä»¬é€‰æ‹©ä¿ç•™å‡ ä¸ªcontinuationï¼Œå¹¶ä¸”æ¯æ¬¡éƒ½æ‰©å±•most promisingçš„é‚£ä¸ªã€‚æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤º
<img loading="lazy" src="/img/nluplus/beam_search_smooth.gif" alt="beam search"  />

Beam Searchingå¦‚ä½•å®ç°ï¼Ÿ</p>
<ul>
<li>ç”¨ä¼˜å…ˆé˜Ÿåˆ—è¿™æ ·çš„æ•°æ®ç»“æ„æ¥å­˜å‚¨æ¯ä¸ªcontinuation</li>
<li>è¿™ä¸ªé˜Ÿåˆ—ä¸­ä¿æŒæœ€å¤škä¸ªè§£æ</li>
<li>å½“è¦è§£ç ä¸€ä¸ªå•è¯æ—¶ï¼Œä»é˜Ÿåˆ—ä¸­popå‡ºä¸€ä¸ªè§£æï¼Œæ‰©å±•å®Œå†pushå›é˜Ÿåˆ—</li>
<li>ç”¨priority queueè¿™ç§ç»“æ„æ¥åšï¼Œæ—¶é—´å¤æ‚åº¦ä¸ä¼šè¶…è¿‡O(logk)ï¼Œç”šè‡³èƒ½è¾¾åˆ°O(1)</li>
</ul>
<p>é˜Ÿåˆ—è§„æ¨¡kçš„å½±å“</p>
<ul>
<li>å½“k=1ï¼Œå°±æ˜¯greedy decodingã€‚æ‰€ä»¥å½“kè¾ƒå°æ—¶ï¼Œæˆ‘ä»¬ä¼šå‡ºç°ç±»ä¼¼çš„é—®é¢˜</li>
<li>å½“kå¾ˆå¤§æ—¶ï¼Œä¸€æ–¹é¢æ˜¯è®¡ç®—æ˜‚è´µï¼Œä¸€æ–¹é¢æ˜¯æœ‰å·¥ä½œè¡¨æ˜è¾ƒå¤§çš„kå¾€å¾€å¯¼è‡´worse translation <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>ã€‚</li>
<li>è€Œä¸”å½“kå¾ˆå¤§æ—¶ï¼Œå¾€å¾€ç”Ÿæˆçš„continationéƒ½å¾ˆçŸ­ï¼Œå› ä¸ºè¾ƒçŸ­çš„continuationä¸€èˆ¬éƒ½æ˜¯ä¼˜é€‰</li>
<li>å› æ­¤ï¼Œé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„kéå¸¸é‡è¦</li>
</ul>
<p>è§£å†³æ–¹æ³•ï¼š</p>
<h3 id="sampling">Sampling<a hidden class="anchor" aria-hidden="true" href="#sampling">#</a></h3>
<p>ä¸å…¶æ¯æ¬¡é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ï¼Œä¸å¦‚ä»è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ã€‚ä¾‹å¦‚Top-k Samplingå’ŒNucleus sampling</p>
<p>Holtzman et al. å¯¹æ¯”äº†å‡ ç§æ–¹æ³•çš„æ•ˆæœï¼Œå¦‚å›¾æ‰€ç¤º<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>
<img loading="lazy" src="/img/nluplus/neuclues-sampling.png" alt="sampling"  />
</p>
<p>é™¤æ­¤ä»¥å¤–ï¼Œè¿˜æœ‰Locally Typical Sampling
<strong>Top-k Sampling vs. Nucleus Sampling vs. Locally Typical Sampling</strong>
è¿™äº›éƒ½æ˜¯<strong>æ–‡æœ¬ç”Ÿæˆä»»åŠ¡</strong>ä¸­ç”¨äº<strong>è§£ç ï¼ˆdecodingï¼‰<strong>çš„ç­–ç•¥ï¼Œæ—¨åœ¨</strong>å¹³è¡¡å¤šæ ·æ€§å’Œåˆç†æ€§</strong>ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTã€LLMï¼‰ä¸­åº”ç”¨ã€‚<br>
å®ƒä»¬çš„æ ¸å¿ƒç›®æ ‡æ˜¯<strong>é¿å…å•ä¸€ç¡®å®šæ€§è¾“å‡ºï¼ˆå¦‚ Greedy Searchï¼‰</strong>ï¼Œè®©ç”Ÿæˆæ–‡æœ¬æ›´åŠ è‡ªç„¶å’Œå¯Œæœ‰å˜åŒ–ã€‚</p>
<p><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼š</p>
<ul>
<li>é€‰æ‹©é‚£äº›ä½¿å¾—<strong>æ¨¡å‹ç”Ÿæˆçš„ä¸‹ä¸€ä¸ªå•è¯åœ¨ä¸Šä¸‹æ–‡ä¸­æœ€ç¬¦åˆâ€œå±€éƒ¨å…¸å‹æ€§â€çš„å•è¯</strong>ã€‚</li>
<li>ä¸åªæ˜¯çœ‹å•è¯æ¦‚ç‡æœ¬èº«ï¼Œè€Œæ˜¯çœ‹å®ƒæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­â€œå¬èµ·æ¥â€åˆç†ï¼ˆå±€éƒ¨ç†µæœ€å°ï¼‰ã€‚</li>
</ul>
<p><strong>æµç¨‹</strong>ï¼š</p>
<ol>
<li>è®¡ç®—æ‰€æœ‰å•è¯çš„æ¦‚ç‡ $ p(w \mid \text{context}) $ã€‚</li>
<li>è®¡ç®—è¿™äº›å•è¯çš„ <strong>ä¿¡æ¯ç†µï¼ˆself-informationï¼‰</strong>ï¼š
$$
s(w) = -\log p(w \mid \text{context})
$$</li>
<li>é€‰æ‹©<strong>ä¿¡æ¯ç†µæœ€æ¥è¿‘å¹³å‡ä¿¡æ¯ç†µï¼ˆå…¸å‹ç†µï¼‰</strong> çš„ä¸€éƒ¨åˆ†å•è¯ã€‚</li>
<li>åœ¨è¿™ä¸ªå­é›†ä¸­è¿›è¡Œé‡‡æ ·ã€‚</li>
</ol>
<p><strong>ä¼˜ç‚¹</strong>ï¼š</p>
<ul>
<li>é¿å…äº† Top-k å’Œ Nucleus Sampling çš„å±€é™ï¼Œä½¿å¾—è¾“å‡ºæ›´åŠ <strong>ç¬¦åˆä¸Šä¸‹æ–‡</strong>ï¼Œè€Œä¸æ˜¯ä»…ä»…åŸºäºå…¨å±€æ¦‚ç‡é«˜ä½ã€‚</li>
<li>é€‚ç”¨äº<strong>å¯¹å±€éƒ¨è¯­å¢ƒè¦æ±‚é«˜çš„ä»»åŠ¡</strong>ï¼Œæ¯”å¦‚å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç­‰ã€‚</li>
</ul>
<p><strong>ç¼ºç‚¹</strong>ï¼š</p>
<ul>
<li>è®¡ç®—å¤æ‚åº¦ç•¥é«˜ï¼Œéœ€è¦é¢å¤–è®¡ç®—ä¿¡æ¯ç†µã€‚</li>
</ul>
<blockquote>
<p>é‡‡æ ·çš„ç›®çš„å°±æ˜¯å¹³è¡¡å¤šæ ·æ€§å’Œåˆç†æ€§ã€‚é¿å…è¯­è¨€æ¨¡å‹å•ä¸€ç¡®å®šæ€§çš„è¾“å‡ºã€‚</p>
</blockquote>
<h2 id="neural-parsing">Neural Parsing<a hidden class="anchor" aria-hidden="true" href="#neural-parsing">#</a></h2>
<p>å…ˆå¤ä¹ ä¸‹Encoder-Decoderæ¶æ„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒEncoderä½œç”¨æ˜¯å°†è¾“å…¥åºåˆ—ï¼ˆå¦‚å¾·è¯­å¥å­ï¼‰ç¼–ç æˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡å‘é‡ï¼ˆcontext vectorï¼‰ã€‚</p>
<ul>
<li>è¾“å…¥è¯ï¼ˆå¦‚ &ldquo;natÃ¼rlich&rdquo;, &ldquo;hat&rdquo;, &ldquo;john&rdquo;, &ldquo;spaÃŸ&rdquo;ï¼‰é¦–å…ˆè¢«æ˜ å°„æˆè¯å‘é‡ï¼ˆ$ x_1, x_2, x_3, x_4 $ï¼‰ã€‚</li>
<li>è¿™äº›è¯å‘é‡ä¾æ¬¡è¾“å…¥åˆ°ä¸€ä¸ªRNN ç¼–ç å™¨ï¼ˆé€šå¸¸æ˜¯ LSTM æˆ– GRUï¼‰ï¼Œäº§ç”Ÿéšè—çŠ¶æ€ $ h_1, h_2, h_3, h_4 $ã€‚</li>
<li>è¿™äº›éšè—çŠ¶æ€æ•æ‰äº†å¥å­çš„ä¿¡æ¯ï¼Œæœ€ç»ˆæœ€åä¸€ä¸ªéšè—çŠ¶æ€ï¼ˆè¿™é‡Œæ˜¯ $ h_4 $ï¼‰è¢«ç”¨ä½œæ•´ä¸ªè¾“å…¥å¥å­çš„ä¸Šä¸‹æ–‡å‘é‡ï¼ˆcontext vectorï¼‰ã€‚</li>
</ul>
<p>Decoderæ¥æ”¶ç¼–ç å™¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç„¶åé€æ­¥ç”Ÿæˆç›®æ ‡è¯­è¨€çš„å¥å­ï¼ˆå¦‚è‹±æ–‡ï¼‰ã€‚</p>
<ul>
<li>ç¼–ç å™¨æœ€åçš„éšè—çŠ¶æ€è¢«ä¼ å…¥è§£ç å™¨çš„ RNN ä½œä¸ºåˆå§‹çŠ¶æ€ã€‚</li>
<li>è§£ç å™¨ä½¿ç”¨ RNNï¼ˆLSTM/GRUï¼‰é€æ­¥ç”Ÿæˆç›®æ ‡è¯­è¨€çš„å•è¯ï¼š
<ul>
<li>åˆå§‹çŠ¶æ€ $ s_1 $ ä¾èµ–äºç¼–ç å™¨çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚</li>
<li>æ¯ä¸ªæ—¶é—´æ­¥ï¼Œè§£ç å™¨çš„ RNN ç”Ÿæˆä¸€ä¸ªæ–°çš„éšè—çŠ¶æ€ $ s_t $ã€‚</li>
<li>é€šè¿‡ä¸€ä¸ª <strong>softmax å±‚</strong>ï¼Œ$ s_t $ è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºè¯ $ y_t $ï¼ˆå¦‚ &ldquo;of&rdquo;, &ldquo;course&rdquo;, &ldquo;john&rdquo;, &ldquo;has&rdquo;, &ldquo;fun&rdquo;ï¼‰ã€‚</li>
<li>è§£ç å™¨çš„è¾“å‡ºä½œä¸ºè¾“å…¥ä¼ é€’åˆ°ä¸‹ä¸€æ­¥ï¼Œç›´åˆ°ç”Ÿæˆå®Œæ•´å¥å­ã€‚</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="/img/nluplus/encoder_decoder.png" alt="Encoder Decoder"  />
</p>
<blockquote>
<p>Parsing is the task of turning a sequence of words</p>
</blockquote>
<p>é—®é¢˜æ¥äº†ï¼Œå¥å­è§£æçš„è¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œä½†è¾“å‡ºä¸æ˜¯ï¼Œé‚£æˆ‘ä»¬å¦‚ä½•ç”¨encoder-decoderæ¨¡å‹æ¥è¿›è¡Œparsingï¼Ÿ</p>
<p>æˆ‘ä»¬å¯ä»¥linearize the syntax tree. è¿™æ ·æˆ‘ä»¬å°±æœ‰ä¸€ä¸ªåºåˆ—æ¥è¡¨ç¤ºè¾“å‡ºäº†ã€‚ä¾‹å¦‚ï¼šI saw a man with a telescopeçš„å¥å­è§£æå°±æ˜¯
<code>(S (NP (Pro You ) ) (VP (V saw ) (NP (Det a ) (N man ) (PP (P with ) (Det a ) (N telescope ) ) ) ) )</code></p>
<p>æ­¤å¤–ï¼Œè¿˜éœ€è¦ä¸€äº›ä¼˜åŒ–</p>
<ul>
<li>æ·»åŠ EOSã€‚ä½¿è§£ç å™¨çŸ¥é“ä½•æ—¶åœæ­¢ç”Ÿæˆï¼Œè€Œä¸ä¼šä¸€ç›´æ— é™åˆ¶åœ°é¢„æµ‹å•è¯ã€‚</li>
<li>åè½¬è¾“å…¥å­—ç¬¦ä¸²å¯ä»¥å¸¦æ¥å°å¹…çš„æ€§èƒ½æå‡ã€‚</li>
<li>åŠ æ·±ç½‘ç»œå±‚æ•°ã€‚ä¾‹å¦‚å¯¹encoderå’Œdecoderéƒ½ä½¿ç”¨ä¸‰å±‚LSTM<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>ã€‚</li>
<li>æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶</li>
<li>ä½¿ç”¨word2vecä½œä¸ºè¾“å…¥ï¼ˆé¢„è®­ç»ƒçš„è¯åµŒå…¥ï¼‰</li>
<li>è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œè€Œä¸éœ€è¦äººå·¥æ ‡æ³¨ã€‚Vinyals et al. (2015) <sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> é‡‡ç”¨ Berkeley Parserï¼ˆä¸€ä¸ªå·²æœ‰çš„å¥æ³•è§£æå™¨ï¼‰æ¥è§£æå¤§é‡æ–‡æœ¬ï¼Œå¹¶ç”¨è¿™äº›è§£æç»“æœä½œä¸ºè®­ç»ƒæ•°æ®ã€‚</li>
</ul>
<p>å¯èƒ½å‘ç”Ÿçš„é—®é¢˜ï¼ˆæ¯”ä¾‹å¾ˆå°ï¼Œæ— éœ€æ‹…å¿ƒï¼‰ï¼šæ¯”å¦‚æˆ‘ä»¬å¦‚ä½•ç¡®å®šopening and closing brackets match? å¦‚ä½•å¯¹åº”è¾“å…¥å’Œè¾“å‡ºï¼Ÿå¦‚ä½•ç¡®ä¿æ¨¡å‹è¾“å‡ºçš„æ˜¯æ•´ä¸ªåºåˆ—çš„æœ€ä¼˜parsingè€Œä¸æ˜¯ä»…ä»…é¢„æµ‹æ¯ä¸ªtime stepä¸Šçš„best symbol?</p>
<p><strong>Parsing with Transformers</strong>
Kitaevç­‰äººç”¨transformersè¿›è¡Œparsingã€‚</p>
<ul>
<li>Use a transformer to encode the input</li>
<li>This results in a â€œcontext aware summary vectorâ€ (embedding) for each input word.</li>
<li>The embedding encodes word, PoS tag, and position information.</li>
<li>The embedding layers are combined to obtain <strong>span scores</strong></li>
<li>But they also try <strong>factored attention heads</strong>, which separate position and content information.</li>
</ul>
<p>ä»€ä¹ˆæ˜¯span scores? åœ¨NLPä¸­ï¼Œå°¤å…¶æ˜¯å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€å¥æ³•è§£æï¼ˆParsingï¼‰ å’Œ ä¿¡æ¯æŠ½å–ï¼ˆIEï¼‰ ç­‰ä»»åŠ¡ä¸­ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚
Span scores é€šå¸¸åŸºäº Precisionï¼ˆç²¾ç¡®ç‡ï¼‰ã€Recallï¼ˆå¬å›ç‡ï¼‰ å’Œ F1-scoreï¼ˆF1 åˆ†æ•°ï¼‰ æ¥è¯„ä¼°æ¨¡å‹åœ¨ span-level çš„è¡¨ç°ï¼š</p>
<p>Span scoresåœ¨Transformerè§£æä»»åŠ¡ä¸­çš„è®¡ç®—æ–¹æ³•ï¼šè®¡ç®—spanç›¸å…³çš„å‘é‡å·®å€¼ï¼Œç»è¿‡çº¿æ€§å˜æ¢ã€å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰ã€éçº¿æ€§æ¿€æ´»ï¼ˆReLUï¼‰å’Œå†æ¬¡å˜æ¢ï¼Œå¾—åˆ° span scoresã€‚æœ€ç»ˆï¼Œè¿™äº›å¾—åˆ†ç”¨äº è§£ç ï¼ˆdecode the outputï¼‰ï¼Œå³æ‰¾åˆ°æœ€ä¼˜çš„è§£ææ ‘ã€‚</p>
<h2 id="unsupervised-parsing">Unsupervised Parsing<a hidden class="anchor" aria-hidden="true" href="#unsupervised-parsing">#</a></h2>
<p>Unsupervised Parsing<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>ä¸»è¦æ˜¯ä»æœªæ ‡è®°çš„æ–‡æœ¬ä¸­å½’çº³å‡ºè¯­æ³•ç»“æ„ã€‚</p>
<p>Unsupervised Parsingçš„SOTAçš„F-scoreåªæœ‰60ï¼Œå¯¹æ¯”supervised parsingæ¥è¯´å¾ˆéš¾ï¼Œsupervised parsingçš„F-scoreå¯ä»¥è¾¾åˆ°95ä»¥ä¸Š</p>
<h1 id="llmå‰æ²¿ç ”ç©¶">LLMå‰æ²¿ç ”ç©¶<a hidden class="anchor" aria-hidden="true" href="#llmå‰æ²¿ç ”ç©¶">#</a></h1>
<h2 id="scaling-law">Scaling Law<a hidden class="anchor" aria-hidden="true" href="#scaling-law">#</a></h2>
<p>ä¾‹å¦‚ï¼Œç»™å®šå›ºå®šçš„è®¡ç®—èµ„æºé¢„ç®—ï¼Œè®­ç»ƒTransformerè¯­è¨€æ¨¡å‹çš„æœ€ä½³æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ•°æ®é›†å¤§å°æ˜¯å¤šå°‘ï¼Ÿ</p>
<p>è¿™ç±»é—®é¢˜å°±å¯ä»¥ç”¨Scaling lawsæ¥å›ç­”ã€‚å®ƒæä¾›äº†Compute Budget(C)ï¼Œsize of model(N)ä»¥åŠnumber of training tokens(D)ä¹‹é—´çš„ä¸€ç§å…³ç³»ã€‚æœ‰ä¸€ä¸ªç²—ç•¥çš„å…±è¯†æ˜¯ï¼Œå®ƒä»¬å¯ä»¥ä½¿ç”¨power lawsæˆ–ç±»ä¼¼çš„å®šå¾‹ï¼ˆä»¥åŠå®ƒä»¬çš„ç»„åˆï¼‰æ¥å»ºæ¨¡ã€‚</p>
<h3 id="power-law">Power Law<a hidden class="anchor" aria-hidden="true" href="#power-law">#</a></h3>
<p>Power Lawæè¿°çš„æ˜¯å˜é‡xå’Œå®ƒæŸäº›è¡Œä¸ºä¹‹é—´çš„å…³ç³»ï¼Œå…¬å¼è¡¨ç¤ºä¸º$f(x) = \alpha x^\mathcal{-K}$</p>
<p>å®ƒæœ‰ä¸€ä¸ªé‡è¦çš„æ€§è´¨ï¼šå½“$x$ä¹˜ä»¥ä¸€ä¸ªå› å­æ—¶ï¼Œ$f(x)$ä¹ŸåŒæ ·ä½œä¸º$Îº$çš„å‡½æ•°ä¹˜ä»¥ä¸€ä¸ªå› å­ã€‚å³ï¼š
$$
f(cx) = \alpha (cx)^\mathcal{-K} = c^\mathcal{-K}f(x)
$$</p>
<h3 id="kaplan-et-al-2020">Kaplan et al. (2020)<a hidden class="anchor" aria-hidden="true" href="#kaplan-et-al-2020">#</a></h3>
<p>Kaplanç­‰äººçš„ç ”ç©¶<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>æ€»ç»“</p>
<ul>
<li>æ¨¡å‹çš„æ€§èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºscaleï¼ˆæ¨¡å‹å‚æ•°çš„æ•°é‡ï¼Œæ•°æ®é›†å¤§å°ï¼Œè®¡ç®—é¢„ç®—ï¼‰è€Œä¸æ˜¯ç½‘ç»œæ‹“æ‰‘ï¼ˆå±‚æ•°ï¼Œæ¯å±‚çš„å®½åº¦ï¼‰ã€‚</li>
<li>æ•°æ®é›†å¤§å°å’Œæ¨¡å‹å¤§å°çš„ä¸€èµ·å¢é•¿æ˜¯å¾ˆé‡è¦çš„ã€‚</li>
<li>æ€§èƒ½å¯ä»¥ç”¨power lawæ¥å»ºæ¨¡ã€‚</li>
</ul>
<p>è¿˜æœ‰ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨floating-point operations per second (FLOPs) æ¥è¡¡é‡computeï¼Œè€Œä¸æ˜¯å•çº¯ä¾èµ–æ—¶é—´æ¥è¡¡é‡ã€‚</p>
<p>å¯¹ä»–ä»¬çš„å·¥ä½œï¼ˆKaplan Scaling Lawï¼‰çš„ä¸»è¦æ‰¹åˆ¤ç‚¹åœ¨äºï¼Œä»–ä»¬åœ¨è®­ç»ƒä¸­ä½¿ç”¨äº†ç›¸åŒçš„å­¦ä¹ ç‡ã€‚</p>
<h3 id="hoffmann-et-al-2020">Hoffmann et al. (2020)<a hidden class="anchor" aria-hidden="true" href="#hoffmann-et-al-2020">#</a></h3>
<p>ä»¥å‰çš„scaling lawè®¤ä¸ºï¼Œå¢åŠ è®¡ç®—é¢„ç®—ï¼ˆcompute budgetï¼‰æ—¶ï¼Œä¼˜å…ˆå¢åŠ æ¨¡å‹å¤§å°ï¼ˆmodel sizeï¼‰ï¼Œå…¶æ¬¡æ‰å¢åŠ æ•°æ®é‡ï¼ˆdata sizeï¼‰ã€‚
æ¯”å¦‚è®¡ç®—é¢„ç®— Ã—100 æ—¶ï¼Œæ¨¡å‹å‚æ•°é‡ Ã—25ï¼Œæ•°æ®é‡ Ã—4ã€‚å› ä¸ºç›´è§‰ä¸Šæ›´å¤§çš„æ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®ï¼Œæ‰€ä»¥æ•°æ®è§„æ¨¡çš„å¢é•¿ä¸éœ€è¦å¤ªå¿«ã€‚
ä½†æ˜¯ï¼Œè¿‡åº¦å¢åŠ æ¨¡å‹å¤§å°å¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆæˆ–è®¡ç®—èµ„æºæµªè´¹ã€‚</p>
<p>Hoffmannç­‰äºº2022å¹´çš„ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„ç»“è®º<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>ï¼š</p>
<p>è®¡ç®—é¢„ç®—å¢åŠ æ—¶ï¼Œæ¨¡å‹å¤§å°å’Œæ•°æ®é‡åº”è¯¥åŒæ¯”ä¾‹å¢é•¿ã€‚
æ¯”å¦‚è®¡ç®—é¢„ç®— Ã—100 æ—¶ï¼Œæ¨¡å‹å¤§å° Ã—10ï¼Œæ•°æ®é‡ Ã—10ã€‚å› ä¸ºä¹‹å‰çš„ç­–ç•¥å¯¼è‡´å¾ˆå¤šå¤§æ¨¡å‹åœ¨å°æ•°æ®ä¸Šè®­ç»ƒï¼Œå‡ºç°æ¬ è®­ç»ƒï¼ˆundertrained modelsï¼‰ çš„é—®é¢˜ã€‚
Hoffmann å‘ç°ï¼Œæ›´å¤§æ¨¡å‹å¹¶ä¸ä¸€å®šèƒ½å®Œå…¨åˆ©ç”¨å›ºå®šæ•°é‡çš„æ•°æ®ï¼Œåˆç†å¢åŠ æ•°æ®é‡å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚è¿™ä¸ªå‘ç°ä¹Ÿå½±å“äº†åç»­ LLM è®­ç»ƒç­–ç•¥ï¼Œä¾‹å¦‚ GPT-4ã€Geminiã€PaLM ç­‰æ›´å…³æ³¨ æ•°æ®ä¸æ¨¡å‹è§„æ¨¡çš„å‡è¡¡å¢é•¿ã€‚</p>
<p>å¦‚ä½•æ¨å¯¼Hoffmannç­‰äººæå‡ºçš„Chinchilla&rsquo;s Scaling Law?</p>
<ul>
<li>L: LM average test loss (entropy loss)</li>
<li>D: dataset size, number of tokens</li>
<li>N: number of parameters</li>
<li>C: compute budget, C=C(N, D)
å·²çŸ¥ä¸€ä¸ªå›ºå®šçš„compute budget C*, æ‰¾åˆ°
$$
\arg\min_{N, D}L(N, D) = C^*
$$
ç”¨power lawæ¥å»ºæ¨¡ï¼š
$$
L(N, D) = \frac{a}{N^\alpha} + \frac{b}{D^\beta} + c
$$
$c$æ˜¯ç†æƒ³çš„test loss</li>
</ul>
<p>å› æ­¤ï¼ŒHoffmannç­‰äººè®­ç»ƒå‡ºæ¥äº†ä¸¤ä¸ªæ¨¡å‹ï¼š</p>
<ul>
<li>Gopher: 280 billion parameters, 300 billion tokens, L(N, D) = 1.993</li>
<li>Chinchilla: 70 billion parameters, 1.4 trillion tokens, L(N, D) = 1.936</li>
</ul>
<p>Chinchilla éµå¾ªè¿™ä¸€ç­–ç•¥ï¼ˆè¾ƒå°æ¨¡å‹ + æ›´å¤šæ•°æ®ï¼‰ï¼Œç»“æœè¯æ˜å®ƒåœ¨test losså’Œä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚</p>
<h3 id="å¢åŠ æ•ˆç‡">å¢åŠ æ•ˆç‡<a hidden class="anchor" aria-hidden="true" href="#å¢åŠ æ•ˆç‡">#</a></h3>
<p>è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œä¸»è¦åšçš„è®¡ç®—å°±æ˜¯çŸ©é˜µä¹˜æ³•å’Œæ±‚å’Œå½’ä¸€åŒ–ï¼ˆSoftmaxï¼‰ã€‚ç”¨ä¸€ä¸ªå…¬å¼æ¦‚æ‹¬Transformeråšçš„äº‹æƒ…å°±æ˜¯$\text{Softmax}(QK^T)V$</p>
<p>GPUæ˜¯å¹¶è¡Œå¤„ç†å™¨ï¼Œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º
<img loading="lazy" src="/img/nluplus/gpu_hardware.png" alt="gpu hardware"  />
</p>
<ul>
<li>åŸºç¡€çš„è®¡ç®—å•å…ƒæ˜¯çº¿ç¨‹</li>
<li>çº¿ç¨‹è¢«åˆ†ç»„æˆBlockï¼ŒBlockä¸­çš„æ‰€æœ‰çº¿ç¨‹éƒ½æœ‰å”¯ä¸€çš„IDï¼Œä½†è¿è¡Œç›¸åŒçš„ä»£ç </li>
<li>è¿™ä½¿å¾—GPUå¯ä»¥åšæç«¯çš„å¹¶è¡ŒåŒ–</li>
<li>Gridæ˜¯ä¸€ç»„Blockã€‚æ¯ä¸ªBlockå¯ä»¥è¿è¡Œä¸åŒçš„ä»£ç æ®µ</li>
<li>GPUæœ‰å¾ˆå¤§çš„global memoryï¼Œå¾ˆå¤§ï¼Œä½†æ˜¯å¾ˆæ…¢</li>
<li>æ¯ä¸ªBlockéƒ½æœ‰ä¸€ä¸ªå…±äº«å†…å­˜-Blockä¸­çš„æ‰€æœ‰çº¿ç¨‹ä¹‹é—´å…±äº«è¿™ä¸ªå†…å­˜ï¼Œå®ƒæ•ˆç‡å¾ˆé«˜ï¼Œä½†å¾ˆå°</li>
<li>å†™ä»£ç æ—¶è¦å°½å¯èƒ½å°‘ç”¨å…¨å±€å†…å­˜ï¼Œå¤šç”¨Blockçš„å…±äº«å†…å­˜</li>
</ul>
<h3 id="double-descent">Double Descent<a hidden class="anchor" aria-hidden="true" href="#double-descent">#</a></h3>
<p>Double Descentæ˜¯æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°çš„ä¸€ç§ç°è±¡ï¼Œå®ƒæè¿°äº†æ¨¡å‹çš„test erroréšç€model sizeæˆ–dataset sizeå˜åŒ–çš„éå•è°ƒè¶‹åŠ¿ã€‚
Double Descent ç°è±¡è¡¨æ˜ï¼Œåœ¨â€œè¿‡æ‹ŸåˆåŒºåŸŸâ€ä¹‹åï¼Œç»§ç»­å¢åŠ æ¨¡å‹è§„æ¨¡ï¼Œtest erroråè€Œä¼šå†æ¬¡ä¸‹é™ï¼Œè¿›å…¥â€œç¬¬äºŒæ¬¡æ³›åŒ–åŒºåŸŸâ€ã€‚</p>
<p>å› æ­¤ï¼Œä¼ ç»Ÿç»Ÿè®¡å­¦ä¹ ç†è®ºé¢„æµ‹è¿‡æ‹Ÿåˆä¼šå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸‹é™ï¼Œä½†Double Descentè¯´æ˜åœ¨è¶…å¤§æ¨¡å‹ä¸‹ï¼Œæ³›åŒ–èƒ½åŠ›åè€Œä¼šå›å‡ã€‚</p>
<h2 id="llms-as-formal-machines">LLMs as Formal Machines<a hidden class="anchor" aria-hidden="true" href="#llms-as-formal-machines">#</a></h2>
<h1 id="llmå¾®è°ƒ">LLMå¾®è°ƒ<a hidden class="anchor" aria-hidden="true" href="#llmå¾®è°ƒ">#</a></h1>
<h1 id="evaluating-generation-and-machine-translation">Evaluating Generation and Machine Translation<a hidden class="anchor" aria-hidden="true" href="#evaluating-generation-and-machine-translation">#</a></h1>
<h1 id="ethics">Ethics<a hidden class="anchor" aria-hidden="true" href="#ethics">#</a></h1>
<h1 id="evaluation-of-llm-and-alignment">Evaluation of LLM and Alignment<a hidden class="anchor" aria-hidden="true" href="#evaluation-of-llm-and-alignment">#</a></h1>
<h1 id="qa-and-rag">QA and RAG<a hidden class="anchor" aria-hidden="true" href="#qa-and-rag">#</a></h1>
<h1 id="tutorials">Tutorials<a hidden class="anchor" aria-hidden="true" href="#tutorials">#</a></h1>
<h2 id="language-models">Language Models<a hidden class="anchor" aria-hidden="true" href="#language-models">#</a></h2>
<ol>
<li>Softmax Function</li>
</ol>
<ul>
<li>What is the purpose of Softmax function?
<ul>
<li>The softmax converts an arbitrary vector of |v| dimensions into a valid categorical probability distribution over |v| possible outcomes. In particular it ensures that all individual elements (probabilities) are non-negative and sum to one.</li>
</ul>
</li>
<li>What is the purpose of the expression in the numerator?
<ul>
<li>The numerator ensures that all values are positive. Note that this is stronger than needed: the axioms of probability simply require all values to be non-negative. But exponentiation is only zero in the (negative) limit.</li>
</ul>
</li>
<li>What is the purpose of the expression in the denominator?
<ul>
<li>The denominator normalises the distribution so that all individual probabilities sum to one.</li>
</ul>
</li>
<li>Now consider how a neural language model with a softmax output layer compares with a classic n-gram language model. Typically, we use techniques like smoothing or backoff in conjunction with n-gram models. Does this problem arise in the neural model? Why or why not?
<ul>
<li>Noâ€”softmax ensures that the model will always return a non-zero probability for any n-gram.</li>
</ul>
</li>
</ul>
<ol start="2">
<li>Feedforward Language Models</li>
</ol>
<ul>
<li>In feedforward language model, the probability $P(w_i\mid w_{i-n+1}, &hellip;, w_{i-1}) = \text{softmax}(Vh_2 + b_2)$, where $h_2 = \text{tanh}(Wh_1 + b_1)$, $h_1 = \text{concatenate}(Cw_{i-n+1}, &hellip;, Cw_{i-1})$, $w_i = \text{onehot}(w_i)$ for all i. Now consider the number of parameters required to represent this model. This number is determined by the size of the vocabulary (given to you by the data), the order $n$, and the dimension of the two hidden layers, $h_1$ and $h_2$, which we will denote $d_1$ and $d_2$, respectively (Note that the first dimension must be divisible by n âˆ’ 1, but you can ignore this detail in your calculations). Dimensions $d_1$ and $d_2$ are modeling choices, though the practical consideration is how they impact the modelâ€™s accuracy.
<ul>
<li>How to express the number of model parameters in terms of $|V|, n, d_1$ and $d_2$?
<ul>
<li>For $V$: $d_2|V|$</li>
<li>For $W$: $d_1d_2$ because $d_1$ is predefined, and will be equal to $(n-1) * \text{embedding}$</li>
<li>For $C$: $|V|d_1/(n-1)$ because $|\text{embedding}| = \frac{d_1}{n-1}$</li>
<li>For $b_1$: $d_2$</li>
<li>For the complete model, add up the aboveL $(1+\frac{d_1}{n-1} + d_2){V} + d_1d_2 + d_2$</li>
</ul>
</li>
<li>An effective size for the hidden dimension of a neural NLP model is often in the hundreds. For n from 2 to 5, how many parameters would your model have if $\frac{d_1}{n-1} = d_2 = 100$, what if $\frac{d_1}{n-1} = d_2 = 1000$
<ul>
<li>The key here is that $(1 + \frac{d_1}{n-1} + d_2)|V|$ dominates, and this is determined by the mapping between the one-hot vocabulary vectors and the hidden dimensions. A reasonable guess for |V| in most neural network language models is 20000 and $\frac{d_1}{n-1} = d_2 = 100$ should give parameters of about 4M parameters. If $\frac{d_1}{n-1} = d_2 = 1000$ then you have about 40M parameters. However if we try to model an open vocabulary in this model we will struggle to fit this in memory on a GPU whose memory is generally far smaller than a CPU  machine.</li>
</ul>
</li>
<li>What do you conclude about the relative memory efficiency of classic n-gram and feedforward neural language models? If you increased n even further, what would happen?
<ul>
<li>The number of parameters in the n-gram model is highly sensitive to changes in n, while the number of parameters in the neural model is almost unchanged. Hence, the feedforward model can be easily extended to larger n, which might be advantageous.</li>
</ul>
</li>
<li>How would you expect the number of parameters in an RNN model to scale with $n$ and $|V|$
<ul>
<li>An RNN scales in the same way as the feedforward model: the dominant factor is the vocabulary size. Itâ€™s entirely insensitive to n since it (theoretically) models $n = \infty$</li>
</ul>
</li>
<li>Think of any strategies to substantially reduce the number of parameters?
<ul>
<li>For RNN models, the key to reducing the number of parameters is to reduce vocabulary size. This can be done with subword modeling. Notice that this is inappropriate for the n-gram model, since it would be conditioning on less information! Note that the feedforward model has a similar limitation, though it is easier to increase the order $n$ of the feedforward model.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="3">
<li>Model Design</li>
</ol>
<ul>
<li>Design a feedforward neural network to model $P(y_i\mid x)$. . Identify any independence assumptions you make. Draw a diagram that illustrates how the model computes probabilities for the tag of the word â€œwithâ€: What is the input, and how is the output distribution computed from the input? Write out the basic equations of the model, and explain the choice.
<ul>
<li>An effective design for the feedforward network is to model $P(y_i \mid x_{i-k}, &hellip;, x_{i+k})$ for some fixed window size 2k+1. For example, use something like: $P(y_i \mid x_{i-k}, &hellip;, x_{i+k}) = \text{softmax}(Wh+b_2)$, where $h = \text{tanh}(Vx+b_1)$, and $x$ is onehot encoded like $x = \text{onehot}(x_{i-k});&hellip;;\text{onehot}(x_{i+k})$. The choice of non-linearity is not important for this question, but since it asks for a feedforward network, you should have a hidden layer. This is about the simplest possible model.</li>
</ul>
</li>
<li>Design a RNN to model $P(y_i\mid x)$. . Identify any independence assumptions you make. Draw a diagram that illustrates how the model computes probabilities for the tag of the word â€œwithâ€: What is the input, and how is the output distribution computed from the input? Write out the basic equations of the model, and explain your choices.
<ul>
<li>One design for the RNN is to model $P(y_i\mid x_1, &hellip;, x_i)$. That is, the RNN reads $x_1$ through $x_i$ one step at a time, and at the ith step produces a distribution for possible tags $y_i$. For simplicity, letâ€™s use RNN to denote a unit that receives an input and a previous hidden state, and produces a new hidden state; it can easily be replaced with an LSTM or other recurrent unit of your choice: $P(y_i\mid x_1, &hellip;, x_i) = \text{softmax}(Wh_i+b)$, where $h_i = \text{RNN}(\text{onehot}(x_i), h_{i-1})$.</li>
</ul>
</li>
</ul>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>&lsquo;Transformers from scratch&rsquo; Available: <a href="https://peterbloem.nl/blog/transformers">https://peterbloem.nl/blog/transformers</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Tu, Zhaopeng, et al. &ldquo;Neural machine translation with reconstruction.&rdquo; Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 31. No. 1. 2017.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Koehn, Philipp, and Rebecca Knowles. &ldquo;Six challenges for neural machine translation.&rdquo; arXiv preprint arXiv:1706.03872 (2017).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Holtzman, Ari, et al. &ldquo;The curious case of neural text degeneration.&rdquo; arXiv preprint arXiv:1904.09751 (2019).&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Vinyals, Oriol, et al. &ldquo;Grammar as a foreign language.&rdquo; Advances in neural information processing systems 28 (2015).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Cao, Steven, Nikita Kitaev, and Dan Klein. &ldquo;Unsupervised parsing via constituency tests.&rdquo; arXiv preprint arXiv:2010.03146 (2020).&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Kaplan, Jared, et al. &ldquo;Scaling laws for neural language models.&rdquo; arXiv preprint arXiv:2001.08361 (2020).&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Hoffmann, Jordan, et al. &ldquo;Training compute-optimal large language models.&rdquo; arXiv preprint arXiv:2203.15556 (2022).&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://martinspace.top/zh/tags/nlp/">NLP</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://martinspace.top/zh/dmrl-survey/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>Preference Alignment on Diffusion Model: A Comprehensive Survey for Image Generation and Editing</span>
  </a>
  <a class="next" href="https://martinspace.top/zh/uq-cp/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>å…±å½¢é¢„æµ‹ç†è®ºç ”ç©¶</span>
  </a>
</nav>
<script src="https://utteranc.es/client.js"
        repo="oudushu/utterances"
        issue-term="title"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>
</script>



  </footer><script src="https://utteranc.es/client.js"
    repo="MartinRepo/utterancesInblog"
    issue-term="pathname"
    label="Comment"
    theme="preferred-color-scheme"
    crossorigin="anonymous"
    async>
</script>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://martinspace.top/zh/">Martin&#39;s space</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
        <br> 
        
        <a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">è¾½ ICP å¤‡ 2022011010 å· -1</a>
    </span>
    
    <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv">
        è¢«è®¿é—®äº†<span id="busuanzi_value_site_pv"></span>æ¬¡
    </span>
    <span id="busuanzi_container_site_uv">
        è¿æ¥äº†<span id="busuanzi_value_site_uv"></span>ä½å®¢äºº
    </span>
    </div>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerHTML = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerHTML = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script></body>

</html>

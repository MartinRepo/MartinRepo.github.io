<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>å¼ºåŒ–å­¦ä¹  | Martin&#39;s space</title>
<meta name="keywords" content="RL">
<meta name="description" content="å¤šè‡‚è€è™æœºé—®é¢˜(MAB) ç¬¦å·&amp;é—®é¢˜å®šä¹‰ å¤§å†™æ–œä½“è¡¨ç¤ºéšæœºå˜é‡ï¼Œä¾‹å¦‚$A, R, A_t, R_t$ å°å†™å­—æ¯è¡¨ç¤ºè¿™äº›éšæœºå˜é‡çš„å®ç°ï¼Œä¾‹å¦‚$a, r, a_t, r_t, Pr${$A_t=a_t$} èŠ±ä½“ï¼Œ">
<meta name="author" content="Martin">
<link rel="canonical" href="https://martinspace.top/zh/rl-note/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bc3e848411f8e2e3b39fd084e8d998d9f2d9782118c440525c90992eaecdc9f0.css" integrity="sha256-vD6EhBH44uOzn9CE6NmY2fLZeCEYxEBSXJCZLq7NyfA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://martinspace.top/icebear.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="https://martinspace.top/icebear.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="https://martinspace.top/icebear.jpg">
<link rel="apple-touch-icon" href="https://martinspace.top/icebear.jpg">
<link rel="mask-icon" href="https://martinspace.top/icebear.jpg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://martinspace.top/zh/rl-note/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css" />
<script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
          },
          svg: {
            fontCache: 'global'
          }
        };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-J8TFFL1ZS7"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-J8TFFL1ZS7', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="å¼ºåŒ–å­¦ä¹ " />
<meta property="og:description" content="å¤šè‡‚è€è™æœºé—®é¢˜(MAB) ç¬¦å·&amp;é—®é¢˜å®šä¹‰ å¤§å†™æ–œä½“è¡¨ç¤ºéšæœºå˜é‡ï¼Œä¾‹å¦‚$A, R, A_t, R_t$ å°å†™å­—æ¯è¡¨ç¤ºè¿™äº›éšæœºå˜é‡çš„å®ç°ï¼Œä¾‹å¦‚$a, r, a_t, r_t, Pr${$A_t=a_t$} èŠ±ä½“ï¼Œ" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://martinspace.top/zh/rl-note/" />
<meta property="og:image" content="https://martinspace.top/zh/rl-note/img/rl-note/whatisrl.png" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2025-02-01T10:39:49+00:00" />
<meta property="article:modified_time" content="2025-02-01T10:39:49+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://martinspace.top/zh/rl-note/img/rl-note/whatisrl.png" />
<meta name="twitter:title" content="å¼ºåŒ–å­¦ä¹ "/>
<meta name="twitter:description" content="å¤šè‡‚è€è™æœºé—®é¢˜(MAB) ç¬¦å·&amp;é—®é¢˜å®šä¹‰ å¤§å†™æ–œä½“è¡¨ç¤ºéšæœºå˜é‡ï¼Œä¾‹å¦‚$A, R, A_t, R_t$ å°å†™å­—æ¯è¡¨ç¤ºè¿™äº›éšæœºå˜é‡çš„å®ç°ï¼Œä¾‹å¦‚$a, r, a_t, r_t, Pr${$A_t=a_t$} èŠ±ä½“ï¼Œ"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "ğŸ“’ æ–‡ç« ",
      "item": "https://martinspace.top/zh/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "ğŸ”¬ ç ”ç©¶è®°å½•",
      "item": "https://martinspace.top/zh/post/2research/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "å¼ºåŒ–å­¦ä¹ ",
      "item": "https://martinspace.top/zh/rl-note/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "å¼ºåŒ–å­¦ä¹ ",
  "name": "å¼ºåŒ–å­¦ä¹ ",
  "description": "å¤šè‡‚è€è™æœºé—®é¢˜(MAB) ç¬¦å·\u0026amp;é—®é¢˜å®šä¹‰ å¤§å†™æ–œä½“è¡¨ç¤ºéšæœºå˜é‡ï¼Œä¾‹å¦‚$A, R, A_t, R_t$ å°å†™å­—æ¯è¡¨ç¤ºè¿™äº›éšæœºå˜é‡çš„å®ç°ï¼Œä¾‹å¦‚$a, r, a_t, r_t, Pr${$A_t=a_t$} èŠ±ä½“ï¼Œ",
  "keywords": [
    "RL"
  ],
  "articleBody": "å¤šè‡‚è€è™æœºé—®é¢˜(MAB) ç¬¦å·\u0026é—®é¢˜å®šä¹‰ å¤§å†™æ–œä½“è¡¨ç¤ºéšæœºå˜é‡ï¼Œä¾‹å¦‚$A, R, A_t, R_t$ å°å†™å­—æ¯è¡¨ç¤ºè¿™äº›éšæœºå˜é‡çš„å®ç°ï¼Œä¾‹å¦‚$a, r, a_t, r_t, Pr${$A_t=a_t$} èŠ±ä½“ï¼ŒåŒºé—´ç­‰è¡¨ç¤ºé›†åˆï¼Œä¾‹å¦‚$\\mathcal{A}, [0, 1], \\mathbb{N}$ Given: a set of k actions, $\\mathcal{A}$, number of rounds T. Repeat for t in T rounds:\nAlgorithm selects arm $A_t \\in \\mathcal{A}$ Algorithm observes reward $R_t \\in [0, 1]$ Goal: maximise expected total reward. é¢„æœŸçš„å¥–åŠ±è¢«ç§°ä¸ºValueï¼š$q_*(a) = \\mathbb{E}[R_t \\mid A_t = a]$\né—®é¢˜æ ¸å¿ƒ(Explore-exploit dilemma) ç”±äºå„ä¸ªè€è™æœºçš„å¥–åŠ±åˆ†å¸ƒæœªçŸ¥ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¢ç´¢å’Œåˆ©ç”¨ä¹‹é—´åšå‡ºæƒè¡¡ï¼Œæ¥æœ€å¤§åŒ–é•¿æœŸå¥–åŠ±ã€‚\næ¢ç´¢ï¼ˆExplorationï¼‰ å’Œ åˆ©ç”¨ï¼ˆExploitationï¼‰ ä¹‹é—´çš„æƒè¡¡æ˜¯ MAB é—®é¢˜çš„æ ¸å¿ƒï¼š\nExploration: é€‰æ‹©å½“å‰ä¿¡æ¯ä¸è¶³çš„é€‰é¡¹ï¼Œä»¥è·å¾—å…³äºå¥–åŠ±åˆ†å¸ƒçš„æ›´å¤šçŸ¥è¯†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæŸä¸ªè€è™æœºçš„å¥–åŠ±ä¸ç¡®å®šï¼Œä½ å¯èƒ½ä¼šå°è¯•æ‹‰å–å®ƒï¼Œä»¥æ”¶é›†æ›´å¤šæ•°æ®ã€‚ Exploitation: é€‰æ‹©å½“å‰ä¼°è®¡å¥–åŠ±æœ€é«˜çš„é€‰é¡¹ï¼Œä»¥æœ€å¤§åŒ–å³æ—¶æ”¶ç›Šã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ å·²ç»çŸ¥é“æŸä¸ªè€è™æœºçš„å¹³å‡å¥–åŠ±æœ€é«˜ï¼Œä½ ä¼šæ›´å€¾å‘äºæŒç»­æ‹‰å–å®ƒã€‚ è¿™ç§æƒè¡¡çš„æ ¸å¿ƒéš¾ç‚¹åœ¨äºï¼š\nè¿‡åº¦æ¢ç´¢ï¼ˆè¿‡å¤šå°è¯•æœªçŸ¥é€‰é¡¹ï¼‰å¯èƒ½å¯¼è‡´è¾ƒä½çš„çŸ­æœŸæ”¶ç›Šã€‚ è¿‡åº¦åˆ©ç”¨ï¼ˆè¿‡æ—©é”å®šæŸä¸ªé€‰é¡¹ï¼‰å¯èƒ½å¯¼è‡´é”™å¤±æ›´ä¼˜çš„é•¿æœŸæ”¶ç›Šã€‚ é—®é¢˜ç®—æ³• Action-Value Methods æˆ‘ä»¬å¯ä»¥ä¼°è®¡æŸä¸ªåŠ¨ä½œ$a$æ‰€è·å¾—å¥–åŠ±çš„æ ·æœ¬å‡å€¼ã€‚ï¼Œå…·ä½“å‡½æ•°è¡¨ç¤ºä¸ºï¼š $$ Q_t(a) = \\frac{\\text{Sum of rewards when taken a so far}}{\\text{Number of times taken a so far}} $$ $$ Q_t(a) = \\frac{1}{N_t(a)} \\sum^{t-1}_{\\tau=1}R _ {\\tau=1} \\cdot \\mathbb{1} _ {A_t=a} $$ å…¶ä¸­\n$N_t(a)$ æ˜¯åˆ°å½“å‰æ—¶é—´ $t$ ä¸ºæ­¢ï¼ŒåŠ¨ä½œ $a$ è¢«é€‰æ‹©çš„æ¬¡æ•°ã€‚ $R_{\\tau}$ æ˜¯ç¬¬ $\\tau$ æ¬¡æ‰§è¡ŒåŠ¨ä½œçš„å¥–åŠ±ã€‚ $\\mathbb{1} _ {A_{\\tau} = a}$ æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå‡½æ•°ï¼Œå½“ $\\tau$ æ—¶åˆ»é€‰æ‹©äº†åŠ¨ä½œ $a$ æ—¶ï¼Œå®ƒçš„å€¼ä¸º 1ï¼Œå¦åˆ™ä¸º 0ã€‚ å¦‚æœä¸€ä¸ªåŠ¨ä½œè¢«é€‰å–æ— ç©·æ¬¡ï¼Œé‚£ä¹ˆå®ƒçš„æ ·æœ¬å‡å€¼ä¼šæ”¶æ•›åˆ°çœŸå®çš„æœŸæœ›å¥–åŠ±ï¼Œå³ï¼š $$ \\lim_{N_t(a) \\to \\infty} Q_t(a) = q_*(a) $$\n$\\epsilon$- Greedy Action Selection æ ¹æ®ä¸Šé¢çš„å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠexploitè¡¨ç¤ºä¸º $$ A_t = A_t^* = \\mathop{\\arg\\max}\\limits_{a} Q_t(a) $$ exploration(éšæœºé€‰æ‹©action)è¡¨ç¤ºä¸º $$ A_t\\sim\\text{Unif}(\\mathcal{A}) $$\ndef epsilon_greedy(Q, epsilon, actions): if np.random.rand() \u003c epsilon: # ä»¥æ¦‚ç‡ epsilon è¿›è¡Œæ¢ç´¢ return np.random.choice(actions) else: # ä»¥æ¦‚ç‡ (1 - epsilon) é€‰æ‹©å½“å‰æœ€ä¼˜åŠ¨ä½œ return np.argmax(Q) self.execute(A_t) self.observe(R_t) self.update(N_t_a, Q_t_a) å¦‚ä½•åœ¨é€’å¢çš„åŒæ—¶é¿å…é‡å¤è®¡ç®—ï¼Ÿ $$ Q_n = \\frac{R_1 + â€¦ + R_{n-1}}{n-1} $$ $$ Q_{n+1} = Q_n + \\frac{1}{n}[R_n - Q_n] $$ è¿™ä¹Ÿæ˜¯å¼ºåŒ–å­¦ä¹ ä¸­update rulesçš„æ ‡å‡†å½¢å¼ $$ \\text{NewEstimate \u003câ€” OldEstimate + StepSize[Target - OldEstimate]} $$\nNon-Stationary Problem å‡è®¾true action valueä¼šéšæ—¶é—´è€Œå˜åŒ–ï¼Œè¿™æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­ç»å¸¸é‡åˆ°çš„non-stationary problemï¼Œè¿™ä¸ªæ—¶å€™å†ç”¨sample averageå°±ä¸åˆé€‚äº†ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªstep-size parameter $\\alpha\\in[0, 1]$æ¥è·Ÿè¸ªaction valueï¼Œé‚£ä¹ˆæ›´æ–°å‡½æ•°å˜ä¸º $$ Q_{n+1} = Q_n + \\alpha[R_n - Q_n] $$\nStochastic Approximation Convergence Conditions éšæœºé€¼è¿‘ï¼ˆStochastic Approximation, SAï¼‰ æ˜¯ä¸€ç§ç”¨äºåœ¨å™ªå£°ç¯å¢ƒä¸­è¿­ä»£é€¼è¿‘æœ€ä¼˜è§£çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å¸¸ç”¨äºæœºå™¨å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚ Q-learningï¼‰ã€ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚éšæœºæ¢¯åº¦ä¸‹é™ï¼‰ç­‰ã€‚\nåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå€¼å‡½æ•°æ›´æ–° é‡‡ç”¨éšæœºé€¼è¿‘çš„æ–¹å¼ï¼Œä¾‹å¦‚ï¼š $$ Q_{t+1}(s, a) = Q_t(s, a) + \\alpha_t [R_t + \\gamma \\max_{aâ€™} Q_t(sâ€™, aâ€™) - Q_t(s, a)] $$ å…¶ä¸­ $\\alpha_t$ æ˜¯å­¦ä¹ ç‡ï¼Œç›®æ ‡æ˜¯è®© $Q_t(s,a)$ æ”¶æ•›åˆ°æœ€ä¼˜å€¼ $Q^*(s,a)$ã€‚\nä¸ºäº†ä¿è¯ éšæœºé€¼è¿‘ç®—æ³• èƒ½å¤Ÿæ”¶æ•›ï¼Œå¿…é¡»æ»¡è¶³æ”¶æ•›æ¡ä»¶ï¼ˆConvergence Conditionsï¼‰:\nå­¦ä¹ ç‡æ»¡è¶³ Robbins-Monro æ¡ä»¶ $$ \\sum_{t=1}^{\\infty} \\alpha_t = \\infty, \\quad \\sum_{t=1}^{\\infty} \\alpha_t^2 \u003c \\infty $$\nç¬¬ä¸€é¡¹ $ \\sum \\alpha_t = \\infty $ ç¡®ä¿ç®—æ³•åœ¨é•¿æœŸå†…ç»§ç»­å­¦ä¹ ï¼Œå¦åˆ™ç®—æ³•å¯èƒ½è¿‡æ—©åœæ­¢æ›´æ–°ã€‚ ç¬¬äºŒé¡¹ $ \\sum \\alpha_t^2 \u003c \\infty $ ç¡®ä¿å­¦ä¹ ç‡æœ€ç»ˆè¶³å¤Ÿå°ï¼Œå¦åˆ™æ”¶æ•›ä¼šå—å™ªå£°å½±å“ã€‚ ç¤ºä¾‹ï¼š\næ»¡è¶³æ¡ä»¶çš„å­¦ä¹ ç‡ï¼š $ \\alpha_t = \\frac{1}{t} $ã€$ \\alpha_t = \\frac{1}{\\sqrt{t}} $ ä¸æ»¡è¶³æ¡ä»¶çš„å­¦ä¹ ç‡ï¼š å›ºå®šçš„ $ \\alpha_t = 0.1 $ï¼ˆå› ä¸ºæ±‚å’Œä¸å‘æ•£ï¼‰ Upper Confidence Bound (UCB) ç›¸æ¯”äº $\\epsilon$-Greedy ç®—æ³•çš„éšæœºæ¢ç´¢ç­–ç•¥ï¼ŒUCB é€šè¿‡æ„é€ ä¸€ä¸ªä¸Šç½®ä¿¡ç•Œï¼ˆUpper Confidence Boundï¼‰æ¥è¿›è¡Œæ¢ç´¢ï¼Œä»è€Œæ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚\nUCB é‡‡ç”¨ ç½®ä¿¡åŒºé—´ï¼ˆconfidence boundï¼‰ æ¥å†³å®šæ˜¯å¦æ¢ç´¢æŸä¸ªåŠ¨ä½œï¼š\nåˆ©ç”¨ï¼ˆExploitationï¼‰ï¼š é€‰æ‹©å†å²ä¸Šå¹³å‡å¥–åŠ±æœ€é«˜çš„åŠ¨ä½œï¼ˆå³å½“å‰æœ€ä¼˜åŠ¨ä½œï¼‰ã€‚ æ¢ç´¢ï¼ˆExplorationï¼‰ï¼š é€‰æ‹©é‚£äº›å°è¯•æ¬¡æ•°è¾ƒå°‘ã€ä¸ç¡®å®šæ€§è¾ƒå¤§çš„åŠ¨ä½œã€‚ å…¬å¼ï¼š $$ A_t = \\arg\\max_{a} \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right] $$ å…¶ä¸­ï¼š\n$ Q_t(a) $ æ˜¯åŠ¨ä½œ $ a $ åœ¨æ—¶é—´ $ t $ æ—¶åˆ»çš„å¹³å‡å¥–åŠ±ï¼ˆå†å²ç»éªŒï¼‰ã€‚ $ N_t(a) $ æ˜¯åŠ¨ä½œ $ a $ è¢«é€‰æ‹©çš„æ¬¡æ•°ã€‚ $ t $ æ˜¯å½“å‰æ—¶é—´æ­¥ï¼ˆå½“å‰å®éªŒçš„æ€»æ¬¡æ•°ï¼‰ã€‚ $ c $ æ˜¯æ¢ç´¢å‚æ•°ï¼Œæ§åˆ¶æ¢ç´¢ç¨‹åº¦ï¼ˆé€šå¸¸ $ c $ å– $\\sqrt{2}$ï¼‰ã€‚ $ \\ln t $ æ˜¯å¯¹æ•°é¡¹ï¼Œä½¿å¾—æ¢ç´¢éšæ—¶é—´å‡å°‘ã€‚ è§£é‡Šï¼š\n$ Q_t(a) $ ä»£è¡¨ åˆ©ç”¨ï¼ˆExploitationï¼‰ï¼šå€¾å‘äºé€‰æ‹©å†å²ä¸Šè¡¨ç°æœ€å¥½çš„åŠ¨ä½œã€‚ $ \\sqrt{\\frac{\\ln t}{N_t(a)}} $ ä»£è¡¨ æ¢ç´¢ï¼ˆExplorationï¼‰ï¼šå½“æŸä¸ªåŠ¨ä½œé€‰æ‹©æ¬¡æ•° ( N_t(a) ) å¾ˆå°æ—¶ï¼Œè¯¥é¡¹è¾ƒå¤§ï¼Œé¼“åŠ±æ¢ç´¢ã€‚ import numpy as np def UCB(Q, N, t, c=1.0): ucb_values = Q + c * np.sqrt(np.log(t) / (N + 1e-5)) # é¿å…é™¤é›¶é”™è¯¯ return np.argmax(ucb_values) # é€‰æ‹© UCB å€¼æœ€é«˜çš„åŠ¨ä½œ å…¶ä¸­ï¼š\nQï¼šå­˜å‚¨æ¯ä¸ªåŠ¨ä½œçš„å¹³å‡å¥–åŠ±ã€‚ Nï¼šå­˜å‚¨æ¯ä¸ªåŠ¨ä½œçš„é€‰æ‹©æ¬¡æ•°ã€‚ tï¼šå½“å‰æ—¶é—´æ­¥æ•°ï¼ˆæ€»å®éªŒæ¬¡æ•°ï¼‰ã€‚ cï¼šæ¢ç´¢å‚æ•°ï¼Œæ§åˆ¶æ¢ç´¢å¼ºåº¦ã€‚ é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDPs) ç†è®ºæ¡†æ¶ é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹åŒ…æ‹¬çŠ¶æ€ç©ºé—´$\\mathcal{S}$ï¼ŒåŠ¨ä½œç©ºé—´$\\mathcal{A}$ï¼Œå¥–åŠ±ç©ºé—´$\\mathcal{R}$ã€‚MDPæ˜¯æœ‰é™é›†ï¼Œå¦‚æœ$\\mathcal{S}$, $\\mathcal{A}$, $\\mathcal{R}$éƒ½æ˜¯æœ‰é™é›†\né©¬å°”å¯å¤«æ€§è´¨ï¼šç®€å•è¯´å°±æ˜¯æœªæ¥çš„çŠ¶æ€åªä¸å½“å‰ä¸€ä¸ªçŠ¶æ€æœ‰å…³ï¼Œç‹¬ç«‹äºè¿‡å»çš„æ‰€æœ‰çŠ¶æ€ï¼ˆFuture state and reward are independent of past states and actions, given the current state and actionï¼‰\n$Pr${$S_{t+1},R_{t+1} | S_t,A_t,S_{tâˆ’1},A_{tâˆ’1},â€¦,S_0,A_0$}$ = Pr${$S_{t+1},R_{t+1} | S_t,A_t$}\nçŠ¶æ€$S_t$æ˜¯äº¤äº’å†å²çš„å……åˆ†æ€»ç»“ã€‚è®¾è®¡compact Markov statesæ˜¯RLä¸­çš„ä¸€é¡¹å·¥ç¨‹å·¥ä½œã€‚\nä»¥recycling robotä¸ºä¾‹ã€‚\nStates high battery level low battery level Actions search for can wait for someone to bring can recharge battery at charging station Rewards: number of cans collected $s$ $a$ $s'$ $p(sâ€™\\mid s, a)$ $r(s, a, sâ€™)$ high search high $\\alpha$ $r_{\\text{research}}$ high search low $1 - \\alpha$ $r_{\\text{research}}$ low search high $1 - \\beta$ -3 low search low $\\beta$ $r_{\\text{research}}$ high wait high 1 $r_{\\text{wait}}$ high wait low 0 - low wait high 0 - low wait low 1 $r_{\\text{wait}}$ high recharge high 1 0 high recharge low 0 - æ ¸å¿ƒæ•°å­¦æ¦‚å¿µ\u0026å‡½æ•° Policy é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ç”±policyæ§åˆ¶ã€‚\n$\\pi(a\\mid s) = $ åœ¨çŠ¶æ€$s$çš„æƒ…å†µä¸‹é€‰æ‹©åŠ¨ä½œ$a$çš„æ¦‚ç‡\n$\\pi(a\\mid s)$ search wait recharge high 0.9 0.1 0 low 0.2 0.3 0.5 Agentçš„ç›®æ ‡å°±æ˜¯å­¦ä¸€ä¸ªèƒ½æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±çš„ç­–ç•¥\nReturnsï¼ˆå›æŠ¥ï¼‰ Total Return\nç­–ç•¥åº”è¯¥æœ€å¤§åŒ–é¢„æœŸå›æŠ¥ï¼ˆ$G_t$ï¼‰ $$ G_t = R_{t+1} + R_{t+2} + â€¦ + R_{T} = R_{t+1} + G_{t+1} $$ where $T$ is final time step\nAssumes terminating episodes: é€‚ç”¨äºå­˜åœ¨æ˜ç¡®ç»ˆæ­¢æ¡ä»¶çš„ä»»åŠ¡ã€‚ä¾‹å¦‚åœ¨è±¡æ£‹æ¸¸æˆä¸­ï¼Œä¸€ä¸ªç©å®¶è·èƒœå°±ç»ˆæ­¢ã€‚é€šè¿‡è®¾ç½®å…è®¸çš„æ—¶é—´æ­¥æ•°æ¥å¼ºåˆ¶ç»ˆæ­¢\nDiscounted Return\nå¯¹äºä¸èƒ½ç»ˆæ­¢çš„ï¼ˆæ— é™çš„ï¼‰åœºæ™¯ï¼Œå¯ä»¥ä½¿ç”¨æŠ˜æ‰£ç‡ $\\gamma \\in [0, 1)$ $$ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + â€¦ = R_{t+1} + \\gamma G_{t+1} $$ $\\gamma$æ˜¯æŠ˜æ‰£å› å­ï¼Œç”¨äºé™ä½è¿œæœŸå¥–åŠ±çš„ä»·å€¼ã€‚\nè¿™é€‚ç”¨äºæ— é™å›åˆä»»åŠ¡ï¼Œä¾‹å¦‚è‚¡ç¥¨äº¤æ˜“ç­–ç•¥ï¼Œæ™ºèƒ½ä½“æŒç»­è¿›è¡Œäº¤æ˜“ï¼Œæ²¡æœ‰å›ºå®šç»“æŸæ—¶é—´ã€‚\nValue Functions è¿™å¼ å›¾ç»™ä¸€ä¸ªoverview Bellman Equation åœ¨å¼ºåŒ–å­¦ä¹ å’Œé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­ï¼Œè´å°”æ›¼æ–¹ç¨‹ç”¨äºæè¿°çŠ¶æ€å€¼å‡½æ•°å’ŒåŠ¨ä½œå€¼å‡½æ•°çš„é€’å½’å…³ç³»ã€‚è¿™ä¸¤æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­æ±‚è§£æœ€ä¼˜ç­–ç•¥çš„é‡è¦æ•°å­¦å·¥å…·ã€‚\nState Value Function and the Bellman Equation\næ ¹æ®Markov propertyï¼Œå¯ä»¥æŠŠçŠ¶æ€ä»·å€¼å‡½æ•°å†™æˆBellman Equationçš„é€’å½’å½¢å¼ã€‚ $$ v_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s] $$ è¿™ä¸ªå…¬å¼çš„æ„æ€å°±æ˜¯ç»™å®šå½“å‰çŠ¶æ€çš„æƒ…å†µä¸‹ï¼Œç­–ç•¥$\\pi$å¯ä»¥è·å¾—çš„å›æŠ¥æœŸæœ›å€¼\nå°†å…¬å¼å±•å¼€ï¼Œå¾—åˆ°ï¼š\n$$ v_\\pi(s) = \\sum_a \\pi(a \\mid s)r(s, a) + \\gamma\\sum_{sâ€™ \\in S} p(sâ€™\\mid s, a)\\cdot v_\\pi(sâ€™) $$\nå…¶ä¸­$\\sum_a \\pi(a \\mid s)r(s, a)$è¡¨ç¤ºå³æ—¶å¥–åŠ±ï¼ˆé€‰æ‹©åŠ¨ä½œ$a$çš„æ¦‚ç‡$\\times$è¿™æ ·åšå¸¦æ¥çš„å¥–åŠ±å€¼ï¼‰ï¼Œ$\\gamma$æ˜¯æŠ˜æ‰£å› å­ï¼Œ$\\sum_{sâ€™ \\in S} p(sâ€™\\mid s, a)\\cdot v_\\pi(sâ€™)$è¡¨ç¤ºexpected future value (å½“å‰çŠ¶æ€$s$, é‡‡å–åŠ¨ä½œ$a$ä¹‹åï¼Œæœ‰å¤šå¤§æ¦‚ç‡è½¬åˆ°æ¯ä¸ªå¯èƒ½çš„$sâ€™$,æ¯ä¸ª$sâ€™$ä¼šå¸¦æ¥å¤šå¤§ä»·å€¼ï¼Œå…¨éƒ¨åŠ æƒæ±‚å’Œåå°±æ˜¯æœªæ¥çš„é¢„æœŸä»·å€¼)\nç›´è§‰ç†è§£ï¼šçœ¼å‰æ”¶ç›Š+æœªæ¥æ”¶ç›Šï¼ˆå¸¦æœ‰æŠ˜æ‰£ç‡æ¥é™ä½å½±å“ï¼‰\nAction Value Functionçš„bellmanå½¢å¼ $$ q_\\pi(s, a) = r(s, a) + \\gamma \\sum_{sâ€™ \\in S} p(sâ€™ \\mid s, a)\\cdot v_\\pi(sâ€™) $$ ç”¨æœŸæœ›ç®€å†™ $$ q_\\pi(s, a) = r(s, a) + \\gamma \\mathbb{E} _ {sâ€™}[ v _ \\pi(sâ€™) ] $$\nState valueå’Œaction valueæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\nstate valueæ˜¯åœ¨æŸä¸ªçŠ¶æ€ä¸‹ï¼ŒæŒ‰ç…§ç­–ç•¥$\\pi$èµ°ä¸‹å»ï¼Œæœªæ¥çš„æœŸæœ›æ€»å¥–åŠ± action valueæ˜¯åœ¨æŸä¸ªçŠ¶æ€ä¸‹ï¼Œæ‰§è¡ŒåŠ¨ä½œ$a$ï¼Œå†æŒ‰ç…§ç­–ç•¥$\\pi$èµ°ä¸‹å»ï¼Œæœªæ¥çš„æœŸæœ›æ€»å¥–åŠ±ã€‚ ç­–ç•¥å’ŒåŠ¨ä½œçš„åŒºåˆ«ï¼šç­–ç•¥ä¸æ˜¯åŠ¨ä½œçš„é›†åˆï¼Œè€Œæ˜¯ä¸€ä¸ªâ€œä»çŠ¶æ€åˆ°åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒâ€çš„æ˜ å°„å‡½æ•°ã€‚\nå¦‚æœ $$ v _ \\pi(s) = v_*(s) = \\max_{\\piâ€™}v_{\\piâ€™}(s) $$\nå¹¶ä¸”\n$$ q_{\\pi}(s, a) = q_*(s, a) = \\max_{\\piâ€™}q_{\\piâ€™}(s, a) $$ é‚£ä¹ˆç­–ç•¥$\\pi$å°±æ˜¯æœ€ä¼˜çš„ã€‚\nå¦‚ä½•è®¡ç®—å¹¶æ‰¾åˆ°è¿™ä¸ªæœ€ä¼˜ç­–ç•¥ï¼Ÿæ¶‰åŠåˆ°åŠ¨æ€è§„åˆ’ä¸­çš„policy iterationå’Œvalue iterationï¼š\nåŠ¨æ€è§„åˆ’ åŠ¨æ€è§„åˆ’æ ¸å¿ƒæ€æƒ³ï¼šuse Bellman Equations to organise search for good policies\nDP Algo1: Policy Iteration è¿™ä¸ªç®—æ³•åŒ…å«ä¸¤ä¸ªé˜¶æ®µ:\nç­–ç•¥è¯„ä¼°ï¼šç»™å®šç­–ç•¥$\\pi$, è®¡ç®—å¯¹åº”çš„state value function $v_\\pi(s)$ï¼› è§£å†³Bellmanæ–¹ç¨‹ï¼Œå¯è¿­ä»£é€¼è¿‘ ç­–ç•¥æå‡ï¼šç”¨å½“å‰çš„$v_\\pi(s)$æ¥æ›´æ–°ç­–ç•¥ï¼Œå¦‚æœæ–°ç­–ç•¥ = æ—§ç­–ç•¥ï¼Œè¯´æ˜æœ€ä¼˜ï¼Œåœæ­¢ã€‚ $$ \\pi_{\\text{new}}(s) = \\arg \\max_a \\sum_{sâ€™}p(sâ€™\\mid s, a)[r(s, a) + \\gamma v_\\pi(sâ€™)] $$ å…·ä½“è¡¨ç¤ºå¦‚ä¸‹ï¼ˆè¿™ä¸ªè¿‡ç¨‹ä¼šå¾ˆå¿«æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ï¼‰ $$ \\pi_0 \\xrightarrow{E} v_{\\pi_0} \\xrightarrow{I} \\pi_1 \\xrightarrow{E} v_{\\pi_1} \\xrightarrow{I} â€¦ \\xrightarrow{I} \\pi_* \\xrightarrow{E} v_* $$\nIterative policy evaluation ä¼ªä»£ç å¦‚ä¸‹\nInput: pi, the policy to be evaluated Initialize an array V(s) = 0, for all s Repeat delta \u003c- 0 For each s in S: v \u003c- V(s) V(s) \u003c- State Value Function delta \u003c- max(delta, |v - V(s)|) until delta \u003c theta (a small positive number) Output V \\approx= v_pi Policy Improvement è®¡ç®—åœ¨å½“å‰state value funtion $V(s)$ä¸‹ï¼Œæ¯ä¸ªçŠ¶æ€$s$å¯¹ä¸åŒåŠ¨ä½œ$a$çš„action valueã€‚ç„¶åé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ$\\piâ€™(s)$ï¼Œå¦‚æœè¿™ä¸ä¹‹å‰çš„$\\pi(s)$ä¸åŒï¼Œåˆ™ç­–ç•¥å‘ç”Ÿäº†æ›´æ–°ï¼Œå¹¶ç»§ç»­è¿­ä»£ï¼Œå¦åˆ™åœæ­¢ã€‚\nDP Algo2: Value Iteration ä»¥ç§Ÿè½¦é—®é¢˜ä¸ºä¾‹ã€‚æœ‰ä¸¤ä¸ªè½¦è¾†ç§Ÿèµç‚¹ï¼Œæ ¹æ®åˆ†å¸ƒéšæœºè¯·æ±‚å’Œè¿”å›è½¦è¾†ã€‚çŠ¶æ€ï¼ˆStatesï¼‰è¡¨ç¤ºä¸º(n1, n2)ï¼Œå…¶ä¸­niæ˜¯ä½ç½®içš„æ±½è½¦æ•°é‡ï¼ˆæ¯ä¸ªåœ°æ–¹æœ€å¤š20è¾†ï¼‰ã€‚åŠ¨ä½œï¼ˆActionsï¼‰è¡¨ç¤ºä»ä¸€ä¸ªä½ç½®ç§»åŠ¨åˆ°å¦ä¸€ä¸ªä½ç½®çš„è½¦å­çš„æ•°é‡ã€‚å…¶ä¸­å¦‚æœæ˜¯ä»1ç§»åˆ°2å°±æ˜¯æ­£æ•°ï¼Œä»2ç§»åˆ°1å°±æ˜¯è´Ÿæ•°ï¼Œæœ€å¤š5è¾†ã€‚å¥–åŠ±ï¼ˆRewardsï¼‰è¡¨ç¤ºä¸ºæ¯ä¸ªæ—¶é—´æ­¥ä¸­ï¼Œç§Ÿå‡ºå»ä¸€è¾†è½¦+$10ï¼Œç§»åŠ¨ä¸€è¾†è½¦-$2ã€‚æœ€å$\\gamma$ = 0.9ã€‚\nPolicy iterationä½¿ç”¨Bellman equationä½œä¸ºoperator: $$ v_{k+1}(s) = \\sum_a \\pi(a\\mid s)\\sum_{sâ€™, r}p(sâ€™, r\\mid s, a) [r+\\gamma v_k(sâ€™)] $$ å…¶ä¸­$s\\in S$\nValue iterationä½¿ç”¨Bellman optimality equationä½œä¸ºoperator: $$ v_{k+1}(s) = \\max_a \\sum_{sâ€™, r}p(sâ€™, r\\mid s, a) [r+\\gamma v_k(sâ€™)] $$ å…¶ä¸­$s\\in S$\nValue iterationçš„ä¼ªä»£ç å¦‚ä¸‹ Asynchronous and generalised DP ç›®å‰ä¸ºæ­¢ï¼ŒåŠ¨æ€è§„åˆ’æ–¹æ³•æ‰§è¡Œçš„æ˜¯ç©·ä¸¾ã€‚å¦‚æœçŠ¶æ€ç©ºé—´å¾ˆå¤§ï¼Œpolicy evaluation and improvementå°±ä¸å¯è®¡ç®—äº†ã€‚\nå¼‚æ­¥åŠ¨æ€è§„åˆ’æ–¹æ³•é¿å…äº†æ ‡å‡†åŠ¨æ€è§„åˆ’ä¸­å¿…é¡»åŒæ—¶æ›´æ–°æ‰€æœ‰çŠ¶æ€çš„é™åˆ¶ï¼Œè€Œæ˜¯é€‰æ‹©æ€§åœ°æ›´æ–°æŸäº›çŠ¶æ€ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚\næ”¹è¿›ç‚¹ï¼š\nçŠ¶æ€å¼‚æ­¥æ›´æ–°ï¼šä¸è¦æ±‚æ‰€æœ‰çŠ¶æ€åœ¨åŒä¸€æ—¶é—´æ›´æ–°ï¼Œè€Œæ˜¯æŒ‰éœ€æ›´æ–°ä¸€éƒ¨åˆ†çŠ¶æ€ã€‚ ä¼˜å…ˆçº§æ›´æ–°ï¼šä¼˜å…ˆæ›´æ–°é‚£äº› ä»·å€¼å˜åŒ–è¾ƒå¤§æˆ–ä¸æœ€ç»ˆç­–ç•¥æœ€ç›¸å…³çš„çŠ¶æ€ï¼Œä½¿æœ‰ä»·å€¼çš„ä¿¡æ¯æ›´å¿«ä¼ æ’­ã€‚ åŠ é€Ÿæ”¶æ•›ï¼šå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤§è§„æ¨¡é—®é¢˜ï¼Œå¦‚æœºå™¨äººæ§åˆ¶ã€è·¯å¾„è§„åˆ’ã€å¼ºåŒ–å­¦ä¹ ç­‰ã€‚ è’™ç‰¹å¡æ´›æ–¹æ³• Monte Carloç­–ç•¥è¯„ä¼° - åœ¨æ²¡æœ‰æ¨¡å‹çš„æƒ…å†µä¸‹è¿›è¡Œç­–ç•¥ä»·å€¼è¯„ä¼°\nMCä¸éœ€è¦çŸ¥é“ç¯å¢ƒçš„æ¨¡å‹$p(sâ€™, r \\mid s, a)$ï¼Œåªéœ€è¦sampled episodesã€‚ä½ åªéœ€è¦èƒ½è·Ÿç¯å¢ƒç©ã€ç©å®Œæ•´åœºæ¸¸æˆï¼Œç„¶åè®°å½•æ¯ä¸ªçŠ¶æ€å‡ºç°åæœ€ç»ˆèµšäº†å¤šå°‘é’±ï¼Œ æŠŠè¿™ä¸ªâ€œèµšçš„é’±â€åœ¨æ‰€æœ‰è®¿é—®è¿‡è¿™ä¸ªçŠ¶æ€çš„ episode é‡Œå¹³å‡ä¸€ä¸‹ï¼Œè¿™å°±æ˜¯å®ƒçš„å€¼$v_\\pi(s)$ã€‚å…¬å¼å¯ä»¥è¡¨ç¤ºä¸º $$ v_\\pi(s) = \\mathbb{E} _ \\pi[G_i \\mid s_i = s] = \\frac{1}{N(s)}\\sum^{N(s)}_{i=1}G_i $$ å…¶ä¸­$N(s)$å°±æ˜¯çŠ¶æ€sè¢«è®¿é—®çš„æ¬¡æ•°ï¼Œ$G_i$æ˜¯ç¬¬iæ¬¡è®¿é—®såçš„å®é™…returnã€‚\nså¯èƒ½è¢«å¤šæ¬¡è®¿é—®ï¼Œå› æ­¤è’™ç‰¹å¡æ´›æ–¹æ³•åˆ†ä¸ºfirst-visit MCå’Œevery-visit MCã€‚ä¸¤è€…çš„åŒºåˆ«åœ¨äºæ›´æ–°æ—¶æ˜¯å¦æ ¡éªŒ$S_t$å·²ç»åœ¨å½“å‰episodeä¸­å‡ºç°è¿‡ã€‚First-visitåªæ›´æ–°è¯¥çŠ¶æ€ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®ï¼ŒEvery-visitåˆ™æ˜¯è¯¥çŠ¶æ€å‡ºç°å‡ æ¬¡å°±æ›´æ–°å‡ æ¬¡ã€‚\nå¦‚æœæ— æ³•å¾—åˆ°ç¯å¢ƒçš„æ¨¡å‹ï¼Œé‚£ä¹ˆè®¡ç®—åŠ¨ä½œçš„ä»·å€¼ï¼ˆâ€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„çš„ä»·å€¼ï¼‰æ¯”è®¡ç®—çŠ¶æ€çš„ä»·å€¼æ›´åŠ æœ‰ç”¨ã€‚åªéœ€å°†å¯¹çŠ¶æ€çš„è®¿é—®æ”¹ä¸ºå¯¹â€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„çš„è®¿é—®ï¼Œè’™ç‰¹å¡æ´›ç®—æ³•å°±å¯ä»¥å‡ ä¹å’Œä¹‹å‰å®Œå…¨ç›¸åŒçš„æ–¹å¼è§£å†³è¯¥é—®é¢˜ï¼Œå”¯ä¸€å¤æ‚ä¹‹å¤„åœ¨äºä¸€äº›â€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„å¯èƒ½æ°¸è¿œä¸ä¼šè¢«è®¿é—®åˆ°ã€‚ä¸ºäº†å®ç°åŸºäºåŠ¨ä½œä»·å€¼å‡½æ•°çš„ç­–ç•¥è¯„ä¼°ï¼Œæˆ‘ä»¬å¿…é¡»ä¿è¯æŒç»­çš„è¯•æ¢ã€‚ä¸€ç§æ–¹å¼æ˜¯å°†æŒ‡å®šçš„â€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„ä½œä¸ºèµ·ç‚¹å¼€å§‹ä¸€å¹•é‡‡æ ·ï¼ŒåŒæ—¶ä¿è¯æ‰€æœ‰â€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„éƒ½æœ‰éé›¶çš„æ¦‚ç‡å¯ä»¥è¢«é€‰ä¸ºèµ·ç‚¹ã€‚è¿™æ ·å°±ä¿è¯äº†åœ¨é‡‡æ ·çš„å¹•ä¸ªæ•°è¶‹äºæ— ç©·æ—¶ï¼Œæ¯ä¸€ä¸ªâ€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„éƒ½ä¼šè¢«è®¿é—®åˆ°æ— æ•°æ¬¡ã€‚æˆ‘ä»¬æŠŠè¿™ç§å‡è®¾ç§°ä¸ºMC control with exploring startsã€‚\nç­–ç•¥æ”¹è¿›çš„æ–¹æ³•æ˜¯åœ¨å½“å‰ä»·å€¼å‡½æ•°ä¸Šè´ªå¿ƒåœ°é€‰æ‹©åŠ¨ä½œã€‚ç”±äºæˆ‘ä»¬æœ‰åŠ¨ä½œä»·å€¼å‡½æ•°ï¼Œæ‰€ä»¥åœ¨è´ªå¿ƒçš„æ—¶å€™å®Œå…¨ä¸éœ€è¦ä½¿ç”¨ä»»ä½•çš„æ¨¡å‹ä¿¡æ¯ã€‚\nExploring startsçš„ä¸€ä¸ªé‡è¦å‡è®¾æ˜¯ç¯å¢ƒå¿…é¡»æ”¯æŒä»»æ„(s,a)èµ·ç‚¹ï¼Œè¿™åœ¨å®é™…ä¸­å‡ ä¹ä¸æˆç«‹ã€‚\nä½†æ˜¯ç°å®ä¸­ï¼Œæˆ‘ä»¬å¾ˆéš¾æ»¡è¶³è¯•æ¢æ€§å‡ºå‘çš„å‡è®¾ï¼Œä¸€èˆ¬æ€§çš„è§£æ³•æ˜¯æ™ºèƒ½ä½“èƒ½å¤ŸæŒç»­ä¸æ–­åœ°é€‰æ‹©æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œæœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥ä¿è¯è¿™ä¸€ç‚¹ï¼ŒåŒè½¨ç­–ç•¥ï¼ˆsoft on-policyï¼‰å’Œç¦»è½¨ç­–ç•¥ï¼ˆoff-policyï¼‰ã€‚åœ¨åŒè½¨ç­–ç•¥ä¸­ï¼Œç”¨äºç”Ÿæˆé‡‡æ ·æ•°æ®åºåˆ—çš„ç­–ç•¥å’Œç”¨äºå®é™…å†³ç­–çš„å¾…è¯„ä¼°å’Œæ”¹è¿›çš„ç­–ç•¥æ˜¯ç›¸åŒçš„ï¼›è€Œåœ¨ç¦»è½¨ç­–ç•¥ä¸­ï¼Œç”¨äºè¯„ä¼°æˆ–æ”¹è¿›çš„ç­–ç•¥ä¸ç”Ÿæˆé‡‡æ ·æ•°æ®çš„ç­–ç•¥æ˜¯ä¸åŒçš„ï¼Œå³ç”Ÿæˆçš„æ•°æ®â€œç¦»å¼€â€äº†å¾…ä¼˜åŒ–çš„ç­–ç•¥æ‰€å†³å®šçš„å†³ç­–åºåˆ—è½¨è¿¹ã€‚\nTemporal Difference Learning TD policy evaluation TD control Sarsa åŒè½¨ç­–ç•¥ä¸‹çš„TD\nQ-learning ç¦»è½¨ç­–ç•¥ä¸‹çš„TD\nn-steps TD methods Planning and Learning Value function approximation Policy Gradient Methods Reference [1] https://leovan.me/cn/2020/07/model-free-policy-prediction-and-control-monte-carlo-learning/ [2] https://leovan.me/cn/2020/07/model-free-policy-prediction-and-control-temporal-difference-learning/\n",
  "wordCount" : "5142",
  "inLanguage": "zh",
  "image":"https://martinspace.top/zh/rl-note/img/rl-note/whatisrl.png","datePublished": "2025-02-01T10:39:49Z",
  "dateModified": "2025-02-01T10:39:49Z",
  "author":[{
    "@type": "Person",
    "name": "Martin"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://martinspace.top/zh/rl-note/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Martin's space",
    "logo": {
      "@type": "ImageObject",
      "url": "https://martinspace.top/icebear.jpg"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>



<script async src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://martinspace.top/zh/" accesskey="h" title="Martin&#39;s space (Alt + H)">
                <img src="https://martinspace.top/icebear.jpg" alt="" aria-label="logo"
                    height="35">Martin&#39;s space</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://martinspace.top/en/" title="English"
                            aria-label="English">English</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://martinspace.top/zh/" title="ğŸ  ä¸»é¡µ">
                    <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/archives/" title="ğŸ“ å½’æ¡£">
                    <span>ğŸ“ å½’æ¡£</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/tags" title="ğŸ·ï¸ æ ‡ç­¾">
                    <span>ğŸ·ï¸ æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                    <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/about" title="ğŸ’­ å…³äº">
                    <span>ğŸ’­ å…³äº</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://martinspace.top/zh/">ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://martinspace.top/zh/post/">ğŸ“’ æ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://martinspace.top/zh/post/2research/">ğŸ”¬ ç ”ç©¶è®°å½•</a></div>
    <h1 class="post-title">
      å¼ºåŒ–å­¦ä¹ 
    </h1>
    <div class="post-meta">










åˆ›å»º: 2025-02-01 | æ›´æ–°: 2025-02-01 | å­—æ•°: 5142å­— | é˜…è¯»æ—¶é•¿: 11åˆ†é’Ÿ | 
ä½œè€…:Martin&nbsp;|&nbsp;æ ‡ç­¾: &nbsp;
    <ul class="post-tags-meta">
        <a href="https://martinspace.top/zh/tags/rl/">RL</a>
    </ul>


    
    </div>
  </header> 
<figure class="entry-cover1"><img loading="lazy" src="https://martinspace.top/img/rl-note/whatisrl.png" alt="what-is-rl">
        
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">ç›®å½•</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e5%a4%9a%e8%87%82%e8%80%81%e8%99%8e%e6%9c%ba%e9%97%ae%e9%a2%98mab" aria-label="å¤šè‡‚è€è™æœºé—®é¢˜(MAB)">å¤šè‡‚è€è™æœºé—®é¢˜(MAB)</a><ul>
                            
                    <li>
                        <a href="#%e7%ac%a6%e5%8f%b7%e9%97%ae%e9%a2%98%e5%ae%9a%e4%b9%89" aria-label="ç¬¦å·&amp;amp;é—®é¢˜å®šä¹‰">ç¬¦å·&amp;é—®é¢˜å®šä¹‰</a></li>
                    <li>
                        <a href="#%e9%97%ae%e9%a2%98%e6%a0%b8%e5%bf%83explore-exploit-dilemma" aria-label="é—®é¢˜æ ¸å¿ƒ(Explore-exploit dilemma)">é—®é¢˜æ ¸å¿ƒ(Explore-exploit dilemma)</a></li>
                    <li>
                        <a href="#%e9%97%ae%e9%a2%98%e7%ae%97%e6%b3%95" aria-label="é—®é¢˜ç®—æ³•">é—®é¢˜ç®—æ³•</a><ul>
                            
                    <li>
                        <a href="#action-value-methods" aria-label="Action-Value Methods">Action-Value Methods</a></li>
                    <li>
                        <a href="#epsilon--greedy-action-selection" aria-label="$\epsilon$- Greedy Action Selection">$\epsilon$- Greedy Action Selection</a></li>
                    <li>
                        <a href="#non-stationary-problem" aria-label="Non-Stationary Problem">Non-Stationary Problem</a></li>
                    <li>
                        <a href="#stochastic-approximation-convergence-conditions" aria-label="Stochastic Approximation Convergence Conditions">Stochastic Approximation Convergence Conditions</a></li>
                    <li>
                        <a href="#upper-confidence-bound-ucb" aria-label="Upper Confidence Bound (UCB)">Upper Confidence Bound (UCB)</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8bmdps" aria-label="é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDPs)">é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDPs)</a><ul>
                            
                    <li>
                        <a href="#%e7%90%86%e8%ae%ba%e6%a1%86%e6%9e%b6" aria-label="ç†è®ºæ¡†æ¶">ç†è®ºæ¡†æ¶</a></li>
                    <li>
                        <a href="#%e6%a0%b8%e5%bf%83%e6%95%b0%e5%ad%a6%e6%a6%82%e5%bf%b5%e5%87%bd%e6%95%b0" aria-label="æ ¸å¿ƒæ•°å­¦æ¦‚å¿µ&amp;amp;å‡½æ•°">æ ¸å¿ƒæ•°å­¦æ¦‚å¿µ&amp;å‡½æ•°</a><ul>
                            
                    <li>
                        <a href="#policy" aria-label="Policy">Policy</a></li>
                    <li>
                        <a href="#returns%e5%9b%9e%e6%8a%a5" aria-label="Returnsï¼ˆå›æŠ¥ï¼‰">Returnsï¼ˆå›æŠ¥ï¼‰</a></li>
                    <li>
                        <a href="#value-functions" aria-label="Value Functions">Value Functions</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#%e5%8a%a8%e6%80%81%e8%a7%84%e5%88%92" aria-label="åŠ¨æ€è§„åˆ’">åŠ¨æ€è§„åˆ’</a><ul>
                            
                    <li>
                        <a href="#dp-algo1-policy-iteration" aria-label="DP Algo1: Policy Iteration">DP Algo1: Policy Iteration</a><ul>
                            
                    <li>
                        <a href="#iterative-policy-evaluation" aria-label="Iterative policy evaluation">Iterative policy evaluation</a></li>
                    <li>
                        <a href="#policy-improvement" aria-label="Policy Improvement">Policy Improvement</a></li></ul>
                    </li>
                    <li>
                        <a href="#dp-algo2-value-iteration" aria-label="DP Algo2: Value Iteration">DP Algo2: Value Iteration</a></li>
                    <li>
                        <a href="#asynchronous-and-generalised-dp" aria-label="Asynchronous and generalised DP">Asynchronous and generalised DP</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95" aria-label="è’™ç‰¹å¡æ´›æ–¹æ³•">è’™ç‰¹å¡æ´›æ–¹æ³•</a></li>
                    <li>
                        <a href="#temporal-difference-learning" aria-label="Temporal Difference Learning">Temporal Difference Learning</a><ul>
                            
                    <li>
                        <a href="#td-policy-evaluation" aria-label="TD policy evaluation">TD policy evaluation</a></li>
                    <li>
                        <a href="#td-control" aria-label="TD control">TD control</a><ul>
                            
                    <li>
                        <a href="#sarsa" aria-label="Sarsa">Sarsa</a></li>
                    <li>
                        <a href="#q-learning" aria-label="Q-learning">Q-learning</a></li></ul>
                    </li>
                    <li>
                        <a href="#n-steps-td-methods" aria-label="n-steps TD methods">n-steps TD methods</a></li></ul>
                    </li>
                    <li>
                        <a href="#planning-and-learning" aria-label="Planning and Learning">Planning and Learning</a></li>
                    <li>
                        <a href="#value-function-approximation" aria-label="Value function approximation">Value function approximation</a></li>
                    <li>
                        <a href="#policy-gradient-methods" aria-label="Policy Gradient Methods">Policy Gradient Methods</a></li>
                    <li>
                        <a href="#reference" aria-label="Reference">Reference</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>
  <div class="post-content"><h1 id="å¤šè‡‚è€è™æœºé—®é¢˜mab">å¤šè‡‚è€è™æœºé—®é¢˜(MAB)<a hidden class="anchor" aria-hidden="true" href="#å¤šè‡‚è€è™æœºé—®é¢˜mab">#</a></h1>
<h2 id="ç¬¦å·é—®é¢˜å®šä¹‰">ç¬¦å·&amp;é—®é¢˜å®šä¹‰<a hidden class="anchor" aria-hidden="true" href="#ç¬¦å·é—®é¢˜å®šä¹‰">#</a></h2>
<ul>
<li>å¤§å†™æ–œä½“è¡¨ç¤ºéšæœºå˜é‡ï¼Œä¾‹å¦‚$A, R, A_t, R_t$</li>
<li>å°å†™å­—æ¯è¡¨ç¤ºè¿™äº›éšæœºå˜é‡çš„å®ç°ï¼Œä¾‹å¦‚$a, r, a_t, r_t, Pr${$A_t=a_t$}</li>
<li>èŠ±ä½“ï¼ŒåŒºé—´ç­‰è¡¨ç¤ºé›†åˆï¼Œä¾‹å¦‚$\mathcal{A}, [0, 1], \mathbb{N}$</li>
</ul>
<p>Given: a set of k actions, $\mathcal{A}$, number of rounds T.
Repeat for t in T rounds:</p>
<ol>
<li>Algorithm selects arm $A_t \in \mathcal{A}$</li>
<li>Algorithm observes reward $R_t \in [0, 1]$
Goal: maximise expected total reward.</li>
</ol>
<p>é¢„æœŸçš„å¥–åŠ±è¢«ç§°ä¸ºValueï¼š$q_*(a) = \mathbb{E}[R_t \mid A_t = a]$</p>
<h2 id="é—®é¢˜æ ¸å¿ƒexplore-exploit-dilemma">é—®é¢˜æ ¸å¿ƒ(Explore-exploit dilemma)<a hidden class="anchor" aria-hidden="true" href="#é—®é¢˜æ ¸å¿ƒexplore-exploit-dilemma">#</a></h2>
<p>ç”±äºå„ä¸ªè€è™æœºçš„å¥–åŠ±åˆ†å¸ƒæœªçŸ¥ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¢ç´¢å’Œåˆ©ç”¨ä¹‹é—´åšå‡ºæƒè¡¡ï¼Œæ¥æœ€å¤§åŒ–é•¿æœŸå¥–åŠ±ã€‚</p>
<p>æ¢ç´¢ï¼ˆExplorationï¼‰ å’Œ åˆ©ç”¨ï¼ˆExploitationï¼‰ ä¹‹é—´çš„æƒè¡¡æ˜¯ MAB é—®é¢˜çš„æ ¸å¿ƒï¼š</p>
<ul>
<li>Exploration: é€‰æ‹©å½“å‰ä¿¡æ¯ä¸è¶³çš„é€‰é¡¹ï¼Œä»¥è·å¾—å…³äºå¥–åŠ±åˆ†å¸ƒçš„æ›´å¤šçŸ¥è¯†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæŸä¸ªè€è™æœºçš„å¥–åŠ±ä¸ç¡®å®šï¼Œä½ å¯èƒ½ä¼šå°è¯•æ‹‰å–å®ƒï¼Œä»¥æ”¶é›†æ›´å¤šæ•°æ®ã€‚</li>
<li>Exploitation: é€‰æ‹©å½“å‰ä¼°è®¡å¥–åŠ±æœ€é«˜çš„é€‰é¡¹ï¼Œä»¥æœ€å¤§åŒ–å³æ—¶æ”¶ç›Šã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ å·²ç»çŸ¥é“æŸä¸ªè€è™æœºçš„å¹³å‡å¥–åŠ±æœ€é«˜ï¼Œä½ ä¼šæ›´å€¾å‘äºæŒç»­æ‹‰å–å®ƒã€‚</li>
</ul>
<p>è¿™ç§æƒè¡¡çš„æ ¸å¿ƒéš¾ç‚¹åœ¨äºï¼š</p>
<ul>
<li>è¿‡åº¦æ¢ç´¢ï¼ˆè¿‡å¤šå°è¯•æœªçŸ¥é€‰é¡¹ï¼‰å¯èƒ½å¯¼è‡´è¾ƒä½çš„çŸ­æœŸæ”¶ç›Šã€‚</li>
<li>è¿‡åº¦åˆ©ç”¨ï¼ˆè¿‡æ—©é”å®šæŸä¸ªé€‰é¡¹ï¼‰å¯èƒ½å¯¼è‡´é”™å¤±æ›´ä¼˜çš„é•¿æœŸæ”¶ç›Šã€‚</li>
</ul>
<h2 id="é—®é¢˜ç®—æ³•">é—®é¢˜ç®—æ³•<a hidden class="anchor" aria-hidden="true" href="#é—®é¢˜ç®—æ³•">#</a></h2>
<h3 id="action-value-methods">Action-Value Methods<a hidden class="anchor" aria-hidden="true" href="#action-value-methods">#</a></h3>
<p>æˆ‘ä»¬å¯ä»¥ä¼°è®¡æŸä¸ªåŠ¨ä½œ$a$æ‰€è·å¾—å¥–åŠ±çš„æ ·æœ¬å‡å€¼ã€‚ï¼Œå…·ä½“å‡½æ•°è¡¨ç¤ºä¸ºï¼š
$$
Q_t(a) = \frac{\text{Sum of rewards when taken a so far}}{\text{Number of times taken a so far}}
$$
$$
Q_t(a) = \frac{1}{N_t(a)} \sum^{t-1}_{\tau=1}R _ {\tau=1} \cdot \mathbb{1} _ {A_t=a}
$$
å…¶ä¸­</p>
<ul>
<li>$N_t(a)$ æ˜¯åˆ°å½“å‰æ—¶é—´ $t$ ä¸ºæ­¢ï¼ŒåŠ¨ä½œ $a$ è¢«é€‰æ‹©çš„æ¬¡æ•°ã€‚</li>
<li>$R_{\tau}$ æ˜¯ç¬¬ $\tau$ æ¬¡æ‰§è¡ŒåŠ¨ä½œçš„å¥–åŠ±ã€‚</li>
<li>$\mathbb{1} _ {A_{\tau} = a}$ æ˜¯ä¸€ä¸ª<strong>æŒ‡ç¤ºå‡½æ•°</strong>ï¼Œå½“ $\tau$ æ—¶åˆ»é€‰æ‹©äº†åŠ¨ä½œ $a$ æ—¶ï¼Œå®ƒçš„å€¼ä¸º 1ï¼Œå¦åˆ™ä¸º 0ã€‚</li>
</ul>
<p><strong>å¦‚æœä¸€ä¸ªåŠ¨ä½œè¢«é€‰å–æ— ç©·æ¬¡</strong>ï¼Œé‚£ä¹ˆå®ƒçš„<strong>æ ·æœ¬å‡å€¼ä¼šæ”¶æ•›</strong>åˆ°çœŸå®çš„æœŸæœ›å¥–åŠ±ï¼Œå³ï¼š
$$
\lim_{N_t(a) \to \infty} Q_t(a) = q_*(a)
$$</p>
<h3 id="epsilon--greedy-action-selection">$\epsilon$- Greedy Action Selection<a hidden class="anchor" aria-hidden="true" href="#epsilon--greedy-action-selection">#</a></h3>
<p>æ ¹æ®ä¸Šé¢çš„å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠexploitè¡¨ç¤ºä¸º
$$
A_t = A_t^* = \mathop{\arg\max}\limits_{a} Q_t(a)
$$
exploration(éšæœºé€‰æ‹©action)è¡¨ç¤ºä¸º
$$
A_t\sim\text{Unif}(\mathcal{A})
$$</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> epsilon_greedy(Q, epsilon, actions):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> np.random.rand() &lt; epsilon:
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># ä»¥æ¦‚ç‡ epsilon è¿›è¡Œæ¢ç´¢</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> np.random.choice(actions)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># ä»¥æ¦‚ç‡ (1 - epsilon) é€‰æ‹©å½“å‰æœ€ä¼˜åŠ¨ä½œ</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> np.argmax(Q)
</span></span><span style="display:flex;"><span>    self.execute(A_t)
</span></span><span style="display:flex;"><span>    self.observe(R_t)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    self.update(N_t_a, Q_t_a)
</span></span></code></pre></div><p>å¦‚ä½•åœ¨é€’å¢çš„åŒæ—¶é¿å…é‡å¤è®¡ç®—ï¼Ÿ
$$
Q_n = \frac{R_1 + &hellip; + R_{n-1}}{n-1}
$$
$$
Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]
$$
è¿™ä¹Ÿæ˜¯å¼ºåŒ–å­¦ä¹ ä¸­update rulesçš„æ ‡å‡†å½¢å¼
$$
\text{NewEstimate &lt;â€” OldEstimate + StepSize[Target - OldEstimate]}
$$</p>
<h3 id="non-stationary-problem">Non-Stationary Problem<a hidden class="anchor" aria-hidden="true" href="#non-stationary-problem">#</a></h3>
<p>å‡è®¾true action valueä¼šéšæ—¶é—´è€Œå˜åŒ–ï¼Œè¿™æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­ç»å¸¸é‡åˆ°çš„non-stationary problemï¼Œè¿™ä¸ªæ—¶å€™å†ç”¨sample averageå°±ä¸åˆé€‚äº†ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªstep-size parameter $\alpha\in[0, 1]$æ¥è·Ÿè¸ªaction valueï¼Œé‚£ä¹ˆæ›´æ–°å‡½æ•°å˜ä¸º
$$
Q_{n+1} = Q_n + \alpha[R_n - Q_n]
$$</p>
<h3 id="stochastic-approximation-convergence-conditions">Stochastic Approximation Convergence Conditions<a hidden class="anchor" aria-hidden="true" href="#stochastic-approximation-convergence-conditions">#</a></h3>
<p><strong>éšæœºé€¼è¿‘ï¼ˆStochastic Approximation, SAï¼‰</strong> æ˜¯ä¸€ç§ç”¨äºåœ¨å™ªå£°ç¯å¢ƒä¸­è¿­ä»£é€¼è¿‘æœ€ä¼˜è§£çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å¸¸ç”¨äºæœºå™¨å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚ Q-learningï¼‰ã€ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚éšæœºæ¢¯åº¦ä¸‹é™ï¼‰ç­‰ã€‚</p>
<p>åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œ<strong>å€¼å‡½æ•°æ›´æ–°</strong> é‡‡ç”¨éšæœºé€¼è¿‘çš„æ–¹å¼ï¼Œä¾‹å¦‚ï¼š
$$
Q_{t+1}(s, a) = Q_t(s, a) + \alpha_t [R_t + \gamma \max_{a&rsquo;} Q_t(s&rsquo;, a&rsquo;) - Q_t(s, a)]
$$
å…¶ä¸­ $\alpha_t$ æ˜¯å­¦ä¹ ç‡ï¼Œç›®æ ‡æ˜¯è®© $Q_t(s,a)$ æ”¶æ•›åˆ°æœ€ä¼˜å€¼ $Q^*(s,a)$ã€‚</p>
<p>ä¸ºäº†ä¿è¯ <strong>éšæœºé€¼è¿‘ç®—æ³•</strong> èƒ½å¤Ÿæ”¶æ•›ï¼Œå¿…é¡»æ»¡è¶³<strong>æ”¶æ•›æ¡ä»¶ï¼ˆConvergence Conditionsï¼‰</strong>:</p>
<p><strong>å­¦ä¹ ç‡æ»¡è¶³ Robbins-Monro æ¡ä»¶</strong>
$$
\sum_{t=1}^{\infty} \alpha_t = \infty, \quad \sum_{t=1}^{\infty} \alpha_t^2 &lt; \infty
$$</p>
<ul>
<li><strong>ç¬¬ä¸€é¡¹</strong> $ \sum \alpha_t = \infty $ ç¡®ä¿ç®—æ³•åœ¨<strong>é•¿æœŸå†…ç»§ç»­å­¦ä¹ </strong>ï¼Œå¦åˆ™ç®—æ³•å¯èƒ½è¿‡æ—©åœæ­¢æ›´æ–°ã€‚</li>
<li><strong>ç¬¬äºŒé¡¹</strong> $ \sum \alpha_t^2 &lt; \infty $ ç¡®ä¿å­¦ä¹ ç‡æœ€ç»ˆ<strong>è¶³å¤Ÿå°</strong>ï¼Œå¦åˆ™æ”¶æ•›ä¼šå—å™ªå£°å½±å“ã€‚</li>
</ul>
<p>ç¤ºä¾‹ï¼š</p>
<ul>
<li><strong>æ»¡è¶³æ¡ä»¶çš„å­¦ä¹ ç‡ï¼š</strong> $ \alpha_t = \frac{1}{t} $ã€$ \alpha_t = \frac{1}{\sqrt{t}} $</li>
<li><strong>ä¸æ»¡è¶³æ¡ä»¶çš„å­¦ä¹ ç‡ï¼š</strong> å›ºå®šçš„ $ \alpha_t = 0.1 $ï¼ˆå› ä¸ºæ±‚å’Œä¸å‘æ•£ï¼‰</li>
</ul>
<h3 id="upper-confidence-bound-ucb">Upper Confidence Bound (UCB)<a hidden class="anchor" aria-hidden="true" href="#upper-confidence-bound-ucb">#</a></h3>
<p>ç›¸æ¯”äº <strong>$\epsilon$-Greedy</strong> ç®—æ³•çš„éšæœºæ¢ç´¢ç­–ç•¥ï¼ŒUCB é€šè¿‡æ„é€ ä¸€ä¸ªä¸Šç½®ä¿¡ç•Œï¼ˆUpper Confidence Boundï¼‰æ¥è¿›è¡Œæ¢ç´¢ï¼Œä»è€Œæ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚</p>
<p>UCB é‡‡ç”¨ <strong>ç½®ä¿¡åŒºé—´ï¼ˆconfidence boundï¼‰</strong> æ¥å†³å®šæ˜¯å¦æ¢ç´¢æŸä¸ªåŠ¨ä½œï¼š</p>
<ul>
<li><strong>åˆ©ç”¨ï¼ˆExploitationï¼‰ï¼š</strong> é€‰æ‹©å†å²ä¸Šå¹³å‡å¥–åŠ±æœ€é«˜çš„åŠ¨ä½œï¼ˆå³å½“å‰æœ€ä¼˜åŠ¨ä½œï¼‰ã€‚</li>
<li><strong>æ¢ç´¢ï¼ˆExplorationï¼‰ï¼š</strong> é€‰æ‹©é‚£äº›å°è¯•æ¬¡æ•°è¾ƒå°‘ã€ä¸ç¡®å®šæ€§è¾ƒå¤§çš„åŠ¨ä½œã€‚</li>
</ul>
<p><strong>å…¬å¼ï¼š</strong>
$$
A_t = \arg\max_{a} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
$$
å…¶ä¸­ï¼š</p>
<ul>
<li>$ Q_t(a) $ æ˜¯åŠ¨ä½œ $ a $ åœ¨æ—¶é—´ $ t $ æ—¶åˆ»çš„å¹³å‡å¥–åŠ±ï¼ˆå†å²ç»éªŒï¼‰ã€‚</li>
<li>$ N_t(a) $ æ˜¯<strong>åŠ¨ä½œ $ a $ è¢«é€‰æ‹©çš„æ¬¡æ•°</strong>ã€‚</li>
<li>$ t $ æ˜¯å½“å‰æ—¶é—´æ­¥ï¼ˆå½“å‰å®éªŒçš„æ€»æ¬¡æ•°ï¼‰ã€‚</li>
<li>$ c $ æ˜¯<strong>æ¢ç´¢å‚æ•°</strong>ï¼Œæ§åˆ¶æ¢ç´¢ç¨‹åº¦ï¼ˆé€šå¸¸ $ c $ å– $\sqrt{2}$ï¼‰ã€‚</li>
<li>$ \ln t $ æ˜¯å¯¹æ•°é¡¹ï¼Œä½¿å¾—æ¢ç´¢éšæ—¶é—´å‡å°‘ã€‚</li>
</ul>
<p><strong>è§£é‡Šï¼š</strong></p>
<ul>
<li>$ Q_t(a) $ ä»£è¡¨ <strong>åˆ©ç”¨</strong>ï¼ˆExploitationï¼‰ï¼šå€¾å‘äºé€‰æ‹©å†å²ä¸Šè¡¨ç°æœ€å¥½çš„åŠ¨ä½œã€‚</li>
<li>$ \sqrt{\frac{\ln t}{N_t(a)}} $ ä»£è¡¨ <strong>æ¢ç´¢</strong>ï¼ˆExplorationï¼‰ï¼šå½“æŸä¸ªåŠ¨ä½œé€‰æ‹©æ¬¡æ•° ( N_t(a) ) å¾ˆå°æ—¶ï¼Œè¯¥é¡¹è¾ƒå¤§ï¼Œé¼“åŠ±æ¢ç´¢ã€‚</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> numpy <span style="color:#fff;font-weight:bold">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> UCB(Q, N, t, c=<span style="color:#ff0;font-weight:bold">1.0</span>):
</span></span><span style="display:flex;"><span>    ucb_values = Q + c * np.sqrt(np.log(t) / (N + <span style="color:#ff0;font-weight:bold">1e-5</span>))  <span style="color:#007f7f"># é¿å…é™¤é›¶é”™è¯¯</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> np.argmax(ucb_values)  <span style="color:#007f7f"># é€‰æ‹© UCB å€¼æœ€é«˜çš„åŠ¨ä½œ</span>
</span></span></code></pre></div><p>å…¶ä¸­ï¼š</p>
<ul>
<li><code>Q</code>ï¼šå­˜å‚¨æ¯ä¸ªåŠ¨ä½œçš„<strong>å¹³å‡å¥–åŠ±</strong>ã€‚</li>
<li><code>N</code>ï¼šå­˜å‚¨æ¯ä¸ªåŠ¨ä½œçš„<strong>é€‰æ‹©æ¬¡æ•°</strong>ã€‚</li>
<li><code>t</code>ï¼šå½“å‰æ—¶é—´æ­¥æ•°ï¼ˆæ€»å®éªŒæ¬¡æ•°ï¼‰ã€‚</li>
<li><code>c</code>ï¼šæ¢ç´¢å‚æ•°ï¼Œæ§åˆ¶æ¢ç´¢å¼ºåº¦ã€‚</li>
</ul>
<h1 id="é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹mdps">é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDPs)<a hidden class="anchor" aria-hidden="true" href="#é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹mdps">#</a></h1>
<h2 id="ç†è®ºæ¡†æ¶">ç†è®ºæ¡†æ¶<a hidden class="anchor" aria-hidden="true" href="#ç†è®ºæ¡†æ¶">#</a></h2>
<p>é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹åŒ…æ‹¬çŠ¶æ€ç©ºé—´$\mathcal{S}$ï¼ŒåŠ¨ä½œç©ºé—´$\mathcal{A}$ï¼Œå¥–åŠ±ç©ºé—´$\mathcal{R}$ã€‚MDPæ˜¯æœ‰é™é›†ï¼Œå¦‚æœ$\mathcal{S}$, $\mathcal{A}$, $\mathcal{R}$éƒ½æ˜¯æœ‰é™é›†</p>
<p>é©¬å°”å¯å¤«æ€§è´¨ï¼šç®€å•è¯´å°±æ˜¯æœªæ¥çš„çŠ¶æ€åªä¸å½“å‰ä¸€ä¸ªçŠ¶æ€æœ‰å…³ï¼Œç‹¬ç«‹äºè¿‡å»çš„æ‰€æœ‰çŠ¶æ€ï¼ˆFuture state and reward are independent of past states and actions, given the current state and actionï¼‰</p>
<p>$Pr${$S_{t+1},R_{t+1} | S_t,A_t,S_{tâˆ’1},A_{tâˆ’1},&hellip;,S_0,A_0$}$ = Pr${$S_{t+1},R_{t+1} | S_t,A_t$}</p>
<p>çŠ¶æ€$S_t$æ˜¯äº¤äº’å†å²çš„å……åˆ†æ€»ç»“ã€‚è®¾è®¡compact Markov statesæ˜¯RLä¸­çš„ä¸€é¡¹å·¥ç¨‹å·¥ä½œã€‚</p>
<p>ä»¥recycling robotä¸ºä¾‹ã€‚</p>
<ul>
<li>States
<ul>
<li>high battery level</li>
<li>low battery level</li>
</ul>
</li>
<li>Actions
<ul>
<li>search for can</li>
<li>wait for someone to bring can</li>
<li>recharge battery at charging station</li>
</ul>
</li>
<li>Rewards: number of cans collected</li>
</ul>
<table>
<thead>
<tr>
<th>$s$</th>
<th>$a$</th>
<th>$s'$</th>
<th>$p(s&rsquo;\mid s, a)$</th>
<th>$r(s, a, s&rsquo;)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>high</td>
<td>search</td>
<td>high</td>
<td>$\alpha$</td>
<td>$r_{\text{research}}$</td>
</tr>
<tr>
<td>high</td>
<td>search</td>
<td>low</td>
<td>$1 - \alpha$</td>
<td>$r_{\text{research}}$</td>
</tr>
<tr>
<td>low</td>
<td>search</td>
<td>high</td>
<td>$1 - \beta$</td>
<td>-3</td>
</tr>
<tr>
<td>low</td>
<td>search</td>
<td>low</td>
<td>$\beta$</td>
<td>$r_{\text{research}}$</td>
</tr>
<tr>
<td>high</td>
<td>wait</td>
<td>high</td>
<td>1</td>
<td>$r_{\text{wait}}$</td>
</tr>
<tr>
<td>high</td>
<td>wait</td>
<td>low</td>
<td>0</td>
<td>-</td>
</tr>
<tr>
<td>low</td>
<td>wait</td>
<td>high</td>
<td>0</td>
<td>-</td>
</tr>
<tr>
<td>low</td>
<td>wait</td>
<td>low</td>
<td>1</td>
<td>$r_{\text{wait}}$</td>
</tr>
<tr>
<td>high</td>
<td>recharge</td>
<td>high</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>high</td>
<td>recharge</td>
<td>low</td>
<td>0</td>
<td>-</td>
</tr>
</tbody>
</table>
<h2 id="æ ¸å¿ƒæ•°å­¦æ¦‚å¿µå‡½æ•°">æ ¸å¿ƒæ•°å­¦æ¦‚å¿µ&amp;å‡½æ•°<a hidden class="anchor" aria-hidden="true" href="#æ ¸å¿ƒæ•°å­¦æ¦‚å¿µå‡½æ•°">#</a></h2>
<h3 id="policy">Policy<a hidden class="anchor" aria-hidden="true" href="#policy">#</a></h3>
<p>é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ç”±policyæ§åˆ¶ã€‚</p>
<p>$\pi(a\mid s) = $ åœ¨çŠ¶æ€$s$çš„æƒ…å†µä¸‹é€‰æ‹©åŠ¨ä½œ$a$çš„æ¦‚ç‡</p>
<table>
<thead>
<tr>
<th>$\pi(a\mid s)$</th>
<th>search</th>
<th>wait</th>
<th>recharge</th>
</tr>
</thead>
<tbody>
<tr>
<td>high</td>
<td>0.9</td>
<td>0.1</td>
<td>0</td>
</tr>
<tr>
<td>low</td>
<td>0.2</td>
<td>0.3</td>
<td>0.5</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Agentçš„ç›®æ ‡å°±æ˜¯å­¦ä¸€ä¸ªèƒ½æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±çš„ç­–ç•¥</p>
</blockquote>
<h3 id="returnså›æŠ¥">Returnsï¼ˆå›æŠ¥ï¼‰<a hidden class="anchor" aria-hidden="true" href="#returnså›æŠ¥">#</a></h3>
<p><strong>Total Return</strong></p>
<p>ç­–ç•¥åº”è¯¥æœ€å¤§åŒ–é¢„æœŸå›æŠ¥ï¼ˆ$G_t$ï¼‰
$$
G_t = R_{t+1} + R_{t+2} + &hellip; + R_{T} = R_{t+1} + G_{t+1}
$$
where $T$ is final time step</p>
<p>Assumes terminating episodes: é€‚ç”¨äºå­˜åœ¨æ˜ç¡®ç»ˆæ­¢æ¡ä»¶çš„ä»»åŠ¡ã€‚ä¾‹å¦‚åœ¨è±¡æ£‹æ¸¸æˆä¸­ï¼Œä¸€ä¸ªç©å®¶è·èƒœå°±ç»ˆæ­¢ã€‚é€šè¿‡è®¾ç½®å…è®¸çš„æ—¶é—´æ­¥æ•°æ¥å¼ºåˆ¶ç»ˆæ­¢</p>
<p><strong>Discounted Return</strong></p>
<p>å¯¹äºä¸èƒ½ç»ˆæ­¢çš„ï¼ˆæ— é™çš„ï¼‰åœºæ™¯ï¼Œå¯ä»¥ä½¿ç”¨æŠ˜æ‰£ç‡ $\gamma \in [0, 1)$
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + &hellip; = R_{t+1} + \gamma G_{t+1}
$$
$\gamma$æ˜¯æŠ˜æ‰£å› å­ï¼Œç”¨äºé™ä½è¿œæœŸå¥–åŠ±çš„ä»·å€¼ã€‚</p>
<p>è¿™é€‚ç”¨äºæ— é™å›åˆä»»åŠ¡ï¼Œä¾‹å¦‚è‚¡ç¥¨äº¤æ˜“ç­–ç•¥ï¼Œæ™ºèƒ½ä½“æŒç»­è¿›è¡Œäº¤æ˜“ï¼Œæ²¡æœ‰å›ºå®šç»“æŸæ—¶é—´ã€‚</p>
<h3 id="value-functions">Value Functions<a hidden class="anchor" aria-hidden="true" href="#value-functions">#</a></h3>
<p>è¿™å¼ å›¾ç»™ä¸€ä¸ªoverview
<img loading="lazy" src="/img/rl-note/value_function.png" alt="main quantities"  />

<strong>Bellman Equation</strong>
åœ¨å¼ºåŒ–å­¦ä¹ å’Œé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­ï¼Œè´å°”æ›¼æ–¹ç¨‹ç”¨äºæè¿°çŠ¶æ€å€¼å‡½æ•°å’ŒåŠ¨ä½œå€¼å‡½æ•°çš„é€’å½’å…³ç³»ã€‚è¿™ä¸¤æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­æ±‚è§£æœ€ä¼˜ç­–ç•¥çš„é‡è¦æ•°å­¦å·¥å…·ã€‚</p>
<p><strong>State Value Function and the Bellman Equation</strong></p>
<p>æ ¹æ®Markov propertyï¼Œå¯ä»¥æŠŠçŠ¶æ€ä»·å€¼å‡½æ•°å†™æˆBellman Equationçš„é€’å½’å½¢å¼ã€‚
$$
v_\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s]
$$
è¿™ä¸ªå…¬å¼çš„æ„æ€å°±æ˜¯ç»™å®šå½“å‰çŠ¶æ€çš„æƒ…å†µä¸‹ï¼Œç­–ç•¥$\pi$å¯ä»¥è·å¾—çš„å›æŠ¥æœŸæœ›å€¼</p>
<p>å°†å…¬å¼å±•å¼€ï¼Œå¾—åˆ°ï¼š</p>
<p>$$
v_\pi(s) = \sum_a \pi(a \mid s)r(s, a) + \gamma\sum_{s&rsquo; \in S} p(s&rsquo;\mid s, a)\cdot v_\pi(s&rsquo;)
$$</p>
<p>å…¶ä¸­$\sum_a \pi(a \mid s)r(s, a)$è¡¨ç¤ºå³æ—¶å¥–åŠ±ï¼ˆé€‰æ‹©åŠ¨ä½œ$a$çš„æ¦‚ç‡$\times$è¿™æ ·åšå¸¦æ¥çš„å¥–åŠ±å€¼ï¼‰ï¼Œ$\gamma$æ˜¯æŠ˜æ‰£å› å­ï¼Œ$\sum_{s&rsquo; \in S} p(s&rsquo;\mid s, a)\cdot v_\pi(s&rsquo;)$è¡¨ç¤ºexpected future value (å½“å‰çŠ¶æ€$s$, é‡‡å–åŠ¨ä½œ$a$ä¹‹åï¼Œæœ‰å¤šå¤§æ¦‚ç‡è½¬åˆ°æ¯ä¸ªå¯èƒ½çš„$s&rsquo;$,æ¯ä¸ª$s&rsquo;$ä¼šå¸¦æ¥å¤šå¤§ä»·å€¼ï¼Œå…¨éƒ¨åŠ æƒæ±‚å’Œåå°±æ˜¯æœªæ¥çš„é¢„æœŸä»·å€¼)</p>
<p>ç›´è§‰ç†è§£ï¼šçœ¼å‰æ”¶ç›Š+æœªæ¥æ”¶ç›Šï¼ˆå¸¦æœ‰æŠ˜æ‰£ç‡æ¥é™ä½å½±å“ï¼‰</p>
<p><strong>Action Value Functionçš„bellmanå½¢å¼</strong>
$$
q_\pi(s, a) = r(s, a) + \gamma \sum_{s&rsquo; \in S} p(s&rsquo; \mid s, a)\cdot v_\pi(s&rsquo;)
$$
ç”¨æœŸæœ›ç®€å†™
$$
q_\pi(s, a) = r(s, a) + \gamma \mathbb{E} _ {s&rsquo;}[ v _ \pi(s&rsquo;) ]
$$</p>
<p>State valueå’Œaction valueæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ</p>
<ul>
<li>state valueæ˜¯åœ¨æŸä¸ªçŠ¶æ€ä¸‹ï¼ŒæŒ‰ç…§ç­–ç•¥$\pi$èµ°ä¸‹å»ï¼Œæœªæ¥çš„æœŸæœ›æ€»å¥–åŠ±</li>
<li>action valueæ˜¯åœ¨æŸä¸ªçŠ¶æ€ä¸‹ï¼Œæ‰§è¡ŒåŠ¨ä½œ$a$ï¼Œå†æŒ‰ç…§ç­–ç•¥$\pi$èµ°ä¸‹å»ï¼Œæœªæ¥çš„æœŸæœ›æ€»å¥–åŠ±ã€‚</li>
</ul>
<blockquote>
<p>ç­–ç•¥å’ŒåŠ¨ä½œçš„åŒºåˆ«ï¼šç­–ç•¥ä¸æ˜¯åŠ¨ä½œçš„é›†åˆï¼Œè€Œæ˜¯ä¸€ä¸ªâ€œä»çŠ¶æ€åˆ°åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒâ€çš„æ˜ å°„å‡½æ•°ã€‚</p>
</blockquote>
<p>å¦‚æœ
$$
v _ \pi(s) = v_*(s) = \max_{\pi&rsquo;}v_{\pi&rsquo;}(s)
$$</p>
<p>å¹¶ä¸”</p>
<p>$$
q_{\pi}(s, a) = q_*(s, a) = \max_{\pi&rsquo;}q_{\pi&rsquo;}(s, a)
$$
é‚£ä¹ˆç­–ç•¥$\pi$å°±æ˜¯æœ€ä¼˜çš„ã€‚</p>
<p>å¦‚ä½•è®¡ç®—å¹¶æ‰¾åˆ°è¿™ä¸ªæœ€ä¼˜ç­–ç•¥ï¼Ÿæ¶‰åŠåˆ°åŠ¨æ€è§„åˆ’ä¸­çš„policy iterationå’Œvalue iterationï¼š</p>
<h1 id="åŠ¨æ€è§„åˆ’">åŠ¨æ€è§„åˆ’<a hidden class="anchor" aria-hidden="true" href="#åŠ¨æ€è§„åˆ’">#</a></h1>
<p>åŠ¨æ€è§„åˆ’æ ¸å¿ƒæ€æƒ³ï¼šuse Bellman Equations to organise search for good policies</p>
<h2 id="dp-algo1-policy-iteration">DP Algo1: Policy Iteration<a hidden class="anchor" aria-hidden="true" href="#dp-algo1-policy-iteration">#</a></h2>
<p>è¿™ä¸ªç®—æ³•åŒ…å«ä¸¤ä¸ªé˜¶æ®µ:</p>
<ul>
<li>ç­–ç•¥è¯„ä¼°ï¼šç»™å®šç­–ç•¥$\pi$, è®¡ç®—å¯¹åº”çš„state value function $v_\pi(s)$ï¼› è§£å†³Bellmanæ–¹ç¨‹ï¼Œå¯è¿­ä»£é€¼è¿‘</li>
<li>ç­–ç•¥æå‡ï¼šç”¨å½“å‰çš„$v_\pi(s)$æ¥æ›´æ–°ç­–ç•¥ï¼Œå¦‚æœæ–°ç­–ç•¥ = æ—§ç­–ç•¥ï¼Œè¯´æ˜æœ€ä¼˜ï¼Œåœæ­¢ã€‚
$$
\pi_{\text{new}}(s) = \arg \max_a \sum_{s&rsquo;}p(s&rsquo;\mid s, a)[r(s, a) + \gamma v_\pi(s&rsquo;)]
$$</li>
</ul>
<p>å…·ä½“è¡¨ç¤ºå¦‚ä¸‹ï¼ˆè¿™ä¸ªè¿‡ç¨‹ä¼šå¾ˆå¿«æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ï¼‰
$$
\pi_0 \xrightarrow{E} v_{\pi_0} \xrightarrow{I} \pi_1 \xrightarrow{E} v_{\pi_1} \xrightarrow{I} &hellip; \xrightarrow{I} \pi_* \xrightarrow{E} v_*
$$</p>
<h3 id="iterative-policy-evaluation">Iterative policy evaluation<a hidden class="anchor" aria-hidden="true" href="#iterative-policy-evaluation">#</a></h3>
<p>ä¼ªä»£ç å¦‚ä¸‹</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>Input: pi, the policy to be evaluated
</span></span><span style="display:flex;"><span>Initialize an array V(s) = 0, for all s
</span></span><span style="display:flex;"><span>Repeat
</span></span><span style="display:flex;"><span>    delta &lt;- 0
</span></span><span style="display:flex;"><span>    For each s in S:
</span></span><span style="display:flex;"><span>        v &lt;- V(s)
</span></span><span style="display:flex;"><span>        V(s) &lt;- State Value Function
</span></span><span style="display:flex;"><span>        delta &lt;- max(delta, |v - V(s)|)
</span></span><span style="display:flex;"><span>until delta &lt; theta (a small positive number)
</span></span><span style="display:flex;"><span>Output V \approx= v_pi
</span></span></code></pre></div><h3 id="policy-improvement">Policy Improvement<a hidden class="anchor" aria-hidden="true" href="#policy-improvement">#</a></h3>
<p>è®¡ç®—åœ¨å½“å‰state value funtion $V(s)$ä¸‹ï¼Œæ¯ä¸ªçŠ¶æ€$s$å¯¹ä¸åŒåŠ¨ä½œ$a$çš„action valueã€‚ç„¶åé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ$\pi&rsquo;(s)$ï¼Œå¦‚æœè¿™ä¸ä¹‹å‰çš„$\pi(s)$ä¸åŒï¼Œåˆ™ç­–ç•¥å‘ç”Ÿäº†æ›´æ–°ï¼Œå¹¶ç»§ç»­è¿­ä»£ï¼Œå¦åˆ™åœæ­¢ã€‚</p>
<h2 id="dp-algo2-value-iteration">DP Algo2: Value Iteration<a hidden class="anchor" aria-hidden="true" href="#dp-algo2-value-iteration">#</a></h2>
<p>ä»¥ç§Ÿè½¦é—®é¢˜ä¸ºä¾‹ã€‚æœ‰ä¸¤ä¸ªè½¦è¾†ç§Ÿèµç‚¹ï¼Œæ ¹æ®åˆ†å¸ƒéšæœºè¯·æ±‚å’Œè¿”å›è½¦è¾†ã€‚çŠ¶æ€ï¼ˆStatesï¼‰è¡¨ç¤ºä¸º(n1, n2)ï¼Œå…¶ä¸­niæ˜¯ä½ç½®içš„æ±½è½¦æ•°é‡ï¼ˆæ¯ä¸ªåœ°æ–¹æœ€å¤š20è¾†ï¼‰ã€‚åŠ¨ä½œï¼ˆActionsï¼‰è¡¨ç¤ºä»ä¸€ä¸ªä½ç½®ç§»åŠ¨åˆ°å¦ä¸€ä¸ªä½ç½®çš„è½¦å­çš„æ•°é‡ã€‚å…¶ä¸­å¦‚æœæ˜¯ä»1ç§»åˆ°2å°±æ˜¯æ­£æ•°ï¼Œä»2ç§»åˆ°1å°±æ˜¯è´Ÿæ•°ï¼Œæœ€å¤š5è¾†ã€‚å¥–åŠ±ï¼ˆRewardsï¼‰è¡¨ç¤ºä¸ºæ¯ä¸ªæ—¶é—´æ­¥ä¸­ï¼Œç§Ÿå‡ºå»ä¸€è¾†è½¦+$10ï¼Œç§»åŠ¨ä¸€è¾†è½¦-$2ã€‚æœ€å$\gamma$ = 0.9ã€‚</p>
<p>Policy iterationä½¿ç”¨Bellman equationä½œä¸ºoperator:
$$
v_{k+1}(s) = \sum_a \pi(a\mid s)\sum_{s&rsquo;, r}p(s&rsquo;, r\mid s, a) [r+\gamma v_k(s&rsquo;)]
$$
å…¶ä¸­$s\in S$</p>
<p>Value iterationä½¿ç”¨Bellman optimality equationä½œä¸ºoperator:
$$
v_{k+1}(s) = \max_a \sum_{s&rsquo;, r}p(s&rsquo;, r\mid s, a) [r+\gamma v_k(s&rsquo;)]
$$
å…¶ä¸­$s\in S$</p>
<p>Value iterationçš„ä¼ªä»£ç å¦‚ä¸‹
<img loading="lazy" src="/img/rl-note/value_iteration.png" alt="value iteration"  />
</p>
<h2 id="asynchronous-and-generalised-dp">Asynchronous and generalised DP<a hidden class="anchor" aria-hidden="true" href="#asynchronous-and-generalised-dp">#</a></h2>
<p>ç›®å‰ä¸ºæ­¢ï¼ŒåŠ¨æ€è§„åˆ’æ–¹æ³•æ‰§è¡Œçš„æ˜¯ç©·ä¸¾ã€‚å¦‚æœçŠ¶æ€ç©ºé—´å¾ˆå¤§ï¼Œpolicy evaluation and improvementå°±ä¸å¯è®¡ç®—äº†ã€‚</p>
<p>å¼‚æ­¥åŠ¨æ€è§„åˆ’æ–¹æ³•é¿å…äº†æ ‡å‡†åŠ¨æ€è§„åˆ’ä¸­å¿…é¡»åŒæ—¶æ›´æ–°æ‰€æœ‰çŠ¶æ€çš„é™åˆ¶ï¼Œè€Œæ˜¯é€‰æ‹©æ€§åœ°æ›´æ–°æŸäº›çŠ¶æ€ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚</p>
<p>æ”¹è¿›ç‚¹ï¼š</p>
<ul>
<li>çŠ¶æ€å¼‚æ­¥æ›´æ–°ï¼šä¸è¦æ±‚æ‰€æœ‰çŠ¶æ€åœ¨åŒä¸€æ—¶é—´æ›´æ–°ï¼Œè€Œæ˜¯æŒ‰éœ€æ›´æ–°ä¸€éƒ¨åˆ†çŠ¶æ€ã€‚</li>
<li>ä¼˜å…ˆçº§æ›´æ–°ï¼šä¼˜å…ˆæ›´æ–°é‚£äº› ä»·å€¼å˜åŒ–è¾ƒå¤§æˆ–ä¸æœ€ç»ˆç­–ç•¥æœ€ç›¸å…³çš„çŠ¶æ€ï¼Œä½¿æœ‰ä»·å€¼çš„ä¿¡æ¯æ›´å¿«ä¼ æ’­ã€‚</li>
<li>åŠ é€Ÿæ”¶æ•›ï¼šå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤§è§„æ¨¡é—®é¢˜ï¼Œå¦‚æœºå™¨äººæ§åˆ¶ã€è·¯å¾„è§„åˆ’ã€å¼ºåŒ–å­¦ä¹ ç­‰ã€‚</li>
</ul>
<h1 id="è’™ç‰¹å¡æ´›æ–¹æ³•">è’™ç‰¹å¡æ´›æ–¹æ³•<a hidden class="anchor" aria-hidden="true" href="#è’™ç‰¹å¡æ´›æ–¹æ³•">#</a></h1>
<p>Monte Carloç­–ç•¥è¯„ä¼° - åœ¨æ²¡æœ‰æ¨¡å‹çš„æƒ…å†µä¸‹è¿›è¡Œç­–ç•¥ä»·å€¼è¯„ä¼°</p>
<p>MCä¸éœ€è¦çŸ¥é“ç¯å¢ƒçš„æ¨¡å‹$p(s&rsquo;, r \mid s, a)$ï¼Œåªéœ€è¦sampled episodesã€‚ä½ åªéœ€è¦èƒ½è·Ÿç¯å¢ƒç©ã€ç©å®Œæ•´åœºæ¸¸æˆï¼Œç„¶åè®°å½•æ¯ä¸ªçŠ¶æ€å‡ºç°åæœ€ç»ˆèµšäº†å¤šå°‘é’±ï¼Œ
æŠŠè¿™ä¸ªâ€œèµšçš„é’±â€åœ¨æ‰€æœ‰è®¿é—®è¿‡è¿™ä¸ªçŠ¶æ€çš„ episode é‡Œå¹³å‡ä¸€ä¸‹ï¼Œè¿™å°±æ˜¯å®ƒçš„å€¼$v_\pi(s)$ã€‚å…¬å¼å¯ä»¥è¡¨ç¤ºä¸º
$$
v_\pi(s) = \mathbb{E} _ \pi[G_i \mid s_i = s] = \frac{1}{N(s)}\sum^{N(s)}_{i=1}G_i
$$
å…¶ä¸­$N(s)$å°±æ˜¯çŠ¶æ€sè¢«è®¿é—®çš„æ¬¡æ•°ï¼Œ$G_i$æ˜¯ç¬¬iæ¬¡è®¿é—®såçš„å®é™…returnã€‚</p>
<p>så¯èƒ½è¢«å¤šæ¬¡è®¿é—®ï¼Œå› æ­¤è’™ç‰¹å¡æ´›æ–¹æ³•åˆ†ä¸ºfirst-visit MCå’Œevery-visit MCã€‚ä¸¤è€…çš„åŒºåˆ«åœ¨äºæ›´æ–°æ—¶æ˜¯å¦æ ¡éªŒ$S_t$å·²ç»åœ¨å½“å‰episodeä¸­å‡ºç°è¿‡ã€‚First-visitåªæ›´æ–°è¯¥çŠ¶æ€ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®ï¼ŒEvery-visitåˆ™æ˜¯è¯¥çŠ¶æ€å‡ºç°å‡ æ¬¡å°±æ›´æ–°å‡ æ¬¡ã€‚</p>
<p>å¦‚æœæ— æ³•å¾—åˆ°ç¯å¢ƒçš„æ¨¡å‹ï¼Œé‚£ä¹ˆè®¡ç®—åŠ¨ä½œçš„ä»·å€¼ï¼ˆâ€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„çš„ä»·å€¼ï¼‰æ¯”è®¡ç®—çŠ¶æ€çš„ä»·å€¼æ›´åŠ æœ‰ç”¨ã€‚åªéœ€å°†å¯¹çŠ¶æ€çš„è®¿é—®æ”¹ä¸ºå¯¹â€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„çš„è®¿é—®ï¼Œè’™ç‰¹å¡æ´›ç®—æ³•å°±å¯ä»¥å‡ ä¹å’Œä¹‹å‰å®Œå…¨ç›¸åŒçš„æ–¹å¼è§£å†³è¯¥é—®é¢˜ï¼Œå”¯ä¸€å¤æ‚ä¹‹å¤„åœ¨äºä¸€äº›â€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„å¯èƒ½æ°¸è¿œä¸ä¼šè¢«è®¿é—®åˆ°ã€‚ä¸ºäº†å®ç°åŸºäºåŠ¨ä½œä»·å€¼å‡½æ•°çš„ç­–ç•¥è¯„ä¼°ï¼Œæˆ‘ä»¬å¿…é¡»ä¿è¯æŒç»­çš„è¯•æ¢ã€‚ä¸€ç§æ–¹å¼æ˜¯å°†æŒ‡å®šçš„â€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„ä½œä¸ºèµ·ç‚¹å¼€å§‹ä¸€å¹•é‡‡æ ·ï¼ŒåŒæ—¶ä¿è¯æ‰€æœ‰â€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„éƒ½æœ‰éé›¶çš„æ¦‚ç‡å¯ä»¥è¢«é€‰ä¸ºèµ·ç‚¹ã€‚è¿™æ ·å°±ä¿è¯äº†åœ¨é‡‡æ ·çš„å¹•ä¸ªæ•°è¶‹äºæ— ç©·æ—¶ï¼Œæ¯ä¸€ä¸ªâ€œçŠ¶æ€-åŠ¨ä½œâ€äºŒå…ƒç»„éƒ½ä¼šè¢«è®¿é—®åˆ°æ— æ•°æ¬¡ã€‚æˆ‘ä»¬æŠŠè¿™ç§å‡è®¾ç§°ä¸º<strong>MC control with exploring starts</strong>ã€‚</p>
<p>ç­–ç•¥æ”¹è¿›çš„æ–¹æ³•æ˜¯åœ¨å½“å‰ä»·å€¼å‡½æ•°ä¸Šè´ªå¿ƒåœ°é€‰æ‹©åŠ¨ä½œã€‚ç”±äºæˆ‘ä»¬æœ‰åŠ¨ä½œä»·å€¼å‡½æ•°ï¼Œæ‰€ä»¥åœ¨è´ªå¿ƒçš„æ—¶å€™å®Œå…¨ä¸éœ€è¦ä½¿ç”¨ä»»ä½•çš„æ¨¡å‹ä¿¡æ¯ã€‚</p>
<p>Exploring startsçš„ä¸€ä¸ªé‡è¦å‡è®¾æ˜¯ç¯å¢ƒå¿…é¡»æ”¯æŒä»»æ„(s,a)èµ·ç‚¹ï¼Œè¿™åœ¨å®é™…ä¸­å‡ ä¹ä¸æˆç«‹ã€‚</p>
<p>ä½†æ˜¯ç°å®ä¸­ï¼Œæˆ‘ä»¬å¾ˆéš¾æ»¡è¶³è¯•æ¢æ€§å‡ºå‘çš„å‡è®¾ï¼Œä¸€èˆ¬æ€§çš„è§£æ³•æ˜¯æ™ºèƒ½ä½“èƒ½å¤ŸæŒç»­ä¸æ–­åœ°é€‰æ‹©æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œæœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥ä¿è¯è¿™ä¸€ç‚¹ï¼ŒåŒè½¨ç­–ç•¥ï¼ˆsoft on-policyï¼‰å’Œç¦»è½¨ç­–ç•¥ï¼ˆoff-policyï¼‰ã€‚åœ¨åŒè½¨ç­–ç•¥ä¸­ï¼Œç”¨äºç”Ÿæˆé‡‡æ ·æ•°æ®åºåˆ—çš„ç­–ç•¥å’Œç”¨äºå®é™…å†³ç­–çš„å¾…è¯„ä¼°å’Œæ”¹è¿›çš„ç­–ç•¥æ˜¯ç›¸åŒçš„ï¼›è€Œåœ¨ç¦»è½¨ç­–ç•¥ä¸­ï¼Œç”¨äºè¯„ä¼°æˆ–æ”¹è¿›çš„ç­–ç•¥ä¸ç”Ÿæˆé‡‡æ ·æ•°æ®çš„ç­–ç•¥æ˜¯ä¸åŒçš„ï¼Œå³ç”Ÿæˆçš„æ•°æ®â€œç¦»å¼€â€äº†å¾…ä¼˜åŒ–çš„ç­–ç•¥æ‰€å†³å®šçš„å†³ç­–åºåˆ—è½¨è¿¹ã€‚</p>
<h1 id="temporal-difference-learning">Temporal Difference Learning<a hidden class="anchor" aria-hidden="true" href="#temporal-difference-learning">#</a></h1>
<h2 id="td-policy-evaluation">TD policy evaluation<a hidden class="anchor" aria-hidden="true" href="#td-policy-evaluation">#</a></h2>
<h2 id="td-control">TD control<a hidden class="anchor" aria-hidden="true" href="#td-control">#</a></h2>
<h3 id="sarsa">Sarsa<a hidden class="anchor" aria-hidden="true" href="#sarsa">#</a></h3>
<p>åŒè½¨ç­–ç•¥ä¸‹çš„TD</p>
<h3 id="q-learning">Q-learning<a hidden class="anchor" aria-hidden="true" href="#q-learning">#</a></h3>
<p>ç¦»è½¨ç­–ç•¥ä¸‹çš„TD</p>
<h2 id="n-steps-td-methods">n-steps TD methods<a hidden class="anchor" aria-hidden="true" href="#n-steps-td-methods">#</a></h2>
<h1 id="planning-and-learning">Planning and Learning<a hidden class="anchor" aria-hidden="true" href="#planning-and-learning">#</a></h1>
<h1 id="value-function-approximation">Value function approximation<a hidden class="anchor" aria-hidden="true" href="#value-function-approximation">#</a></h1>
<h1 id="policy-gradient-methods">Policy Gradient Methods<a hidden class="anchor" aria-hidden="true" href="#policy-gradient-methods">#</a></h1>
<h1 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h1>
<p>[1] <a href="https://leovan.me/cn/2020/07/model-free-policy-prediction-and-control-monte-carlo-learning/">https://leovan.me/cn/2020/07/model-free-policy-prediction-and-control-monte-carlo-learning/</a>
[2] <a href="https://leovan.me/cn/2020/07/model-free-policy-prediction-and-control-temporal-difference-learning/">https://leovan.me/cn/2020/07/model-free-policy-prediction-and-control-temporal-difference-learning/</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://martinspace.top/zh/tags/rl/">RL</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://martinspace.top/zh/uq-cp/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>å…±å½¢é¢„æµ‹ç†è®ºç ”ç©¶</span>
  </a>
  <a class="next" href="https://martinspace.top/zh/data-analysis/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>æ—¶åºåˆ†æ/é¢„æµ‹</span>
  </a>
</nav>
<script src="https://utteranc.es/client.js"
        repo="oudushu/utterances"
        issue-term="title"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>
</script>



  </footer><script src="https://utteranc.es/client.js"
    repo="MartinRepo/utterancesInblog"
    issue-term="pathname"
    label="Comment"
    theme="preferred-color-scheme"
    crossorigin="anonymous"
    async>
</script>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://martinspace.top/zh/">Martin&#39;s space</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
        <br> 
        
        <a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">è¾½ ICP å¤‡ 2022011010 å· -1</a>
    </span>
    
    <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv">
        è¢«è®¿é—®äº†<span id="busuanzi_value_site_pv"></span>æ¬¡
    </span>
    <span id="busuanzi_container_site_uv">
        è¿æ¥äº†<span id="busuanzi_value_site_uv"></span>ä½å®¢äºº
    </span>
    </div>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerHTML = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerHTML = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script></body>

</html>

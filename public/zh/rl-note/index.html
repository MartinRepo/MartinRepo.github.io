<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>强化学习 | Martin&#39;s space</title>
<meta name="keywords" content="RL">
<meta name="description" content="多臂老虎机问题(MAB) 符号&amp;问题定义 大写斜体表示随机变量，例如$A, R, A_t, R_t$ 小写字母表示这些随机变量的实现，例如$a, r, a_t, r_t, Pr${$A_t=a_t$} 花体，">
<meta name="author" content="Martin">
<link rel="canonical" href="https://martinspace.top/zh/rl-note/">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bc3e848411f8e2e3b39fd084e8d998d9f2d9782118c440525c90992eaecdc9f0.css" integrity="sha256-vD6EhBH44uOzn9CE6NmY2fLZeCEYxEBSXJCZLq7NyfA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://martinspace.top/icebear.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="https://martinspace.top/icebear.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="https://martinspace.top/icebear.jpg">
<link rel="apple-touch-icon" href="https://martinspace.top/icebear.jpg">
<link rel="mask-icon" href="https://martinspace.top/icebear.jpg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://martinspace.top/zh/rl-note/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css" />
<script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
          },
          svg: {
            fontCache: 'global'
          }
        };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-J8TFFL1ZS7"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-J8TFFL1ZS7', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="强化学习" />
<meta property="og:description" content="多臂老虎机问题(MAB) 符号&amp;问题定义 大写斜体表示随机变量，例如$A, R, A_t, R_t$ 小写字母表示这些随机变量的实现，例如$a, r, a_t, r_t, Pr${$A_t=a_t$} 花体，" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://martinspace.top/zh/rl-note/" />
<meta property="og:image" content="https://martinspace.top/zh/rl-note/img/rl-note/whatisrl.png" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2025-02-01T10:39:49+00:00" />
<meta property="article:modified_time" content="2025-02-01T10:39:49+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://martinspace.top/zh/rl-note/img/rl-note/whatisrl.png" />
<meta name="twitter:title" content="强化学习"/>
<meta name="twitter:description" content="多臂老虎机问题(MAB) 符号&amp;问题定义 大写斜体表示随机变量，例如$A, R, A_t, R_t$ 小写字母表示这些随机变量的实现，例如$a, r, a_t, r_t, Pr${$A_t=a_t$} 花体，"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "📒 文章",
      "item": "https://martinspace.top/zh/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "🔬 研究记录",
      "item": "https://martinspace.top/zh/post/2tech/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "强化学习",
      "item": "https://martinspace.top/zh/rl-note/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "强化学习",
  "name": "强化学习",
  "description": "多臂老虎机问题(MAB) 符号\u0026amp;问题定义 大写斜体表示随机变量，例如$A, R, A_t, R_t$ 小写字母表示这些随机变量的实现，例如$a, r, a_t, r_t, Pr${$A_t=a_t$} 花体，",
  "keywords": [
    "RL"
  ],
  "articleBody": "多臂老虎机问题(MAB) 符号\u0026问题定义 大写斜体表示随机变量，例如$A, R, A_t, R_t$ 小写字母表示这些随机变量的实现，例如$a, r, a_t, r_t, Pr${$A_t=a_t$} 花体，区间等表示集合，例如$\\mathcal{A}, [0, 1], \\mathbb{N}$ Given: a set of k actions, $\\mathcal{A}$, number of rounds T. Repeat for t in T rounds:\nAlgorithm selects arm $A_t \\in \\mathcal{A}$ Algorithm observes reward $R_t \\in [0, 1]$ Goal: maximise expected total reward. 预期的奖励被称为Value：$q_*(a) = \\mathbb{E}[R_t \\mid A_t = a]$\n问题核心(Explore-exploit dilemma) 由于各个老虎机的奖励分布未知，我们需要在探索和利用之间做出权衡，来最大化长期奖励。\n探索（Exploration） 和 利用（Exploitation） 之间的权衡是 MAB 问题的核心：\nExploration: 选择当前信息不足的选项，以获得关于奖励分布的更多知识。例如，如果某个老虎机的奖励不确定，你可能会尝试拉取它，以收集更多数据。 Exploitation: 选择当前估计奖励最高的选项，以最大化即时收益。例如，如果你已经知道某个老虎机的平均奖励最高，你会更倾向于持续拉取它。 这种权衡的核心难点在于：\n过度探索（过多尝试未知选项）可能导致较低的短期收益。 过度利用（过早锁定某个选项）可能导致错失更优的长期收益。 问题算法 Action-Value Methods 我们可以估计某个动作$a$所获得奖励的样本均值。，具体函数表示为： $$ Q_t(a) = \\frac{\\text{Sum of rewards when taken a so far}}{\\text{Number of times taken a so far}} $$ $$ Q_t(a) = \\frac{1}{N_t(a)} \\sum^{t-1}_{\\tau=1}R _ {\\tau=1} \\cdot \\mathbb{1} _ {A_t=a} $$ 其中\n$N_t(a)$ 是到当前时间 $t$ 为止，动作 $a$ 被选择的次数。 $R_{\\tau}$ 是第 $\\tau$ 次执行动作的奖励。 $\\mathbb{1} _ {A_{\\tau} = a}$ 是一个指示函数，当 $\\tau$ 时刻选择了动作 $a$ 时，它的值为 1，否则为 0。 如果一个动作被选取无穷次，那么它的样本均值会收敛到真实的期望奖励，即： $$ \\lim_{N_t(a) \\to \\infty} Q_t(a) = q_*(a) $$\n$\\epsilon$- Greedy Action Selection 根据上面的公式，我们可以把exploit表示为 $$ A_t = A_t^* = \\mathop{\\arg\\max}\\limits_{a} Q_t(a) $$ exploration(随机选择action)表示为 $$ A_t\\sim\\text{Unif}(\\mathcal{A}) $$\ndef epsilon_greedy(Q, epsilon, actions): if np.random.rand() \u003c epsilon: # 以概率 epsilon 进行探索 return np.random.choice(actions) else: # 以概率 (1 - epsilon) 选择当前最优动作 return np.argmax(Q) self.execute(A_t) self.observe(R_t) self.update(N_t_a, Q_t_a) 如何在递增的同时避免重复计算？ $$ Q_n = \\frac{R_1 + … + R_{n-1}}{n-1} $$ $$ Q_{n+1} = Q_n + \\frac{1}{n}[R_n - Q_n] $$ 这也是强化学习中update rules的标准形式 $$ \\text{NewEstimate \u003c— OldEstimate + StepSize[Target - OldEstimate]} $$\nNon-Stationary Problem 假设true action value会随时间而变化，这是强化学习中经常遇到的non-stationary problem，这个时候再用sample average就不合适了，我们引入一个step-size parameter $\\alpha\\in[0, 1]$来跟踪action value，那么更新函数变为 $$ Q_{n+1} = Q_n + \\alpha[R_n - Q_n] $$\nStochastic Approximation Convergence Conditions 随机逼近（Stochastic Approximation, SA） 是一种用于在噪声环境中迭代逼近最优解的方法。这种方法常用于机器学习、强化学习（如 Q-learning）、优化算法（如随机梯度下降）等。\n在强化学习中，值函数更新 采用随机逼近的方式，例如： $$ Q_{t+1}(s, a) = Q_t(s, a) + \\alpha_t [R_t + \\gamma \\max_{a’} Q_t(s’, a’) - Q_t(s, a)] $$ 其中 $\\alpha_t$ 是学习率，目标是让 $Q_t(s,a)$ 收敛到最优值 $Q^*(s,a)$。\n为了保证 随机逼近算法 能够收敛，必须满足收敛条件（Convergence Conditions）:\n学习率满足 Robbins-Monro 条件 $$ \\sum_{t=1}^{\\infty} \\alpha_t = \\infty, \\quad \\sum_{t=1}^{\\infty} \\alpha_t^2 \u003c \\infty $$\n第一项 $ \\sum \\alpha_t = \\infty $ 确保算法在长期内继续学习，否则算法可能过早停止更新。 第二项 $ \\sum \\alpha_t^2 \u003c \\infty $ 确保学习率最终足够小，否则收敛会受噪声影响。 示例：\n满足条件的学习率： $ \\alpha_t = \\frac{1}{t} $、$ \\alpha_t = \\frac{1}{\\sqrt{t}} $ 不满足条件的学习率： 固定的 $ \\alpha_t = 0.1 $（因为求和不发散） Upper Confidence Bound (UCB) 相比于 $\\epsilon$-Greedy 算法的随机探索策略，UCB 通过构造一个上置信界（Upper Confidence Bound）来进行探索，从而更快地收敛到最优解。\nUCB 采用 置信区间（confidence bound） 来决定是否探索某个动作：\n利用（Exploitation）： 选择历史上平均奖励最高的动作（即当前最优动作）。 探索（Exploration）： 选择那些尝试次数较少、不确定性较大的动作。 公式： $$ A_t = \\arg\\max_{a} \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right] $$ 其中：\n$ Q_t(a) $ 是动作 $ a $ 在时间 $ t $ 时刻的平均奖励（历史经验）。 $ N_t(a) $ 是动作 $ a $ 被选择的次数。 $ t $ 是当前时间步（当前实验的总次数）。 $ c $ 是探索参数，控制探索程度（通常 $ c $ 取 $\\sqrt{2}$）。 $ \\ln t $ 是对数项，使得探索随时间减少。 解释：\n$ Q_t(a) $ 代表 利用（Exploitation）：倾向于选择历史上表现最好的动作。 $ \\sqrt{\\frac{\\ln t}{N_t(a)}} $ 代表 探索（Exploration）：当某个动作选择次数 ( N_t(a) ) 很小时，该项较大，鼓励探索。 import numpy as np def UCB(Q, N, t, c=1.0): ucb_values = Q + c * np.sqrt(np.log(t) / (N + 1e-5)) # 避免除零错误 return np.argmax(ucb_values) # 选择 UCB 值最高的动作 其中：\nQ：存储每个动作的平均奖励。 N：存储每个动作的选择次数。 t：当前时间步数（总实验次数）。 c：探索参数，控制探索强度。 马尔可夫决策过程(MDPs) 理论框架 核心数学概念\u0026函数 Policy Returns Value Functions Bellman Equation 动态规划 Policy Iteration Iterative policy evaluation Policy Improvement Value Iteration Asynchronous and generalised DP 蒙特卡洛方法 ",
  "wordCount" : "1851",
  "inLanguage": "zh",
  "image":"https://martinspace.top/zh/rl-note/img/rl-note/whatisrl.png","datePublished": "2025-02-01T10:39:49Z",
  "dateModified": "2025-02-01T10:39:49Z",
  "author":[{
    "@type": "Person",
    "name": "Martin"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://martinspace.top/zh/rl-note/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Martin's space",
    "logo": {
      "@type": "ImageObject",
      "url": "https://martinspace.top/icebear.jpg"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>



<script async src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://martinspace.top/zh/" accesskey="h" title="Martin&#39;s space (Alt + H)">
                <img src="https://martinspace.top/icebear.jpg" alt="" aria-label="logo"
                    height="35">Martin&#39;s space</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://martinspace.top/en/" title="English"
                            aria-label="English">English</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://martinspace.top/zh/" title="🏠 主页">
                    <span>🏠 主页</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/archives/" title="📁 归档">
                    <span>📁 归档</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/tags" title="🏷️ 标签">
                    <span>🏷️ 标签</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/search" title="🔍 搜索 (Alt &#43; /)" accesskey=/>
                    <span>🔍 搜索</span>
                </a>
            </li>
            <li>
                <a href="https://martinspace.top/zh/about" title="💭 关于">
                    <span>💭 关于</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://martinspace.top/zh/">主页</a>&nbsp;»&nbsp;<a href="https://martinspace.top/zh/post/">📒 文章</a>&nbsp;»&nbsp;<a href="https://martinspace.top/zh/post/2tech/">🔬 研究记录</a></div>
    <h1 class="post-title">
      强化学习
    </h1>
    <div class="post-meta">










创建: 2025-02-01 | 更新: 2025-02-01 | 字数: 1851字 | 阅读时长: 4分钟 | 
作者:Martin&nbsp;|&nbsp;标签: &nbsp;
    <ul class="post-tags-meta">
        <a href="https://martinspace.top/zh/tags/rl/">RL</a>
    </ul>


    
    </div>
  </header> 
<figure class="entry-cover1"><img loading="lazy" src="https://martinspace.top/img/rl-note/whatisrl.png" alt="what-is-rl">
        
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">目录</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e5%a4%9a%e8%87%82%e8%80%81%e8%99%8e%e6%9c%ba%e9%97%ae%e9%a2%98mab" aria-label="多臂老虎机问题(MAB)">多臂老虎机问题(MAB)</a><ul>
                            
                    <li>
                        <a href="#%e7%ac%a6%e5%8f%b7%e9%97%ae%e9%a2%98%e5%ae%9a%e4%b9%89" aria-label="符号&amp;amp;问题定义">符号&amp;问题定义</a></li>
                    <li>
                        <a href="#%e9%97%ae%e9%a2%98%e6%a0%b8%e5%bf%83explore-exploit-dilemma" aria-label="问题核心(Explore-exploit dilemma)">问题核心(Explore-exploit dilemma)</a></li>
                    <li>
                        <a href="#%e9%97%ae%e9%a2%98%e7%ae%97%e6%b3%95" aria-label="问题算法">问题算法</a><ul>
                            
                    <li>
                        <a href="#action-value-methods" aria-label="Action-Value Methods">Action-Value Methods</a></li>
                    <li>
                        <a href="#epsilon--greedy-action-selection" aria-label="$\epsilon$- Greedy Action Selection">$\epsilon$- Greedy Action Selection</a></li>
                    <li>
                        <a href="#non-stationary-problem" aria-label="Non-Stationary Problem">Non-Stationary Problem</a></li>
                    <li>
                        <a href="#stochastic-approximation-convergence-conditions" aria-label="Stochastic Approximation Convergence Conditions">Stochastic Approximation Convergence Conditions</a></li>
                    <li>
                        <a href="#upper-confidence-bound-ucb" aria-label="Upper Confidence Bound (UCB)">Upper Confidence Bound (UCB)</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8bmdps" aria-label="马尔可夫决策过程(MDPs)">马尔可夫决策过程(MDPs)</a><ul>
                            
                    <li>
                        <a href="#%e7%90%86%e8%ae%ba%e6%a1%86%e6%9e%b6" aria-label="理论框架">理论框架</a></li>
                    <li>
                        <a href="#%e6%a0%b8%e5%bf%83%e6%95%b0%e5%ad%a6%e6%a6%82%e5%bf%b5%e5%87%bd%e6%95%b0" aria-label="核心数学概念&amp;amp;函数">核心数学概念&amp;函数</a><ul>
                            
                    <li>
                        <a href="#policy" aria-label="Policy">Policy</a></li>
                    <li>
                        <a href="#returns" aria-label="Returns">Returns</a></li>
                    <li>
                        <a href="#value-functions" aria-label="Value Functions">Value Functions</a></li>
                    <li>
                        <a href="#bellman-equation" aria-label="Bellman Equation">Bellman Equation</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#%e5%8a%a8%e6%80%81%e8%a7%84%e5%88%92" aria-label="动态规划">动态规划</a><ul>
                            
                    <li>
                        <a href="#policy-iteration" aria-label="Policy Iteration">Policy Iteration</a><ul>
                            
                    <li>
                        <a href="#iterative-policy-evaluation" aria-label="Iterative policy evaluation">Iterative policy evaluation</a></li>
                    <li>
                        <a href="#policy-improvement" aria-label="Policy Improvement">Policy Improvement</a></li></ul>
                    </li>
                    <li>
                        <a href="#value-iteration" aria-label="Value Iteration">Value Iteration</a></li>
                    <li>
                        <a href="#asynchronous-and-generalised-dp" aria-label="Asynchronous and generalised DP">Asynchronous and generalised DP</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95" aria-label="蒙特卡洛方法">蒙特卡洛方法</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>
  <div class="post-content"><h1 id="多臂老虎机问题mab">多臂老虎机问题(MAB)<a hidden class="anchor" aria-hidden="true" href="#多臂老虎机问题mab">#</a></h1>
<h2 id="符号问题定义">符号&amp;问题定义<a hidden class="anchor" aria-hidden="true" href="#符号问题定义">#</a></h2>
<ul>
<li>大写斜体表示随机变量，例如$A, R, A_t, R_t$</li>
<li>小写字母表示这些随机变量的实现，例如$a, r, a_t, r_t, Pr${$A_t=a_t$}</li>
<li>花体，区间等表示集合，例如$\mathcal{A}, [0, 1], \mathbb{N}$</li>
</ul>
<p>Given: a set of k actions, $\mathcal{A}$, number of rounds T.
Repeat for t in T rounds:</p>
<ol>
<li>Algorithm selects arm $A_t \in \mathcal{A}$</li>
<li>Algorithm observes reward $R_t \in [0, 1]$
Goal: maximise expected total reward.</li>
</ol>
<p>预期的奖励被称为Value：$q_*(a) = \mathbb{E}[R_t \mid A_t = a]$</p>
<h2 id="问题核心explore-exploit-dilemma">问题核心(Explore-exploit dilemma)<a hidden class="anchor" aria-hidden="true" href="#问题核心explore-exploit-dilemma">#</a></h2>
<p>由于各个老虎机的奖励分布未知，我们需要在探索和利用之间做出权衡，来最大化长期奖励。</p>
<p>探索（Exploration） 和 利用（Exploitation） 之间的权衡是 MAB 问题的核心：</p>
<ul>
<li>Exploration: 选择当前信息不足的选项，以获得关于奖励分布的更多知识。例如，如果某个老虎机的奖励不确定，你可能会尝试拉取它，以收集更多数据。</li>
<li>Exploitation: 选择当前估计奖励最高的选项，以最大化即时收益。例如，如果你已经知道某个老虎机的平均奖励最高，你会更倾向于持续拉取它。</li>
</ul>
<p>这种权衡的核心难点在于：</p>
<ul>
<li>过度探索（过多尝试未知选项）可能导致较低的短期收益。</li>
<li>过度利用（过早锁定某个选项）可能导致错失更优的长期收益。</li>
</ul>
<h2 id="问题算法">问题算法<a hidden class="anchor" aria-hidden="true" href="#问题算法">#</a></h2>
<h3 id="action-value-methods">Action-Value Methods<a hidden class="anchor" aria-hidden="true" href="#action-value-methods">#</a></h3>
<p>我们可以估计某个动作$a$所获得奖励的样本均值。，具体函数表示为：
$$
Q_t(a) = \frac{\text{Sum of rewards when taken a so far}}{\text{Number of times taken a so far}}
$$
$$
Q_t(a) = \frac{1}{N_t(a)} \sum^{t-1}_{\tau=1}R _ {\tau=1} \cdot \mathbb{1} _ {A_t=a}
$$
其中</p>
<ul>
<li>$N_t(a)$ 是到当前时间 $t$ 为止，动作 $a$ 被选择的次数。</li>
<li>$R_{\tau}$ 是第 $\tau$ 次执行动作的奖励。</li>
<li>$\mathbb{1} _ {A_{\tau} = a}$ 是一个<strong>指示函数</strong>，当 $\tau$ 时刻选择了动作 $a$ 时，它的值为 1，否则为 0。</li>
</ul>
<p><strong>如果一个动作被选取无穷次</strong>，那么它的<strong>样本均值会收敛</strong>到真实的期望奖励，即：
$$
\lim_{N_t(a) \to \infty} Q_t(a) = q_*(a)
$$</p>
<h3 id="epsilon--greedy-action-selection">$\epsilon$- Greedy Action Selection<a hidden class="anchor" aria-hidden="true" href="#epsilon--greedy-action-selection">#</a></h3>
<p>根据上面的公式，我们可以把exploit表示为
$$
A_t = A_t^* = \mathop{\arg\max}\limits_{a} Q_t(a)
$$
exploration(随机选择action)表示为
$$
A_t\sim\text{Unif}(\mathcal{A})
$$</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> epsilon_greedy(Q, epsilon, actions):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> np.random.rand() &lt; epsilon:
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 以概率 epsilon 进行探索</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> np.random.choice(actions)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 以概率 (1 - epsilon) 选择当前最优动作</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> np.argmax(Q)
</span></span><span style="display:flex;"><span>    self.execute(A_t)
</span></span><span style="display:flex;"><span>    self.observe(R_t)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    self.update(N_t_a, Q_t_a)
</span></span></code></pre></div><p>如何在递增的同时避免重复计算？
$$
Q_n = \frac{R_1 + &hellip; + R_{n-1}}{n-1}
$$
$$
Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]
$$
这也是强化学习中update rules的标准形式
$$
\text{NewEstimate &lt;— OldEstimate + StepSize[Target - OldEstimate]}
$$</p>
<h3 id="non-stationary-problem">Non-Stationary Problem<a hidden class="anchor" aria-hidden="true" href="#non-stationary-problem">#</a></h3>
<p>假设true action value会随时间而变化，这是强化学习中经常遇到的non-stationary problem，这个时候再用sample average就不合适了，我们引入一个step-size parameter $\alpha\in[0, 1]$来跟踪action value，那么更新函数变为
$$
Q_{n+1} = Q_n + \alpha[R_n - Q_n]
$$</p>
<h3 id="stochastic-approximation-convergence-conditions">Stochastic Approximation Convergence Conditions<a hidden class="anchor" aria-hidden="true" href="#stochastic-approximation-convergence-conditions">#</a></h3>
<p><strong>随机逼近（Stochastic Approximation, SA）</strong> 是一种用于在噪声环境中迭代逼近最优解的方法。这种方法常用于机器学习、强化学习（如 Q-learning）、优化算法（如随机梯度下降）等。</p>
<p>在强化学习中，<strong>值函数更新</strong> 采用随机逼近的方式，例如：
$$
Q_{t+1}(s, a) = Q_t(s, a) + \alpha_t [R_t + \gamma \max_{a&rsquo;} Q_t(s&rsquo;, a&rsquo;) - Q_t(s, a)]
$$
其中 $\alpha_t$ 是学习率，目标是让 $Q_t(s,a)$ 收敛到最优值 $Q^*(s,a)$。</p>
<p>为了保证 <strong>随机逼近算法</strong> 能够收敛，必须满足<strong>收敛条件（Convergence Conditions）</strong>:</p>
<p><strong>学习率满足 Robbins-Monro 条件</strong>
$$
\sum_{t=1}^{\infty} \alpha_t = \infty, \quad \sum_{t=1}^{\infty} \alpha_t^2 &lt; \infty
$$</p>
<ul>
<li><strong>第一项</strong> $ \sum \alpha_t = \infty $ 确保算法在<strong>长期内继续学习</strong>，否则算法可能过早停止更新。</li>
<li><strong>第二项</strong> $ \sum \alpha_t^2 &lt; \infty $ 确保学习率最终<strong>足够小</strong>，否则收敛会受噪声影响。</li>
</ul>
<p>示例：</p>
<ul>
<li><strong>满足条件的学习率：</strong> $ \alpha_t = \frac{1}{t} $、$ \alpha_t = \frac{1}{\sqrt{t}} $</li>
<li><strong>不满足条件的学习率：</strong> 固定的 $ \alpha_t = 0.1 $（因为求和不发散）</li>
</ul>
<h3 id="upper-confidence-bound-ucb">Upper Confidence Bound (UCB)<a hidden class="anchor" aria-hidden="true" href="#upper-confidence-bound-ucb">#</a></h3>
<p>相比于 <strong>$\epsilon$-Greedy</strong> 算法的随机探索策略，UCB 通过构造一个上置信界（Upper Confidence Bound）来进行探索，从而更快地收敛到最优解。</p>
<p>UCB 采用 <strong>置信区间（confidence bound）</strong> 来决定是否探索某个动作：</p>
<ul>
<li><strong>利用（Exploitation）：</strong> 选择历史上平均奖励最高的动作（即当前最优动作）。</li>
<li><strong>探索（Exploration）：</strong> 选择那些尝试次数较少、不确定性较大的动作。</li>
</ul>
<p><strong>公式：</strong>
$$
A_t = \arg\max_{a} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
$$
其中：</p>
<ul>
<li>$ Q_t(a) $ 是动作 $ a $ 在时间 $ t $ 时刻的平均奖励（历史经验）。</li>
<li>$ N_t(a) $ 是<strong>动作 $ a $ 被选择的次数</strong>。</li>
<li>$ t $ 是当前时间步（当前实验的总次数）。</li>
<li>$ c $ 是<strong>探索参数</strong>，控制探索程度（通常 $ c $ 取 $\sqrt{2}$）。</li>
<li>$ \ln t $ 是对数项，使得探索随时间减少。</li>
</ul>
<p><strong>解释：</strong></p>
<ul>
<li>$ Q_t(a) $ 代表 <strong>利用</strong>（Exploitation）：倾向于选择历史上表现最好的动作。</li>
<li>$ \sqrt{\frac{\ln t}{N_t(a)}} $ 代表 <strong>探索</strong>（Exploration）：当某个动作选择次数 ( N_t(a) ) 很小时，该项较大，鼓励探索。</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> numpy <span style="color:#fff;font-weight:bold">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> UCB(Q, N, t, c=<span style="color:#ff0;font-weight:bold">1.0</span>):
</span></span><span style="display:flex;"><span>    ucb_values = Q + c * np.sqrt(np.log(t) / (N + <span style="color:#ff0;font-weight:bold">1e-5</span>))  <span style="color:#007f7f"># 避免除零错误</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> np.argmax(ucb_values)  <span style="color:#007f7f"># 选择 UCB 值最高的动作</span>
</span></span></code></pre></div><p>其中：</p>
<ul>
<li><code>Q</code>：存储每个动作的<strong>平均奖励</strong>。</li>
<li><code>N</code>：存储每个动作的<strong>选择次数</strong>。</li>
<li><code>t</code>：当前时间步数（总实验次数）。</li>
<li><code>c</code>：探索参数，控制探索强度。</li>
</ul>
<h1 id="马尔可夫决策过程mdps">马尔可夫决策过程(MDPs)<a hidden class="anchor" aria-hidden="true" href="#马尔可夫决策过程mdps">#</a></h1>
<h2 id="理论框架">理论框架<a hidden class="anchor" aria-hidden="true" href="#理论框架">#</a></h2>
<h2 id="核心数学概念函数">核心数学概念&amp;函数<a hidden class="anchor" aria-hidden="true" href="#核心数学概念函数">#</a></h2>
<h3 id="policy">Policy<a hidden class="anchor" aria-hidden="true" href="#policy">#</a></h3>
<h3 id="returns">Returns<a hidden class="anchor" aria-hidden="true" href="#returns">#</a></h3>
<h3 id="value-functions">Value Functions<a hidden class="anchor" aria-hidden="true" href="#value-functions">#</a></h3>
<h3 id="bellman-equation">Bellman Equation<a hidden class="anchor" aria-hidden="true" href="#bellman-equation">#</a></h3>
<h1 id="动态规划">动态规划<a hidden class="anchor" aria-hidden="true" href="#动态规划">#</a></h1>
<h2 id="policy-iteration">Policy Iteration<a hidden class="anchor" aria-hidden="true" href="#policy-iteration">#</a></h2>
<h3 id="iterative-policy-evaluation">Iterative policy evaluation<a hidden class="anchor" aria-hidden="true" href="#iterative-policy-evaluation">#</a></h3>
<h3 id="policy-improvement">Policy Improvement<a hidden class="anchor" aria-hidden="true" href="#policy-improvement">#</a></h3>
<h2 id="value-iteration">Value Iteration<a hidden class="anchor" aria-hidden="true" href="#value-iteration">#</a></h2>
<h2 id="asynchronous-and-generalised-dp">Asynchronous and generalised DP<a hidden class="anchor" aria-hidden="true" href="#asynchronous-and-generalised-dp">#</a></h2>
<h1 id="蒙特卡洛方法">蒙特卡洛方法<a hidden class="anchor" aria-hidden="true" href="#蒙特卡洛方法">#</a></h1>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://martinspace.top/zh/tags/rl/">RL</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://martinspace.top/zh/uq-cp/">
    <span class="title">« 上一页</span>
    <br>
    <span>共形预测理论研究</span>
  </a>
  <a class="next" href="https://martinspace.top/zh/mlsys-ft/">
    <span class="title">下一页 »</span>
    <br>
    <span>微调加速研究</span>
  </a>
</nav>
<script src="https://utteranc.es/client.js"
        repo="oudushu/utterances"
        issue-term="title"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>
</script>



  </footer><script src="https://utteranc.es/client.js"
    repo="MartinRepo/utterancesInblog"
    issue-term="pathname"
    label="Comment"
    theme="preferred-color-scheme"
    crossorigin="anonymous"
    async>
</script>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://martinspace.top/zh/">Martin&#39;s space</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
        <br> 
        
        <a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">辽 ICP 备 2022011010 号 -1</a>
    </span>
    
    <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv">
        被访问了<span id="busuanzi_value_site_pv"></span>次
    </span>
    <span id="busuanzi_container_site_uv">
        迎接了<span id="busuanzi_value_site_uv"></span>位客人
    </span>
    </div>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script></body>

</html>
